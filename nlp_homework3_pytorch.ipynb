{"cells":[{"cell_type":"markdown","metadata":{"id":"GjK98yKQ3lnF"},"source":["### Homework: going neural (6 pts)\n","\n","We've checked out statistical approaches to language models in the last notebook. Now let's go find out what deep learning has to offer.\n","\n","<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/expanding_mind_lm_kn_3.png' width=300px>\n","\n","We're gonna use the same dataset as before, except this time we build a language model that's character-level, not word level. Before you go:\n","* If you haven't done seminar already, use `seminar.ipynb` to download the data."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"l0sVcFMX3lnL","executionInfo":{"status":"ok","timestamp":1710779908421,"user_tz":-180,"elapsed":958,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"LLxt-rAx3lnN"},"source":["Working on character level means that we don't need to deal with large vocabulary or missing words. Heck, we can even keep uppercase words in text! The downside, however, is that all our sequences just got a lot longer.\n","\n","However, we still need special tokens:\n","* Begin Of Sequence  (__BOS__) - this token is at the start of each sequence. We use it so that we always have non-empty input to our neural network. $P(x_t) = P(x_1 | BOS)$\n","* End Of Sequence (__EOS__) - you guess it... this token is at the end of each sequence. The catch is that it should __not__ occur anywhere else except at the very end. If our model produces this token, the sequence is over.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxW0LDGw3lnN","executionInfo":{"status":"ok","timestamp":1710779913514,"user_tz":-180,"elapsed":5094,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"92fc6cea-57c3-46c7-c688-191b5f5bd695"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-18 16:38:31--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n","--2024-03-18 16:38:31--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com/cd/0/get/CPX8rM8sgrf5jrT4-hApTd2HSzFLEoXkMGVH2BC9B_AKJmRlhanI6bhQ4I3_vYqc57YU4NXGVfB5EGkTSNIlwPG_eebP3Cev8I6f65lkeBSz61vkvVSWbOqF_O4olDslJxM/file?dl=1# [following]\n","--2024-03-18 16:38:32--  https://uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com/cd/0/get/CPX8rM8sgrf5jrT4-hApTd2HSzFLEoXkMGVH2BC9B_AKJmRlhanI6bhQ4I3_vYqc57YU4NXGVfB5EGkTSNIlwPG_eebP3Cev8I6f65lkeBSz61vkvVSWbOqF_O4olDslJxM/file?dl=1\n","Resolving uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com (uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6018:15::a27d:30f\n","Connecting to uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com (uc2136f81e23c470b25ed176152e.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 18933283 (18M) [application/binary]\n","Saving to: ‘arxivData.json.tar.gz’\n","\n","arxivData.json.tar. 100%[===================>]  18.06M  25.4MB/s    in 0.7s    \n","\n","2024-03-18 16:38:33 (25.4 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n","\n","arxivData.json\n"]}],"source":["BOS, EOS = ' ', '\\n'\n","\n","!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n","!tar -xvzf arxivData.json.tar.gz\n","\n","data = pd.read_json(\"./arxivData.json\")\n","lines = data.apply(lambda row: (row['title'] + ' ; ' + row['summary'])[:512], axis=1) \\\n","            .apply(lambda line: BOS + line.replace(EOS, ' ') + EOS) \\\n","            .tolist()\n","\n","# if you missed the seminar, download data here - https://yadi.sk/d/_nGyU2IajjR9-w"]},{"cell_type":"code","source":["lines"],"metadata":{"id":"JdPcdYo88QVS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710779913514,"user_tz":-180,"elapsed":132,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"c084bf2d-a74f-44d6-bf4f-b1010710767a"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' Dual Recurrent Attention Units for Visual Question Answering ; We propose an architecture for VQA which utilizes recurrent layers to generate visual and textual attention. The memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question. Our single model outperforms the first place winner on the VQA 1.0 dataset, performs within margin to the current state-\\n',\n"," ' Sequential Short-Text Classification with Recurrent and Convolutional   Neural Networks ; Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences in a document or utterances in a dialog), and most existing ANN-based systems do not leverage the preceding short texts when classifying a subsequent one. In this work, we present a model based on recurrent neural networks and convolutiona\\n',\n"," ' Multiresolution Recurrent Neural Networks: An Application to Dialogue   Response Generation ; We introduce the multiresolution recurrent neural network, which extends the sequence-to-sequence framework to model natural language generation as two parallel discrete stochastic processes: a sequence of high-level coarse tokens, and a sequence of natural language tokens. There are many ways to estimate or learn the high-level coarse tokens, but we argue that a simple extraction procedure is sufficient to capture\\n',\n"," ' Learning what to share between loosely related tasks ; Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loose\\n',\n"," ' A Deep Reinforcement Learning Chatbot ; We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neura\\n',\n"," ' Generating Sentences by Editing Prototypes ; We propose a new generative model of sentences that first samples a prototype sentence from the training corpus and then edits it into a new sentence. Compared to traditional models that generate from scratch either left-to-right or by first sampling a latent sentence vector, our prototype-then-edit model improves perplexity on language modeling and generates higher quality outputs according to human evaluation. Furthermore, the model gives rise to a latent edit \\n',\n"," ' A Deep Reinforcement Learning Chatbot (Short Version) ; We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including neural network and template-based models. By applying reinforcement learning to crowdsour\\n',\n"," ' Document Image Coding and Clustering for Script Discrimination ; The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern i\\n',\n"," ' Tutorial on Answering Questions about Images with Deep Learning ; Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competit\\n',\n"," ' pix2code: Generating Code from a Graphical User Interface Screenshot ; Transforming a graphical user interface screenshot created by a designer into computer code is a typical task conducted by a developer in order to build customized software, websites, and mobile applications. In this paper, we show that deep learning methods can be leveraged to train a model end-to-end to automatically generate code from a single input image with over 77% of accuracy for three different platforms (i.e. iOS, Android and w\\n',\n"," ' A Unified Deep Neural Network for Speaker and Language Recognition ; Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge spea\\n',\n"," ' Efficient Neural Architecture Search via Parameter Sharing ; We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trai\\n',\n"," ' Building Machines That Learn and Think Like People ; Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucia\\n',\n"," ' Towards Bayesian Deep Learning: A Survey ; While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible.\\n',\n"," ' Hierarchical Deep Reinforcement Learning: Integrating Temporal   Abstraction and Intrinsic Motivation ; Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could even\\n',\n"," \" Learning Features by Watching Objects Move ; This paper presents a novel yet intuitive approach to unsupervised feature learning. Inspired by the human visual system, we explore whether low-level motion-based grouping cues can be used to learn an effective visual representation. Specifically, we use unsupervised motion-based segmentation on videos to obtain segments, which we use as 'pseudo ground truth' to train a convolutional network to segment objects from a single frame. Given the extensive evidence th\\n\",\n"," ' Domain Adaptive Neural Networks for Object Recognition ; We propose a simple neural network model to deal with the domain adaptation problem in object recognition. Our model incorporates the Maximum Mean Discrepancy (MMD) measure as a regularization in the supervised learning to reduce the distribution mismatch between the source and target domains in the latent space. From experiments, we demonstrate that the MMD regularization is an effective tool to provide good domain adaptation models on both SURF feat\\n',\n"," ' Beyond Temporal Pooling: Recurrence and Temporal Convolutions for   Gesture Recognition in Video ; Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method \\n',\n"," ' Telugu OCR Framework using Deep Learning ; In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph \\n',\n"," ' Adversarial Feature Learning ; The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxil\\n',\n"," ' The Mythos of Model Interpretability ; Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguit\\n',\n"," ' Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a   Changing World ; In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associate\\n',\n"," ' Borrowing Treasures from the Wealthy: Deep Transfer Learning through   Selective Joint Fine-tuning ; Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training da\\n',\n"," ' Aligned Image-Word Representations Improve Inductive Transfer Across   Vision-Language Tasks ; An important goal of computer vision is to build systems that learn visual representations over time that can be applied to many tasks. In this paper, we investigate a vision-language embedding as a core representation and show that it leads to better cross-task transfer than standard multi-task learning. In particular, the task of visual recognition is aligned to the task of visual question answering by forcing e\\n',\n"," ' Universal Adversarial Perturbations Against Semantic Image Segmentation ; While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work h\\n',\n"," ' The loss surface of deep and wide neural networks ; While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activ\\n',\n"," ' Semantically Decomposing the Latent Spaces of Generative Adversarial   Networks ; We propose a new algorithm for training generative adversarial networks that jointly learns latent codes for both identities (e.g. individual humans) and observations (e.g. specific photographs). By fixing the identity portion of the latent codes, we can generate diverse images of the same subject, and by fixing the observation portion, we can traverse the manifold of subjects while maintaining contingent aspects such as light\\n',\n"," ' Variants of RMSProp and Adagrad with Logarithmic Regret Bounds ; Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show $\\\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for\\n',\n"," ' ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching ; We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabiliz\\n',\n"," ' A systematic study of the class imbalance problem in convolutional   neural networks ; In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchma\\n',\n"," ' Regularization for Deep Learning: A Taxonomy ; Regularization is one of the crucial ingredients of deep learning, yet the term regularization has various definitions, and regularization methods are often studied separately from each other. In our work we present a systematic, unifying taxonomy to categorize existing methods. We distinguish methods that affect data, network architectures, error terms, regularization terms, and optimization procedures. We do not provide all details about the listed methods; i\\n',\n"," ' Clustering with Deep Learning: Taxonomy and New Methods ; Clustering is a fundamental machine learning method. The quality of its results is dependent on the data distribution. For this reason, deep neural networks can be used for learning better representations of the data. In this paper, we propose a systematic taxonomy for clustering with deep learning, in addition to a review of methods from the field. Based on our taxonomy, creating new methods is more straightforward. We also propose a new approach wh\\n',\n"," ' Coarse to fine non-rigid registration: a chain of scale-specific neural   networks for multimodal image alignment with application to remote sensing ; We tackle here the problem of multimodal image non-rigid registration, which is of prime importance in remote sensing and medical imaging. The difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. By analyzing these methods, we note the significance of the notion of scale. We design eas\\n',\n"," ' Describing Videos by Exploiting Temporal Structure ; Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal stru\\n',\n"," ' Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in   the Blanks ; Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE)\\n',\n"," ' Sentiment Classification using Images and Label Embeddings ; In this project we analysed how much semantic information images carry, and how much value image data can add to sentiment analysis of the text associated with the images. To better understand the contribution from images, we compared models which only made use of image data, models which only made use of text data, and models which combined both data types. We also analysed if this approach could help sentiment classifiers generalize to unknown s\\n',\n"," ' Natural-Parameter Networks: A Class of Probabilistic Neural Networks ; Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are often prone to overfitting. One effective way to alleviate this problem is to exploit the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the weights and neurons according\\n',\n"," ' Learning to Perform Physics Experiments via Deep Reinforcement Learning ; When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language\\n',\n"," ' A Network-based End-to-End Trainable Task-oriented Dialogue System ; Teaching machines to accomplish tasks by conversing naturally with humans is challenging. Currently, developing task-oriented dialogue systems requires creating multiple components and typically this involves either a large amount of handcrafting, or acquiring costly labelled datasets to solve a statistical learning problem for each component. In this work we introduce a neural network-based text-in, text-out end-to-end trainable goal-orie\\n',\n"," ' A Factorization Machine Framework for Testing Bigram Embeddings in   Knowledgebase Completion ; Embedding-based Knowledge Base Completion models have so far mostly combined distributed representations of individual entities or relations to compute truth scores of missing links. Facts can however also be represented using pairwise embeddings, i.e. embeddings for pairs of entities and relations. In this paper we explore such bigram embeddings with a flexible Factorization Machine model and several ablations f\\n',\n"," ' Neural Networks for Joint Sentence Classification in Medical Paper   Abstracts ; Existing models based on artificial neural networks (ANNs) for sentence classification often do not incorporate the context in which sentences appear, and classify sentences individually. However, traditional sentence classification approaches have been shown to greatly benefit from jointly classifying subsequent sentences, such as with conditional random fields. In this work, we present an ANN architecture that combines the ef\\n',\n"," ' De-identification of Patient Notes with Recurrent Neural Networks ; Objective: Patient notes in electronic health records (EHRs) may contain critical information for medical investigations. However, the vast majority of medical investigators can only access de-identified notes, in order to protect the confidentiality of patients. In the United States, the Health Insurance Portability and Accountability Act (HIPAA) defines 18 types of protected health information (PHI) that needs to be removed to de-identify\\n',\n"," ' Reasoning with Memory Augmented Neural Networks for Language   Comprehension ; Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by us\\n',\n"," ' Automatic Rule Extraction from Long Short Term Memory Networks ; Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LS\\n',\n"," ' Comparing Rule-Based and Deep Learning Models for Patient Phenotyping ; Objective: We investigate whether deep learning techniques for natural language processing (NLP) can be used efficiently for patient phenotyping. Patient phenotyping is a classification task for determining whether a patient has a medical condition, and is a crucial part of secondary analysis of healthcare data. We assess the performance of deep learning algorithms and compare them with classical NLP approaches.   Materials and Methods:\\n',\n"," ' MIT at SemEval-2017 Task 10: Relation Extraction with Convolutional   Neural Networks ; Over 50 million scholarly articles have been published: they constitute a unique repository of knowledge. In particular, one may infer from them relations between scientific concepts, such as synonyms and hyponyms. Artificial neural networks have been recently explored for relation extraction. In this work, we continue this line of work and present a system based on a convolutional neural network to extract relations. Ou\\n',\n"," ' Transfer Learning for Named-Entity Recognition with Neural Networks ; Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we \\n',\n"," ' Adversarial Generation of Natural Language ; Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simpl\\n',\n"," ' Explaining Recurrent Neural Network Predictions in Sentiment Analysis ; Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LS\\n',\n"," ' Text Compression for Sentiment Analysis via Evolutionary Algorithms ; Can textual data be compressed intelligently without losing accuracy in evaluating sentiment? In this study, we propose a novel evolutionary compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression), which makes use of Parts-of-Speech tags to compress text in a way that sacrifices minimal classification accuracy when used in conjunction with sentiment analysis algorithms. An analysis of PARSEC with eight commercial and non\\n',\n"," ' Building competitive direct acoustics-to-word models for English   conversational speech recognition ; Direct acoustics-to-word (A2W) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones, characters, or context-dependent hidden Markov model states. This is because A2W models recognize words from speech without any decoder, pronunciation lexicon, or externally-trained language model, making training and d\\n',\n"," ' Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for   Visual Question Answering ; We address the problem of Visual Question Answering (VQA), which requires joint image and language understanding to answer a question about a given photograph. Recent approaches have applied deep image captioning methods based on convolutional-recurrent networks to this problem, but have failed to model spatial inference. To remedy this, we propose a model we call the Spatial Memory Network and apply it to \\n',\n"," ' Task-driven Visual Saliency and Attention-based Visual Question   Answering ; Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where peo\\n',\n"," ' Optimising The Input Window Alignment in CD-DNN Based Phoneme   Recognition for Low Latency Processing ; We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current o\\n',\n"," ' Bridging LSTM Architecture and the Neural Dynamics during Reading ; Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its int\\n',\n"," ' Feature Weight Tuning for Recursive Neural Networks ; This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \"weight tuning\" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be vi\\n',\n"," ' A New Data Representation Based on Training Data Characteristics to   Extract Drug Named-Entity in Medical Text ; One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining \\n',\n"," ' DopeLearning: A Computational Approach to Rap Lyrics Generation ; Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankS\\n',\n"," ' Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN ; Semantic matching, which aims to determine the matching degree between two texts, is a fundamental problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of th\\n',\n"," ' Piecewise Latent Variables for Neural Variational Text Processing ; Advances in neural variational inference have facilitated the learning of powerful directed graphical models with continuous latent variables, such as variational autoencoders. The hope is that such models will learn to represent rich, multi-modal latent factors in real-world data, such as natural language text. However, current models often assume simplistic priors on the latent variables - such as the uni-modal Gaussian distribution - whi\\n',\n"," ' Recurrent Neural Networks with External Memory for Language   Understanding ; Recurrent Neural Networks (RNNs) have become increasingly popular for the task of language understanding. In this task, a semantic tagger is deployed to associate a semantic label to each word in an input sequence. The success of RNN may be attributed to its ability to memorize long-term dependence that relates the current-time semantic label prediction to the observations many time instances away. However, the memory capacity of \\n',\n"," ' A Neural Network Approach to Context-Sensitive Generation of   Conversational Responses ; We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over bot\\n',\n"," ' The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured   Multi-Turn Dialogue Systems ; This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the \\n',\n"," ' Building End-To-End Dialogue Systems Using Generative Hierarchical   Neural Network Models ; We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue \\n',\n"," ' End-to-End Attention-based Large Vocabulary Speech Recognition ; Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at t\\n',\n"," ' Towards Neural Network-based Reasoning ; We propose Neural Reasoner, a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular stru\\n',\n"," ' What to talk about and how? Selective Generation using LSTMs with   Coarse-to-Fine Alignment ; We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally e\\n',\n"," ' Reasoning about Entailment with Neural Attention ; While most approaches to automatically recognizing entailment relations have used classifiers employing hand engineered features derived from complex natural language processing pipelines, in practice their performance has been only slightly better than bag-of-word pair classifiers using only lexical similarity. The only attempt so far to build an end-to-end differentiable neural network for entailment failed to outperform such a simple similarity classifie\\n',\n"," ' Highway Long Short-Term Memory RNNs for Distant Speech Recognition ; In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can expl\\n',\n"," ' Neural Enquirer: Learning to Query Tables with Natural Language ; We proposed Neural Enquirer as a neural network architecture to execute a natural language (NL) query on a knowledge-base (KB) for answers. Basically, Neural Enquirer finds the distributed representation of a query and then executes it on knowledge-base tables to obtain the answer as one of the values in the tables. Unlike similar efforts in end-to-end training of semantic parsers, Neural Enquirer is fully \"neuralized\": it not only gives dist\\n',\n"," ' Sentence Pair Scoring: Towards Unified Framework for Text Comprehension ; We review the task of Sentence Pair Scoring, popular in the literature in various forms - viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component of Memory Networks.   We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurr\\n',\n"," ' Incorporating Copying Mechanism in Sequence-to-Sequence Learning ; We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide whe\\n',\n"," ' Generating Factoid Questions With Recurrent Neural Networks: The 30M   Factoid Question-Answer Corpus ; Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebas\\n',\n"," \" How NOT To Evaluate Your Dialogue System: An Empirical Study of   Unsupervised Evaluation Metrics for Dialogue Response Generation ; We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-te\\n\",\n"," ' A Hierarchical Latent Variable Encoder-Decoder Model for Generating   Dialogues ; Sequential data often possesses a hierarchical structure with complex dependencies between subsequences, such as found between the utterances in a dialogue. In an effort to model this kind of generative process, we propose a neural network-based generative architecture, with latent stochastic variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare\\n',\n"," ' Neural Associative Memory for Dual-Sequence Modeling ; Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended \\n',\n"," ' Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior   Knowledge ; We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiri\\n',\n"," \" Embracing data abundance: BookTest Dataset for Reading Comprehension ; There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We sho\\n\",\n"," \" Quasi-Recurrent Neural Networks ; Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across \\n\",\n"," ' Input Switched Affine Networks: An RNN Architecture Designed for   Interpretability ; There exist many problem domains where the interpretability of neural network models is essential for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations - in other words an RNN without any explicit nonlinearities, but with input-dependent recurrent weights. This simple form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the\\n',\n"," ' Frustratingly Short Attention Spans in Neural Language Modeling ; Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mec\\n',\n"," ' A Structured Self-attentive Sentence Embedding ; This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence a\\n',\n"," ' A Recurrent Neural Model with Attention for the Recognition of Chinese   Implicit Discourse Relations ; We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches. Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank. We also visualize its attention activity to illustrate the m\\n',\n"," ' Event Representations for Automated Story Generation with Deep Neural   Nets ; Automated story generation is the problem of automatically selecting a sequence of events, actions, or words that can be told as a story. We seek to develop a system that can generate stories by learning everything it needs to know from textual story corpora. To date, recurrent neural networks that learn language models at character, word, or sentence levels have had little success generating coherent stories. We explore the ques\\n',\n"," ' A Joint Model for Question Answering and Question Generation ; We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks.\\n',\n"," ' Learning Intrinsic Sparse Structures within Long Short-Term Memory ; Model compression is significant for the wide adoption of Recurrent Neural Networks (RNNs) in both user devices possessing limited resources and business clusters requiring quick responses to large-scale service requests. This work aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the sizes of basic structures within LSTM units, including input updates, gates, hidden states, cell states and outputs. Independently \\n',\n"," ' Why PairDiff works? -- A Mathematical Analysis of Bilinear Relational   Compositional Operators for Analogy Detection ; Representing the semantic relations that exist between two given words (or entities) is an important first step in a wide-range of NLP applications such as analogical reasoning, knowledge base completion and relational information retrieval. A simple, yet surprisingly accurate method for representing a relation between two words is to compute the vector offset (\\\\PairDiff) between their cor\\n',\n"," ' Object-oriented Neural Programming (OONP) for Document Understanding ; We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure (referred to as ontology in this paper) that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the doc\\n',\n"," ' A Neural Comprehensive Ranker (NCR) for Open-Domain Question Answering ; This paper proposes a novel neural machine reading model for open-domain question answering at scale. Existing machine comprehension models typically assume that a short piece of relevant text containing answers is already identified and given to the models, from which the models are designed to extract answers. This assumption, however, is not realistic for building a large-scale open-domain question answering system which requires bo\\n',\n"," ' Improving speech recognition by revising gated recurrent units ; Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplica\\n',\n"," ' Integrating planning for task-completion dialogue policy learning ; Training a task-completion dialogue agent with real users via reinforcement learning (RL) could be prohibitively expensive, because it requires many interactions with users. One alternative is to resort to a user simulator, while the discrepancy of between simulated and real users makes the learned policy unreliable in practice. This paper addresses these challenges by integrating planning into the dialogue policy learning based on Dyna-Q f\\n',\n"," ' Building DNN Acoustic Models for Large Vocabulary Speech Recognition ; Deep neural networks (DNNs) are now a central component of nearly all state-of-the-art speech recognition systems. Building neural network acoustic models requires several design decisions including network architecture, size, and training loss function. This paper offers an empirical investigation on which aspects of DNN acoustic model design are most important for speech recognition system performance. We report DNN classifier performa\\n',\n"," ' Deep Recurrent Neural Networks for Acoustic Modelling ; We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, a\\n',\n"," \" Regularizing RNNs by Stabilizing Activations ; We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms.   This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout.   We achieve competitive performance (18.6\\\\% PER) on the TIMIT phoneme recognition task for RNNs evaluated witho\\n\",\n"," ' Outrageously Large Neural Networks: The Sparsely-Gated   Mixture-of-Experts Layer ; The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these chal\\n',\n"," ' Discourse-Based Objectives for Fast Unsupervised Sentence Representation   Learning ; This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.\\n',\n"," ' Learning Convolutional Text Representations for Visual Question   Answering ; Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification\\n',\n"," ' Learning Phrase Representations using RNN Encoder-Decoder for   Statistical Machine Translation ; In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequ\\n',\n"," ' Recurrent Neural Network Training with Dark Knowledge Transfer ; Recurrent neural networks (RNNs), particularly long short-term memory (LSTM), have gained much attention in automatic speech recognition (ASR). Although some successful stories have been reported, training RNNs remains highly challenging, especially with limited training data. Recent research found that a well-trained model can be used as a teacher to train other child models, by using the predictions generated by the teacher model as supervis\\n',\n"," ' Long Short-Term Memory Based Recurrent Neural Network Architectures for   Large Vocabulary Speech Recognition ; Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting rec\\n',\n"," ' Monitoring Term Drift Based on Semantic Consistency in an Evolving   Vector Field ; Based on the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Falling back on the distributional hypothesis to statistically model word meaning, we used evolving fields as a metaphor to express time-dependent changes in a vector space model by a combination of random indexing and evolving self-organizing maps (ESOM).\\n',\n"," ' Towards better decoding and language model integration in sequence to   sequence models ; The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in an end-to-end fashion. In this contribution, we analyse an attention-based seq2seq speech recognition system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence in\\n',\n"," ' Neural Machine Translation by Jointly Learning to Align and Translate ; Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence\\n',\n"," ' Overcoming the Curse of Sentence Length for Neural Machine Translation   using Automatic Segmentation ; The authors of (Cho et al., 2014a) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences, unlike existing phrase-based translation systems. In this paper, we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural netw\\n',\n"," ' Transferring Knowledge from a RNN to a DNN ; Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to tr\\n',\n"," ' Correlational Neural Networks ; Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizin\\n',\n"," ' Attention-Based Models for Speech Recognition ; Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks in- cluding machine translation, handwriting synthesis and image caption gen- eration. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phone\\n',\n"," ' Fast and Accurate Recurrent Neural Network Acoustic Models for Speech   Recognition ; We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist \\n',\n"," ' Listen, Attend and Spell ; We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network p\\n',\n"," ' BlackOut: Speeding up Recurrent Neural Network Language Models With Very   Large Vocabularies ; We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an e\\n',\n"," ' Character-based Neural Machine Translation ; Neural Machine Translation (MT) has reached state-of-the-art results. However, one of the main challenges that neural MT still faces is dealing with very large vocabularies and morphologically rich languages. In this paper, we propose a neural MT system using character-based embeddings in combination with convolutional and highway layers to replace the standard lookup-based word representations. The resulting unlimited-vocabulary and affix-aware source word embed\\n',\n"," ' A Latent Variable Recurrent Neural Network for Discourse Relation   Language Models ; This paper presents a novel latent variable recurrent neural network architecture for jointly modeling sequences of words and (possibly latent) discourse relations between adjacent sentences. A recurrent neural network generates individual words, thus reaping the benefits of discriminatively-trained vector representations. The discourse relations are represented with a latent variable, which can be predicted or marginalize\\n',\n"," ' Multi-task Recurrent Model for Speech and Speaker Recognition ; Although highly correlated, speech and speaker recognition have been regarded as two independent tasks and studied by two communities. This is certainly not the way that people behave: we decipher both speech content and speaker traits at the same time. This paper presents a unified model to perform speech and speaker recognition simultaneously and altogether. The model is based on a unified neural network where the output of one task is fed to\\n',\n"," ' Hierarchical Memory Networks ; Memory networks are neural networks with an explicit memory component that can be both read and written to by the network. The memory is often addressed in a soft way using a softmax function, making end-to-end training with backpropagation possible. However, this is not computationally scalable for applications which require the network to read from extremely large memories. On the other hand, it is well known that hard attention mechanisms based on reinforcement learning are\\n',\n"," ' Sequence-to-Sequence Learning as Beam-Search Optimization ; Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to lea\\n',\n"," ' Grounded Recurrent Neural Networks ; In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process \"grounding\"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in c\\n',\n"," ' Latent Intention Dialogue Models ; Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention\\n',\n"," ' Transfer Learning for Speech Recognition on a Budget ; End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network originally trained for English ASR to the German language. We show that this technique allows faste\\n',\n"," ' Optimizing expected word error rate via sampling for speech recognition ; State-level minimum Bayes risk (sMBR) training has become the de facto standard for sequence-level training of speech recognition acoustic models. It has an elegant formulation using the expectation semiring, and gives large improvements in word error rate (WER) over models trained solely using cross-entropy (CE) or connectionist temporal classification (CTC). sMBR training optimizes the expected number of frames at which the referenc\\n',\n"," ' Neural Networks Compression for Language Modeling ; In this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (RNNs). It is known that conventional RNNs, e.g, LSTM-based networks in language modeling, are characterized with either high space complexity or substantial inference time. This problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. By using the Penn Treeba\\n',\n"," \" Avoiding Your Teacher's Mistakes: Training Neural Networks with   Controlled Weak Supervision ; Training deep neural networks requires massive amounts of training data, but for many tasks only limited labeled data is available. This makes weak supervision attractive, using weak or noisy signals like the output of heuristic methods or user click-through data for training. In a semi-supervised setting, we can use a large set of data with weak labels to pretrain a neural network and then fine-tune the paramete\\n\",\n"," ' Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy   Optimisation ; In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Proces\\n',\n"," ' On Extended Long Short-term Memory and Dependent Bidirectional Recurrent   Neural Network ; In this work, we investigate the memory capability of recurrent neural networks (RNNs), where this capability is defined as a function that maps an element in a sequence to the current output. We first analyze the system function of a recurrent neural network (RNN) cell, and provide analytical results for three RNNs. They are the simple recurrent neural network (SRN), the long short-term memory (LSTM), and the gated \\n',\n"," ' Learning to Answer Questions From Image Using Convolutional Neural   Network ; In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CN\\n',\n"," ' Stacked Attention Networks for Image Question Answering ; This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer. We argue that image question answering (QA) often requires multiple steps of reasoning. Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively. Experiment\\n',\n"," ' Neural Module Networks ; Visual question answering is fundamentally compositional in nature---a question like \"where is the dog?\" shares substructure with questions like \"what color is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural \"modules\" into\\n',\n"," ' Symbol Grounding Association in Multimodal Sequences with Missing   Elements ; In this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. The general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. In other words, two different representations of the same abstract concepts can associate in both directions. This scenario has been long interested in Artificial Intelligence\\n',\n"," ' Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe   Noise ; The growing importance of massive datasets with the advent of deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling for large datasets, non-expert labeling, and label corruption by data poisoning adversaries. In the latter case, corruptions may be arbitrarily bad, even so bad that a classifier predicts the wrong labels with high confidence. \\n',\n"," ' Describing Multimedia Content using Attention-based Encoder--Decoder   Networks ; Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that l\\n',\n"," ' Multilingual Image Description with Neural Sequence Models ; In this paper we present an approach to multi-language image description bringing together insights from neural machine translation and neural image description. To create a description of an image for a given target language, our sequence generation models condition on feature vectors from the image, the description from the source language, and/or a multimodal vector computed over the image and a description in the source language. In image desc\\n',\n"," ' Deep Embedding for Spatial Role Labeling ; This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is ap\\n',\n"," ' Image-to-Markup Generation with Coarse-to-Fine Attention ; We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our \\n',\n"," ' Teaching Machines to Code: Neural Markup Generation with Visual   Attention ; We present a deep recurrent neural network model with soft visual attention that learns to generate LaTeX markup of real-world math formulas given their images. Applying neural sequence generation techniques that have been very successful in the fields of machine translation and image/handwriting/speech captioning, recognition, transcription and synthesis, we construct an image-to-markup model that learns to produce syntactically \\n',\n"," ' Evolution in Groups: A deeper look at synaptic cluster driven evolution   of deep neural networks ; A promising paradigm for achieving highly efficient deep neural networks is the idea of evolutionary deep intelligence, which mimics biological evolution processes to progressively synthesize more efficient networks. A crucial design factor in evolutionary deep intelligence is the genetic encoding scheme used to simulate heredity and determine the architectures of offspring networks. In this study, we take a \\n',\n"," ' Mesh Learning for Classifying Cognitive Processes ; A relatively recent advance in cognitive neuroscience has been multi-voxel pattern analysis (MVPA), which enables researchers to decode brain states and/or the type of information represented in the brain during a cognitive operation. MVPA methods utilize machine learning algorithms to distinguish among types of information or cognitive states represented in the brain, based on distributed patterns of neural activity. In the current investigation, we propo\\n',\n"," ' Synthesizing Deep Neural Network Architectures using Biological Synaptic   Strength Distributions ; In this work, we perform an exploratory study on synthesizing deep neural networks using biological synaptic strength distributions, and the potential influence of different distributions on modelling performance particularly for the scenario associated with small data sets. Surprisingly, a CNN with convolutional layer synaptic strengths drawn from biologically-inspired distributions such as log-normal or cor\\n',\n"," ' A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters   Optimization ; Addressing the issue of SVMs parameters optimization, this study proposes an efficient memetic algorithm based on Particle Swarm Optimization algorithm (PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is responsible for exploration of the search space and the detection of the potential regions with optimum solutions, while pattern search (PS) is used to produce an effective exploitation on the potentia\\n',\n"," ' Density estimation using Real NVP ; Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation\\n',\n"," ' Evolution Strategies as a Scalable Alternative to Reinforcement Learning ; We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to com\\n',\n"," ' QMDP-Net: Deep Learning for Planning under Partial Observability ; This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. T\\n',\n"," ' TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep   Reinforcement Learning ; Combining deep model-free reinforcement learning with on-line planning is a promising approach to building on the successes of deep RL. On-line planning with look-ahead trees has proven successful in environments where transition models are known a priori. However, in complex environments where transition models need to be learned from data, the deficiencies of learned models have limited their utility for planning.\\n',\n"," ' Sparse Attentive Backtracking: Long-Range Credit Assignment in Recurrent   Networks ; A major drawback of backpropagation through time (BPTT) is the difficulty of learning long-term dependencies, coming from having to propagate credit information backwards through every single step of the forward computation. This makes BPTT both computationally impractical and biologically implausible. For this reason, full backpropagation through time is rarely used on long sequences, and truncated backpropagation through\\n',\n"," ' Stochastic Deep Learning in Memristive Networks ; We study the performance of stochastically trained deep neural networks (DNNs) whose synaptic weights are implemented using emerging memristive devices that exhibit limited dynamic range, resolution, and variability in their programming characteristics. We show that a key device parameter to optimize the learning efficiency of DNNs is the variability in its programming characteristics. DNNs with such memristive synapses, even with dynamic range as low as $15\\n',\n"," ' PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction ; Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and\\n',\n"," ' Norm-Based Capacity Control in Neural Networks ; We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.\\n',\n"," ' Improving the Performance of Neural Networks in Regression Tasks Using   Drawering ; The method presented extends a given regression neural network to make its performance improve. The modification affects the learning procedure only, hence the extension may be easily omitted during evaluation without any change in prediction. It means that the modified model may be evaluated as quickly as the original one but tends to perform better.   This improvement is possible because the modification gives better expr\\n',\n"," ' Learning unbiased features ; A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irreleva\\n',\n"," ' Compatible Value Gradients for Reinforcement Learning of Continuous Deep   Policies ; This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, an\\n',\n"," ' Learning dynamic Boltzmann machines with spike-timing dependent   plasticity ; We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the \\n',\n"," ' Gated Graph Sequence Neural Networks ; Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class \\n',\n"," ' Deep Reinforcement Learning in Large Discrete Action Spaces ; Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear \\n',\n"," \" Value Iteration Networks ; We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.\\n\",\n"," ' Recurrent Orthogonal Networks and Long-Memory Tasks ; Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN so\\n',\n"," ' Learning values across many orders of magnitude ; Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range.\\n',\n"," ' Genetic Architect: Discovering Genomic Structure with Learned Neural   Architectures ; Each human genome is a 3 billion base pair set of encoding instructions. Decoding the genome using deep learning fundamentally differs from most tasks, as we do not know the full structure of the data and therefore cannot design architectures to suit it. As such, architectures that fit the structure of genomics should be learned not prescribed. Here, we develop a novel search algorithm, applicable across domains, that dis\\n',\n"," ' Deep Successor Reinforcement Learning ; Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The val\\n',\n"," ' RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning ; Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a \"fast\" reinforcement learning algorithm, we propose to represent it as a recurrent neural \\n',\n"," ' Capacity and Trainability in Recurrent Neural Networks ; Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information \\n',\n"," ' Causal Regularization ; In application domains such as healthcare, we want accurate predictive models that are also causally interpretable. In pursuit of such models, we propose a causal regularizer to steer predictive models towards causally-interpretable solutions and theoretically study its properties. In a large-scale analysis of Electronic Health Records (EHR), our causally-regularized model outperforms its L1-regularized counterpart in causal accuracy and is competitive in predictive performance. We p\\n',\n"," ' On the Behavior of Convolutional Nets for Feature Extraction ; Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions\\n',\n"," ' Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in   Generative Models ; Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a\\n',\n"," \" Filtering Variational Objectives ; When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. W\\n\",\n"," ' Kernel Implicit Variational Inference ; Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approac\\n',\n"," ' Non-Markovian Control with Gated End-to-End Memory Policy Networks ; Partially observable environments present an important open challenge in the domain of sequential control learning with delayed rewards. Despite numerous attempts during the two last decades, the majority of reinforcement learning algorithms and associated approximate models, applied to this context, still assume Markovian state transitions. In this paper, we explore the use of a recently proposed attention-based model, the Gated End-to-En\\n',\n"," ' Automated Problem Identification: Regression vs Classification via   Evolutionary Deep Networks ; Regression or classification? This is perhaps the most basic question faced when tackling a new supervised learning problem. We present an Evolutionary Deep Learning (EDL) algorithm that automatically solves this by identifying the question type with high accuracy, along with a proposed deep architecture. Typically, a significant amount of human insight and preparation is required prior to executing machine lea\\n',\n"," ' A Simple Neural Attentive Meta-Learner ; Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning appr\\n',\n"," ' Kafnets: kernel-based non-parametric activation functions for neural   networks ; Neural networks are generally built by interleaving (adaptable) linear layers with (fixed) nonlinear activation functions. To increase their flexibility, several authors have proposed methods for adapting the activation functions themselves, endowing them with varying degrees of flexibility. None of these approaches, however, have gained wide acceptance in practice, and research in this topic remains open. In this paper, we in\\n',\n"," ' Learning model-based planning from scratch ; Conventional wisdom holds that model-based planning is a powerful approach to sequential decision-making. It is often very challenging in practice, however, because while a model can be used to evaluate a plan, it does not prescribe how to construct a plan. Here we introduce the \"Imagination-based Planner\", the first model-based, sequential decision-making agent that can learn to construct, evaluate, and execute plans. Before any action, it can perform a variable\\n',\n"," ' Recurrent Ladder Networks ; We propose a recurrent extension of the Ladder networks whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher \\n',\n"," ' Generalization in Deep Learning ; With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, comple\\n',\n"," ' Parametrizing filters of a CNN with a GAN ; It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to paramet\\n',\n"," ' Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence   Learning ; Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensor\\n',\n"," ' Learning and Real-time Classification of Hand-written Digits With   Spiking Neural Networks ; We describe a novel spiking neural network (SNN) for automated, real-time handwritten digit classification and its implementation on a GP-GPU platform. Information processing within the network, from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. The feature extraction layer of the SNN uses fixed synaptic weight maps to e\\n',\n"," \" Overcoming catastrophic forgetting with hard attention to the task ; Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently \\n\",\n"," ' Detecting and Correcting for Label Shift with Black Box Predictors ; Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE\\n',\n"," ' Generalization in Machine Learning via Analytical Learning Theory ; This paper introduces a novel measure-theoretic learning theory to analyze generalization behaviors of practical interest. The proposed learning theory has the following abilities: 1) to utilize the qualities of each learned representation on the path from raw inputs to outputs in representation learning, 2) to guarantee good generalization errors possibly with arbitrarily rich hypothesis spaces (e.g., arbitrarily large capacity and Rademac\\n',\n"," ' Sensitivity and Generalization in Neural Networks: an Empirical Study ; In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to in\\n',\n"," ' On the importance of single directions for generalization ; Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has\\n',\n"," ' Maximin affinity learning of image segmentation ; Images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation. Machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates. However, this error measure is only indirectly related to the quality of segmentations produc\\n',\n"," ' A General Framework for Development of the Cortex-like Visual Object   Recognition System: Waves of Spikes, Predictive Coding and Universal   Dictionary of Features ; This study is focused on the development of the cortex-like visual object recognition system. We propose a general framework, which consists of three hierarchical levels (modules). These modules functionally correspond to the V1, V4 and IT areas. Both bottom-up and top-down connections between the hierarchical levels V4 and IT are employed. Th\\n',\n"," ' Handwritten Digit Recognition with a Committee of Deep Neural Nets on   GPUs ; The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another \\n',\n"," ' Eclectic Extraction of Propositional Rules from Neural Networks ; Artificial Neural Network is among the most popular algorithm for supervised learning. However, Neural Networks have a well-known drawback of being a \"Black Box\" learner that is not comprehensible to the Users. This lack of transparency makes it unsuitable for many high risk tasks such as medical diagnosis that requires a rational justification for making a decision. Rule Extraction methods attempt to curb this limitation by extracting compre\\n',\n"," ' Message Passing Multi-Agent GANs ; Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that \\n',\n"," ' Mode Regularized Generative Adversarial Networks ; Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the dat\\n',\n"," ' Layer-Specific Adaptive Learning Rates for Deep Networks ; The increasing complexity of deep learning architectures is resulting in training time requiring weeks or even months. This slow training is due in part to vanishing gradients, in which the gradients used by back-propagation are extremely large for weights connecting deep layers (layers near the output layer), and extremely small for shallow layers (near the input layer); this results in slow learning in the shallow layers. Additionally, it has also\\n',\n"," ' Return of Frustratingly Easy Domain Adaptation ; Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being \"frustratingly easy\" to implement. However, in p\\n',\n"," ' Origami: A 803 GOp/s/W Convolutional Network Accelerator ; An ever increasing number of computer vision and image/video processing challenges are being approached using deep convolutional neural networks, obtaining state-of-the-art results in object recognition and detection, semantic segmentation, action recognition, optical flow and superresolution. Hardware acceleration of these algorithms is essential to adopt these improvements in embedded and mobile computer vision systems. We present a new architectu\\n',\n"," ' Option Discovery in Hierarchical Reinforcement Learning using   Spatio-Temporal Clustering ; This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states. Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms. These structures also help to generalize such algorithms over \\n',\n"," ' Residual Networks Behave Like Ensembles of Relatively Shallow Networks ; In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further\\n',\n"," ' Synthesizing the preferred inputs for neurons in neural networks via   deep generator networks ; Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right - similar to why we study the human brain - and will enable researchers to further improve DNNs. One path to understanding how a neur\\n',\n"," ' Structured Convolution Matrices for Energy-efficient Deep learning ; We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train b\\n',\n"," ' Deep CORAL: Correlation Alignment for Deep Domain Adaptation ; Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a \"frustratingly easy\" u\\n',\n"," ' Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition ; 3D action recognition - analysis of human actions based on 3D skeleton data - becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information with\\n',\n"," ' Generalized Dropout ; Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with t\\n',\n"," ' Parsimonious Inference on Convolutional Neural Networks: Learning and   applying on-line kernel activation rules ; A new, radical CNN design approach is presented in this paper, considering the reduction of the total computational load during inference. This is achieved by a new holistic intervention on both the CNN architecture and the training procedure, which targets to the parsimonious inference by learning to exploit or remove the redundant capacity of a CNN architecture. This is accomplished, by the i\\n',\n"," ' Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ; We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In ou\\n',\n"," ' WRPN: Training and Inference using Wide Reduced-Precision Networks ; For computer vision applications, prior works have shown the efficacy of reducing the numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision o\\n',\n"," ' Deep Learning is Robust to Massive Label Noise ; Deep neural networks trained on large supervised datasets have led to impressive results in image classification and other tasks. However, well-annotated datasets can be time-consuming and expensive to collect, lending increased interest to larger but noisy datasets that are more easily obtained. In this paper, we show that deep neural networks are capable of generalizing from training data for which true labels are massively outnumbered by incorrect labels. \\n',\n"," ' Improving Content-Invariance in Gated Autoencoders for 2D and 3D Object   Rotation ; Content-invariance in mapping codes learned by GAEs is a useful feature for various relation learning tasks. In this paper we show that the content-invariance of mapping codes for images of 2D and 3D rotated objects can be substantially improved by extending the standard GAE loss (symmetric reconstruction error) with a regularization term that penalizes the symmetric cross-reconstruction error. This error term involves reco\\n',\n"," ' Deep Learning for Sensor-based Activity Recognition: A Survey ; Sensor-based activity recognition seeks the profound high-level knowledge about human activities from multitudes of low-level sensor readings. Conventional pattern recognition approaches have made tremendous progress in the past years. However, those methods often heavily rely on heuristic hand-crafted feature extraction, which could hinder their generalization performance. Additionally, existing methods are undermined for unsupervised and incr\\n',\n"," ' On the Importance of Consistency in Training Deep Neural Networks ; We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residu\\n',\n"," ' UI-Net: Interactive Artificial Neural Networks for Iterative Image   Segmentation Based on a User Model ; For complex segmentation tasks, fully automatic systems are inherently limited in their achievable accuracy for extracting relevant objects. Especially in cases where only few data sets need to be processed for a highly accurate result, semi-automatic segmentation techniques exhibit a clear benefit for the user. One area of application is medical image processing during an intervention for a single pati\\n',\n"," ' Lightweight Neural Networks ; Most of the weights in a Lightweight Neural Network have a value of zero, while the remaining ones are either +1 or -1. These universal approximators require approximately 1.1 bits/weight of storage, posses a quick forward pass and achieve classification accuracies similar to conventional continuous-weight networks. Their training regimen focuses on error reduction initially, but later emphasizes discretization of weights. They ignore insignificant inputs, remove unnecessary we\\n',\n"," ' Tensor Field Networks: Rotation- and Translation-Equivariant Neural   Networks for 3D Point Clouds ; We introduce tensor field networks, which are locally equivariant to 3D rotations, translations, and permutations of points at every layer. 3D rotation equivariance removes the need for data augmentation to identify features in arbitrary orientations. Our network uses filters built from spherical harmonics; due to the mathematical consequences of this filter choice, each layer accepts as input (and guarantee\\n',\n"," ' Knowledge Matters: Importance of Prior Information for Optimization ; We explore the effect of introducing prior information into the intermediate level of neural networks for a learning task on which all the state-of-the-art machine learning algorithms tested failed to learn. We motivate our work from the hypothesis that humans learn such intermediate concepts from other individuals via a form of supervision or guidance using a curriculum. The experiments we have conducted provide positive evidence in favo\\n',\n"," ' Zero-bias autoencoders and the benefits of co-adapting features ; Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We als\\n',\n"," ' Theory and Tools for the Conversion of Analog to Spiking Convolutional   Neural Networks ; Deep convolutional neural networks (CNNs) have shown great potential for numerous real-world machine learning applications, but performing inference in large CNNs in real-time remains a challenge. We have previously demonstrated that traditional CNNs can be converted into deep spiking neural networks (SNNs), which exhibit similar accuracy while reducing both latency and computational load as a consequence of their dat\\n',\n"," ' Stacked Generative Adversarial Networks ; In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the g\\n',\n"," ' Self-informed neural network structure learning ; We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning \\n',\n"," ' Learning Activation Functions to Improve Deep Neural Networks ; Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR\\n',\n"," ' Denoising autoencoder with modulated lateral connections learns   invariant representations of natural images ; Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed\\n',\n"," ' A Probabilistic Theory of Deep Learning ; A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high\\n',\n"," ' Integrated Inference and Learning of Neural Factors in Structural   Support Vector Machines ; Tackling pattern recognition problems in areas such as computer vision, bioinformatics, speech or text recognition is often done best by taking into account task-specific statistical relations between output variables. In structured prediction, this internal structure is used to predict multiple outputs simultaneously, leading to more accurate and coherent predictions. Structural support vector machines (SSVMs) are\\n',\n"," ' What Happened to My Dog in That Network: Unraveling Top-down Generators   in Convolutional Neural Networks ; Top-down information plays a central role in human perception, but plays relatively little role in many current state-of-the-art deep networks, such as Convolutional Neural Networks (CNNs). This work seeks to explore a path by which top-down information can have a direct impact within current deep networks. We explore this path by learning and using \"generators\" corresponding to the network internal \\n',\n"," ' Virtual Worlds as Proxy for Multi-Object Tracking Analysis ; Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual KITTI (see http://www.xrce.xer\\n',\n"," ' Synthesizing Dynamic Patterns by Spatial-Temporal Generative ConvNet ; Video sequences contain rich dynamic patterns, such as dynamic texture patterns that exhibit stationarity in the temporal domain, and action patterns that are non-stationary in either spatial or temporal domain. We show that a spatial-temporal generative ConvNet can be used to model and synthesize dynamic patterns. The model defines a probability distribution on the video sequence, and the log probability is defined by a spatial-temporal\\n',\n"," ' Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural   Networks ; Taking inspiration from biological evolution, we explore the idea of \"Can deep neural networks evolve naturally over successive generations into highly efficient deep neural networks?\" by introducing the notion of synthesizing new highly efficient, yet powerful deep neural networks over successive generations via an evolutionary process from ancestor deep neural networks. The architectural traits of ancestor deep neural networks a\\n',\n"," ' Alternating Back-Propagation for Generator Network ; This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dyn\\n',\n"," ' Hyperparameter Transfer Learning through Surrogate Alignment for   Efficient Deep Neural Network Training ; Recently, several optimization methods have been successfully applied to the hyperparameter optimization of deep neural networks (DNNs). The methods work by modeling the joint distribution of hyperparameter values and corresponding error. Those methods become less practical when applied to modern DNNs whose training may take a few days and thus one cannot collect sufficient observations to accurately \\n',\n"," ' Towards Bayesian Deep Learning: A Framework and Some Existing Methods ; While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, the subsequent tasks that involve inference, reasoning and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are stil\\n',\n"," ' Deciding How to Decide: Dynamic Routing in Artificial Neural Networks ; We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to pro\\n',\n"," ' Pixel Deconvolutional Networks ; Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel d\\n',\n"," ' Gaussian Prototypical Networks for Few-Shot Learning on Omniglot ; We propose a novel architecture for $k$-shot classification on the Omniglot dataset. Building on prototypical networks, we extend their architecture to what we call Gaussian prototypical networks. Prototypical networks learn a map between images and embedding vectors, and use their clustering for classification. In our model, a part of the encoder output is interpreted as a confidence region estimate about the embedding point, and expressed \\n',\n"," ' Super-Convergence: Very Fast Training of Residual Networks Using Large   Learning Rates ; In this paper, we show a phenomenon, which we named \"super-convergence\", where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. \\n',\n"," ' Generative learning for deep networks ; Learning, taking into account full distribution of the data, referred to as generative, is not feasible with deep neural networks (DNNs) because they model only the conditional distribution of the outputs given the inputs. Current solutions are either based on joint probability models facing difficult estimation problems or learn two separate networks, mapping inputs to outputs (recognition) and vice-versa (generation). We propose an intermediate approach. First, we s\\n',\n"," ' Hierarchical Representations for Efficient Architecture Search ; We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that \\n',\n"," ' Data Augmentation Generative Adversarial Networks ; Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model\\n',\n"," ' DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the   Jigsaw Puzzle Problem ; This paper introduces the first deep neural network-based estimation metric for the jigsaw puzzle problem. Given two puzzle piece edges, the neural network predicts whether or not they should be adjacent in the correct assembly of the puzzle, using nothing but the pixels of each piece. The proposed metric exhibits an extremely high precision even though no manual feature extraction is performed. When incorporated in\\n',\n"," ' DeepPainter: Painter Classification Using Deep Convolutional   Autoencoders ; In this paper we describe the problem of painter classification, and propose a novel approach based on deep convolutional autoencoder neural networks. While previous approaches relied on image processing and manual feature extraction from paintings, our approach operates on the raw pixel level, without any preprocessing or manual feature extraction. We first train a deep convolutional autoencoder on a dataset of paintings, and sub\\n',\n"," ' DeepBrain: Functional Representation of Neural In-Situ Hybridization   Images for Gene Ontology Classification Using Deep Convolutional Autoencoders ; This paper presents a novel deep learning-based method for learning a functional representation of mammalian neural images. The method uses a deep convolutional denoising autoencoder (CDAE) for generating an invariant, compact representation of in situ hybridization (ISH) images. While most existing methods for bio-imaging analysis were not developed to handl\\n',\n"," ' Generative Adversarial Perturbations ; In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impr\\n',\n"," ' A Rotation and a Translation Suffice: Fooling CNNs with Simple   Transformations ; We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few blac\\n',\n"," ' Peephole: Predicting Network Performance Before Training ; The quest for performant networks has been a significant force that drives the advancements of deep learning in recent years. While rewarding, improving network design has never been an easy journey. The large design space combined with the tremendous cost required for network training poses a major obstacle to this endeavor. In this work, we propose a new approach to this problem, namely, predicting the performance of a network before training, bas\\n',\n"," ' An Architecture Combining Convolutional Neural Network (CNN) and Support   Vector Machine (SVM) for Image Classification ; Convolutional neural networks (CNNs) are similar to \"ordinary\" neural networks in the sense that they are made up of hidden layers consisting of neurons with \"learnable\" parameters. These neurons receive inputs, performs a dot product, and then follows it with a non-linearity. The whole network expresses the mapping between raw image pixels and their class scores. Conventionally, the So\\n',\n"," ' Benchmarking Decoupled Neural Interfaces with Synthetic Gradients ; Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning, that is \"neurons that wire together, fire together\". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible s\\n',\n"," ' Segmentation hiérarchique faiblement supervisée ; Image segmentation is the process of partitioning an image into a set of meaningful regions according to some criteria. Hierarchical segmentation has emerged as a major trend in this regard as it favors the emergence of important regions at different scales. On the other hand, many methods allow us to have prior information on the position of structures of interest in the images. In this paper, we present a versatile hierarchical segmentation method that tak\\n',\n"," ' Training wide residual networks for deployment using a single bit for   each weight ; For fast and energy-efficient deployment of trained deep neural networks on resource-constrained embedded hardware, each learned weight parameter should ideally be represented and stored using a single bit. Error-rates usually increase when this requirement is imposed. Here, we report large improvements in error rates on multiple datasets, for deep convolutional neural networks deployed with 1-bit-per-weight. Using wide re\\n',\n"," ' Deep Learning using Rectified Linear Units (ReLU) ; We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a n\\n',\n"," ' Rectified Factor Networks ; We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterio\\n',\n"," ' From Maxout to Channel-Out: Encoding Information on Sparse Pathways ; Motivated by an important insight from neural science, we propose a new framework for understanding the success of the recently proposed \"maxout\" networks. The framework is based on encoding information on sparse pathways and recognizing the correct pathway at inference time. Elaborating further on this insight, we propose a novel deep network architecture, called \"channel-out\" network, which takes a much better advantage of sparse pathwa\\n',\n"," ' Competitive Learning with Feedforward Supervisory Signal for Pre-trained   Multilayered Networks ; We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer for robust leaning and revising internal representation in a multilayer neural network.\\n',\n"," ' Deeply-Supervised Nets ; Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of \\n',\n"," ' Path-SGD: Path-Normalized Optimization in Deep Neural Networks ; We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to\\n',\n"," ' Adapting Resilient Propagation for Deep Learning ; The Resilient Propagation (Rprop) algorithm has been very popular for backpropagation training of multilayer feed-forward neural networks in various applications. The standard Rprop however encounters difficulties in the context of deep neural networks as typically happens with gradient-based learning algorithms. In this paper, we propose a modification of the Rprop that combines standard Rprop steps with a special drop out technique. We apply the method fo\\n',\n"," ' Convolutional Neural Network for Stereotypical Motor Movement Detection   in Autism ; Autism Spectrum Disorders (ASDs) are often associated with specific atypical postural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have a specific visibility. While the identification and the quantification of SMM patterns remain complex, its automation would provide support to accurate tuning of the intervention in the therapy of autism. Therefore, it is essential to develop automatic SMM detection sy\\n',\n"," ' Resnet in Resnet: Generalizing Residual Architectures ; Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.\\n',\n"," ' Evolutionary Synthesis of Deep Neural Networks via Synaptic   Cluster-driven Genetic Encoding ; There has been significant recent interest towards achieving highly efficient deep neural network architectures. A promising paradigm for achieving this is the concept of evolutionary deep intelligence, which attempts to mimic biological evolution processes to synthesize highly-efficient deep neural networks over successive generations. An important aspect of evolutionary deep intelligence is the genetic encoding\\n',\n"," ' Neural Photo Editing with Introspective Adversarial Networks ; The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial \\n',\n"," ' Adaptive Neural Networks for Efficient Inference ; We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples co\\n',\n"," ' Spatial Variational Auto-Encoding via Matrix-Variate Normal   Distributions ; The key idea of variational auto-encoders (VAEs) resembles that of traditional auto-encoder models in which spatial information is supposed to be explicitly encoded in the latent space. However, the latent variables in VAEs are vectors, which are commonly interpreted as multiple feature maps of size 1x1. Such representations can only convey spatial information implicitly when coupled with powerful decoders. In this work, we propos\\n',\n"," ' Dense Transformer Networks ; The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair\\n',\n"," ' Progressive Learning for Systematic Design of Large Neural Networks ; We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropri\\n',\n"," ' A Classification-Based Perspective on GAN Distributions ; A fundamental, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether GANs are actually able to capture the key characteristics of the datasets they are trained on. The current approaches to examining this issue require significant human supervision, such as visual inspection of sampled images, and often offer only fairly limited scalability. In this paper, we propose new techniques that employ a cl\\n',\n"," ' Learning Visual Reasoning Without Strong Priors ; Achieving artificial visual reasoning - the ability to answer image-related questions which require a multi-step, high-level process - is an important step towards artificial general intelligence. This multi-modal task requires learning a question-dependent, structured reasoning process over images from language. Standard deep learning approaches tend to exploit biases in the data rather than learn this underlying structure, while leading methods learn to vi\\n',\n"," ' Men Also Like Shopping: Reducing Gender Bias Amplification using   Corpus-level Constraints ; Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are used in these tasks to take advantage of correlations between co-occurring labels and visual input but risk inadvertently encoding social biases found in web corpora. In this work, we study data and models associated with multilabel object classifica\\n',\n"," ' Acquiring Common Sense Spatial Knowledge through Implicit Spatial   Templates ; Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concep\\n',\n"," ' FiLM: Visual Reasoning with a General Conditioning Layer ; We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep \\n',\n"," ' Unsupervised Induction of Semantic Roles within a Reconstruction-Error   Minimization Framework ; We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimat\\n',\n"," ' Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word   Embeddings ; The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises conce\\n',\n"," ' TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency ; In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies ar\\n',\n"," ' Gaussian Attention Model and Its Application to Knowledge Base Embedding   and Question Answering ; We propose the Gaussian attention model for content-based neural memory access. With the proposed attention model, a neural network has the additional degree of freedom to control the focus of its attention from a laser sharp attention to a broad attention. It is applicable whenever we can assume that the distance in the latent space reflects some notion of semantics. We use the proposed attention model as a \\n',\n"," ' Variable Computation in Recurrent Neural Networks ; Recurrent neural networks (RNNs) have been used extensively and with increasing success to model various types of sequential data. Much of this progress has been achieved through devising recurrent units and architectures with the flexibility to capture complex statistics in the data, such as long range dependency or localized attention phenomena. However, while many sequential data (such as video, speech or language) can have highly variable information f\\n',\n"," ' Learning to Learn from Weak Supervision by Full Supervision ; In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magn\\n',\n"," ' SMILES2Vec: An Interpretable General-Purpose Deep Neural Network for   Predicting Chemical Properties ; Chemical databases store information in text representations, and the SMILES format is a universal standard used in many cheminformatics software. Encoded in each SMILES string is structural information that can be used to predict complex chemical properties. In this work, we develop SMILES2vec, a deep RNN that automatically learns features from SMILES to predict chemical properties, without the need for \\n',\n"," ' Sample Efficient Deep Reinforcement Learning for Dialogue Systems with   Large Action Spaces ; In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. A part of this effort is the policy optimisation task, which attempts to find a policy describing how to respond to humans, in the form of a function taking the current state of the dialogue and returning the response of the system. In this paper, we investigate deep reinforcement \\n',\n"," ' High-Dimensional Vector Semantics ; In this paper we explore the \"vector semantics\" problem from the perspective of \"almost orthogonal\" property of high-dimensional random vectors. We show that this intriguing property can be used to \"memorize\" random vectors by simply adding them, and we provide an efficient probabilistic solution to the set membership problem. Also, we discuss several applications to word context vector embeddings, document sentences similarity, and spam filtering.\\n',\n"," ' Learning Semantic Script Knowledge with Event Embeddings ; Induction of common sense knowledge about prototypical sequences of events has recently received much attention. Instead of inducing this knowledge in the form of graphs, as in much of the previous work, in our method, distributed representations of event realizations are computed based on distributed representations of predicates and their arguments, and then these representations are used to predict prototypical event orderings. The parameters of \\n',\n"," ' Mathematical Language Processing: Automatic Grading and Feedback for   Open Response Mathematical Questions ; While computer and communication technologies have provided effective means to scale up many aspects of education, the submission and grading of assessments such as homework assignments and tests remains a weak link. In this paper, we study the problem of automatically grading the kinds of open response mathematical questions that figure prominently in STEM (science, technology, engineering, and mat\\n',\n"," ' Nonparametric Bayesian Double Articulation Analyzer for Direct Language   Acquisition from Continuous Speech Signals ; Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative mode\\n',\n"," ' Harnessing Deep Neural Networks with Logic Rules ; Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the\\n',\n"," ' Toward Controlled Generation of Text ; Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposit\\n',\n"," ' Adversarial Connective-exploiting Networks for Implicit Discourse   Relation Classification ; Implicit discourse relation classification is of great challenge due to the lack of connectives as strong linguistic cues, which motivates the use of annotated implicit connectives to improve the recognition. We propose a feature imitation framework in which an implicit relation network is driven to learn from another neural network with access to connectives, and thus encouraged to extract similarly salient featur\\n',\n"," ' Abstract Syntax Networks for Code Generation and Semantic Parsing ; Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone datase\\n',\n"," ' Multimodal Word Distributions ; Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian\\n',\n"," ' Guiding Reinforcement Learning Exploration Using Natural Language ; In this work we present a technique to use natural language to help reinforcement learning generalize to unseen environments. This technique uses neural machine translation, specifically the use of encoder-decoder networks, to learn associations between natural language behavior descriptions and state-action information. We then use this learned model to guide agent exploration using a modified version of policy shaping to make it more effe\\n',\n"," ' Robust Task Clustering for Deep Many-Task Learning ; We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, th\\n',\n"," ' Natural Language Multitasking: Analyzing and Improving Syntactic   Saliency of Hidden Representations ; We train multi-task autoencoders on linguistic tasks and analyze the learned hidden sentence representations. The representations change significantly when translation and part-of-speech decoders are added. The more decoders a model employs, the better it clusters sentences according to their syntactic similarity, as the representation space becomes less entangled. We explore the structure of the represen\\n',\n"," ' Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement   Learning ; With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal s\\n',\n"," ' A Supervised Approach to Extractive Summarisation of Scientific Papers ; Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new datase\\n',\n"," ' Language Models for Image Captioning: The Quirks and What Works ; Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generat\\n',\n"," ' Exploring Models and Data for Image Question Answering ; This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation \\n',\n"," ' Making the V in VQA Matter: Elevating the Role of Image Understanding in   Visual Question Answering ; Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.   We \\n',\n"," ' A Multi-World Approach to Question Answering about Real-World Scenes   based on Uncertain Input ; We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and repl\\n',\n"," ' Hard to Cheat: A Turing Test based on Answering Questions about Images ; Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \"question answering about images\" as a particular appealing instance of such a holistic task. In particular, we point out that it is a versio\\n',\n"," ' Analyzing the Behavior of Visual Question Answering Models ; Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The performance of most models is clustered around 60-70%. In this paper we propose systematic methods to analyze the behavior of these models as a first step towards recognizing their strengths and weaknesses, and identifying the most fruitful directions for progress. We analyze two models, one each from two major classes of VQA mo\\n',\n"," ' Sort Story: Sorting Jumbled Images and Captions into Stories ; Temporal common sense has applications in AI tasks such as QA, multi-document summarization, and human-AI communication. We propose the task of sequencing -- given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions, and their ensemble-based combinations, achieving s\\n',\n"," \" Mean Box Pooling: A Rich Image Representation and Output Embedding for   the Visual Madlibs Task ; We present Mean Box Pooling, a novel visual representation that pools over CNN representations of a large number, highly overlapping object proposals. We show that such representation together with nCCA, a successful multimodal embedding technique, achieves state-of-the-art performance on the Visual Madlibs task. Moreover, inspired by the nCCA's objective function, we extend classical CNN+LSTM approach to trai\\n\",\n"," ' Learning to generalize to new compositions in image understanding ; Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured r\\n',\n"," ' Measuring Machine Intelligence Through Visual Question Answering ; As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine int\\n',\n"," ' Towards Transparent AI Systems: Interpreting Visual Question Answering   Models ; Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model fo\\n',\n"," ' Visual Dialog ; We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being g\\n',\n"," ' Multi-task Learning Of Deep Neural Networks For Audio Visual Automatic   Speech Recognition ; Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtai\\n',\n"," \" Learning Cooperative Visual Dialog Agents with Deep Reinforcement   Learning ; We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents -- Qbot and Abot -- who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end -- from pixels to multi-agent multi-round dial\\n\",\n"," ' Being Negative but Constructively: Lessons Learnt from Creating Better   Visual Question Answering Datasets ; Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of \\n',\n"," ' C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0   Dataset ; Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown that these models are heavily driven by superficial correlations in the training data and lack compositionality -- the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central \\n',\n"," \" Deep learning evaluation using deep linguistic processing ; We discuss problems with the standard approaches to evaluation for tasks like visual question answering, and argue that artificial data can be used to address these as a complement to current practice. We demonstrate that with the help of existing 'deep' linguistic processing technology we are able to create challenging abstract datasets, which enable us to investigate the language understanding abilities of multimodal deep learning models in detai\\n\",\n"," ' meProp: Sparsified Back Propagation for Accelerated Deep Learning with   Reduced Overfitting ; We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) of the weight\\n',\n"," ' Towards Crafting Text Adversarial Samples ; Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a classifier at hand. An attacker introduces specially crafted adversarial samples to a deployed classifier, which are being mis-classified by the classifier. However, the samples are perceived to be drawn from entirely different classes and thus it becomes hard to detect the adversarial samples. Most of the prior works have been focused on synthesizing adversaria\\n',\n"," ' Reinforced Video Captioning with Entailment Rewards ; Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we pr\\n',\n"," ' Hierarchically-Attentive RNN for Album Summarization and Storytelling ; We address the problem of end-to-end visual storytelling. Given a photo album, our model first selects the most representative (summary) photos, and then composes a natural language story for the album. For this task, we make use of the Visual Storytelling dataset and a model composed of three hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album photos, select representative (summary) photos, and compose the story.\\n',\n"," ' Generating Natural Adversarial Examples ; Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaning\\n',\n"," ' Training Simplification and Model Simplification for Deep Learning: A   Minimal Effort Back Propagation Method ; We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-$k$ elements (in terms of magnitude) are kept. As a result, only $k$ rows or columns (depending on the layout) o\\n',\n"," ' Embodied Question Answering ; We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (\"orange\").   This challenging task requires a range of AI skills -- active perception, language understanding, \\n',\n"," \" Don't Just Assume; Look and Answer: Overcoming Priors for Visual   Question Answering ; A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the \\n\",\n"," \" CoDraw: Visual Dialog for Collaborative Drawing ; In this work, we propose a goal-driven collaborative task that contains vision, language, and action in a virtual environment as its core components. Specifically, we develop a collaborative `Image Drawing' game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. Two players, Teller and Drawer, are involved. The Teller sees an abstract scene containing multiple clip arts in a semantically meaning\\n\",\n"," ' Answerer in Questioner\\'s Mind for Goal-Oriented Visual Dialogue ; Goal-oriented dialogue has been paid attention for its numerous applications in artificial intelligence. To solve this task, deep learning and reinforcement learning have recently been applied. However, these approaches struggle to find a competent recurrent neural questioner, owing to the complexity of learning a series of sentences. Motivated by theory of mind, we propose \"Answerer in Questioner\\'s Mind\" (AQM), a novel algorithm for goal-ori\\n',\n"," ' Resource Constrained Structured Prediction ; We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation s\\n',\n"," ' Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to   Action Sequences ; We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our\\n',\n"," ' Coupling Distributed and Symbolic Execution for Natural Language Queries ; Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end\\n',\n"," ' An agent-driven semantical identifier using radial basis neural networks   and reinforcement learning ; Due to the huge availability of documents in digital form, and the deception possibility raise bound to the essence of digital documents and the way they are spread, the authorship attribution problem has constantly increased its relevance. Nowadays, authorship attribution,for both information retrieval and analysis, has gained great importance in the context of security, trust and copyright preservation.\\n',\n"," ' Where is my forearm? Clustering of body parts from simultaneous tactile   and linguistic input using sequential mapping ; Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. Ther\\n',\n"," ' Improvements to deep convolutional neural networks for LVCSR ; Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12% relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep an\\n',\n"," ' Collaborative Deep Learning for Recommender Systems ; Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item\\n',\n"," ' Explaining Predictions of Non-Linear Classifiers in NLP ; Layer-wise relevance propagation (LRP) is a recently proposed technique for explaining predictions of complex non-linear classifiers in terms of input variables. In this paper, we apply LRP for the first time to natural language processing (NLP). More precisely, we use it to explain the predictions of a convolutional neural network (CNN) trained on a topic categorization task. Our analysis highlights which words are relevant for a specific prediction\\n',\n"," ' Tensor network language model ; We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of\\n',\n"," ' Language as a matrix product state ; We propose a statistical model for natural language that begins by considering language as a monoid, then representing it in complex matrices with a compatible translation invariant probability measure. We interpret the probability measure as arising via the Born rule from a translation invariant matrix product state.\\n',\n"," ' Accelerating Hessian-free optimization for deep neural networks by   implicit preconditioning and sampling ; Hessian-free training has become a popular parallel second or- der optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction of the number of Krylov subspace solver iterations used for implicit estimation of the Hessian. In this paper, we develop an L-BFG\\n',\n"," ' Is a Picture Worth Ten Thousand Words in a Review Dataset? ; While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-b\\n',\n"," ' Validation of nonlinear PCA ; Linear principal component analysis (PCA) can be extended to a nonlinear PCA by using artificial neural networks. But the benefit of curved components requires a careful control of the model complexity. Moreover, standard techniques for model selection, including cross-validation and more generally the use of an independent test set, fail when applied to nonlinear PCA because of its inherent unsupervised characteristics. This paper presents a new approach for validating the com\\n',\n"," ' Graph Approximation and Clustering on a Budget ; We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters). We also propose a new algorithmic approach based on ada\\n',\n"," ' ShareBoost: Efficient Multiclass Learning with Feature Sharing ; Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that \\n',\n"," ' Functional Principal Component Analysis and Randomized Sparse Clustering   Algorithm for Medical Image Analysis ; Due to advances in sensors, growing large and complex medical image data have the ability to visualize the pathological change in the cellular or even the molecular level or anatomical changes in tissues and organs. As a consequence, the medical images have the potential to enhance diagnosis of disease, prediction of clinical outcomes, characterization of disease progression, management of healt\\n',\n"," ' Jointly Learning Multiple Measures of Similarities from Triplet   Comparisons ; Similarity between objects is multi-faceted and it can be easier for human annotators to measure it when the focus is on a specific aspect. We consider the problem of mapping objects into view-specific embeddings where the distance between them is consistent with the similarity comparisons of the form \"from the t-th view, object A is more similar to B than to C\". Our framework jointly learns view-specific embeddings exploiting c\\n',\n"," ' Variational Inference for Uncertainty on the Inputs of Gaussian Process   Models ; The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational infe\\n',\n"," ' Conditional Generative Adversarial Nets ; Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provid\\n',\n"," ' Visual Causal Feature Learning ; We provide a rigorous definition of the visual cause of a behavior that is broadly applicable to the visually driven behavior in humans, animals, neurons, robots and other perceiving systems. Our framework generalizes standard accounts of causal learning to settings in which the causal variables need to be constructed from micro-variables. We prove the Causal Coarsening Theorem, which allows us to gain causal knowledge from observational data with minimal experimental effort\\n',\n"," ' In Search of the Real Inductive Bias: On the Role of Implicit   Regularization in Deep Learning ; We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.\\n',\n"," ' Domain Generalization for Object Recognition with Multi-task   Autoencoders ; The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.   Our algorithm extends the standard denoising autoencoder framework by subst\\n',\n"," ' Data-Efficient Learning of Feedback Policies from Image Pixels using   Deep Dynamical Models ; Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems. We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (\"torques\") from pixel information only. We introduce a data-efficient, model-based r\\n',\n"," ' Scatter Component Analysis: A Unified Framework for Domain Adaptation   and Domain Generalization ; This paper addresses classification tasks on a particular target domain in which labeled training data are only available from source domains different from (but related to) the target. Two closely related frameworks, domain adaptation and domain generalization, are concerned with such tasks, where the only difference between those frameworks is the availability of the unlabeled target data: domain adaptation\\n',\n"," ' Robust Subspace Clustering via Tighter Rank Approximation ; Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctangent function is used as a tighter approximation to\\n',\n"," ' Recognizing Semantic Features in Faces using Deep Learning ; The human face constantly conveys information, both consciously and subconsciously. However, as basic as it is for humans to visually interpret this information, it is quite a big challenge for machines. Conventional semantic facial feature recognition and analysis techniques are already in use and are based on physiological heuristics, but they suffer from lack of robustness and high computation time. This thesis aims to explore ways for machines\\n',\n"," ' Deep Reconstruction-Classification Networks for Unsupervised Domain   Adaptation ; In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target data.In this way, the learnt\\n',\n"," ' A Convolutional Autoencoder for Multi-Subject fMRI Data Aggregation ; Finding the most effective way to aggregate multi-subject fMRI data is a long-standing and challenging problem. It is of increasing interest in contemporary fMRI studies of human cognition due to the scarcity of data per subject and the variability of brain anatomy and functional response across subjects. Recent work on latent factor models shows promising results in this task but this approach does not preserve spatial locality in the br\\n',\n"," ' Feedback-Controlled Sequential Lasso Screening ; One way to solve lasso problems when the dictionary does not fit into available memory is to first screen the dictionary to remove unneeded features. Prior research has shown that sequential screening methods offer the greatest promise in this endeavor. Most existing work on sequential screening targets the context of tuning parameter selection, where one screens and solves a sequence of $N$ lasso problems with a fixed grid of geometrically spaced regularizat\\n',\n"," \" The Symmetry of a Simple Optimization Problem in Lasso Screening ; Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half s\\n\",\n"," ' Hard Negative Mining for Metric Learning Based Zero-Shot Classification ; Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and a\\n',\n"," ' Pose-Selective Max Pooling for Measuring Similarity ; In this paper, we deal with two challenges for measuring the similarity of the subject identities in practical video-based face recognition - the variation of the head pose in uncontrolled environments and the computational expense of processing videos. Since the frame-wise feature mean is unable to characterize the pose diversity among frames, we define and preserve the overall pose diversity and closeness in a video. Then, identity will be the only sou\\n',\n"," ' Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble   of Autoencoders ; A fall is an abnormal activity that occurs rarely, so it is hard to collect real data for falls. It is, therefore, difficult to use supervised learning methods to automatically detect falls. Another challenge in using machine learning methods to automatically detect falls is the choice of engineered features. In this paper, we propose to use an ensemble of autoencoders to extract features from different channels of \\n',\n"," ' Generalization Error of Invariant Classifiers ; This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-inva\\n',\n"," ' Universal adversarial perturbations ; Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these \\n',\n"," ' Linear Disentangled Representation Learning for Facial Actions ; Limited annotated data available for the recognition of facial expression and action units embarrasses the training of deep networks, which can learn disentangled invariant features. However, a linear model with just several parameters normally is not demanding in terms of training data. In this paper, we propose an elegant linear model to untangle confounding factors in challenging realistic multichannel signals such as 2D face videos. The si\\n',\n"," ' On Detecting Adversarial Perturbations ; Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small \"detector\" subnetwork which is trained on the binary classification task of distinguishing genuine data from dat\\n',\n"," \" Activation Maximization Generative Adversarial Nets ; Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Netw\\n\",\n"," ' Interpretable Explanations of Black Boxes by Meaningful Perturbation ; As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \"look\" in an image for evidence for their predictions. However, these techniques are\\n',\n"," ' A General Theory for Training Learning Machine ; Though the deep learning is pushing the machine learning to a new stage, basic theories of machine learning are still limited. The principle of learning, the role of the a prior knowledge, the role of neuron bias, and the basis for choosing neural transfer function and cost function, etc., are still far from clear. In this paper, we present a general theoretical framework for machine learning. We classify the prior knowledge into common and problem-dependent \\n',\n"," ' A Generalization of Convolutional Neural Networks to Graph-Structured   Data ; This paper introduces a generalization of Convolutional Neural Networks (CNNs) from low-dimensional grid data, such as images, to graph-structured data. We propose a novel spatial convolution utilizing a random walk to uncover the relations within the input, analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. The convolution has an intuitive interpretation, is efficient and scalable\\n',\n"," ' Formal Guarantees on the Robustness of a Classifier against Adversarial   Manipulation ; Recent work has shown that state-of-the-art classifiers are quite brittle, in the sense that a small adversarial change of an originally with high confidence correctly classified input leads to a wrong classification again with high confidence. This raises concerns that such classifiers are vulnerable to attacks and calls into question their usage in safety-critical systems. We show in this paper for the first time form\\n',\n"," ' Classification regions of deep neural networks ; The goal of this paper is to analyze the geometric properties of deep neural network classifiers in the input space. We specifically study the topology of classification regions created by deep networks, as well as their associated decision boundary. Through a systematic empirical investigation, we show that state-of-the-art deep nets learn connected classification regions, and that the decision boundary in the vicinity of datapoints is flat along most direct\\n',\n"," ' Analysis of universal adversarial perturbations ; Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we e\\n',\n"," ' Bayesian GAN ; Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standa\\n',\n"," ' Unsupervised Learning of Disentangled Representations from Video ; We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future \\n',\n"," ' Dualing GANs ; Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its maximin formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the gene\\n',\n"," ' Wavelet Residual Network for Low-Dose CT via Deep Convolutional   Framelets ; Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT are computationally expensive. To address this problem, we recently proposed the world-first deep convolutional neural network (CNN) for low-dose X-ray CT and won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However, some of the texture were not fully recovered. To cope with this problem, here we propose a deep residual learning approach in \\n',\n"," ' 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks ; The success of various applications including robotics, digital content creation, and visualization demand a structured and abstract representation of the 3D world from limited sensor data. Inspired by the nature of human perception of 3D shapes as a collection of simple parts, we explore such an abstract shape representation based on primitives. Given a single depth image of an object, we present 3D-PRNN, a generative recurrent neural ne\\n',\n"," ' Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))   Alternative ; In this article, we mathematically study several GAN related topics, including Inception score, label smoothing, gradient vanishing and the -log(D(x)) alternative.   --- An advanced version is included in arXiv:1703.02000 \"Activation Maximization Generative Adversarial Nets\". Please refer Section 6 in 1703.02000 for detailed analysis on Inception Score, and refer its appendix for the discussions on Label Smoothing, Gradient \\n',\n"," ' A Brief Survey of Deep Reinforcement Learning ; Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots \\n',\n"," ' CirCNN: Accelerating and Compressing Deep Neural Networks Using   Block-CirculantWeight Matrices ; Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregula\\n',\n"," ' XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual   Classification ; We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between the feature extractors, thereby extracting more interpretable features and obtaining a better representation than through unimodal learning, for the same amount of training data. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are therefore nontrivially\\n',\n"," ' Context Embedding Networks ; Low dimensional embeddings that capture the main variations of interest in collections of data are important for many applications. One way to construct these embeddings is to acquire estimates of similarity from the crowd. However, similarity is a multi-dimensional concept that varies from individual to individual. Existing models for learning embeddings from the crowd typically make simplifying assumptions such as all individuals estimate similarity using the same criteria, th\\n',\n"," ' How Much Chemistry Does a Deep Neural Network Need to Know to Make   Accurate Predictions? ; The meteoric rise of deep learning models in computer vision research, having achieved human-level accuracy in image recognition tasks is firm evidence of the impact of representation learning of deep neural networks. In the chemistry domain, recent advances have also led to the development of similar CNN models, such as Chemception, that is trained to predict chemical properties using images of molecular drawings. \\n',\n"," ' Variational Inference of Disentangled Latent Concepts from Unlabeled   Observations ; Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach\\n',\n"," ' Three Factors Influencing Minima in SGD ; We study the properties of the endpoint of stochastic gradient descent (SGD). By approximating SGD as a stochastic differential equation (SDE) we consider the Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption of isotropic variance in loss gradients. Through this analysis, we find that three factors - learning rate, batch size and the variance of the loss gradients - control the trade-off between the depth and width of the minima found by SGD,\\n',\n"," ' Learning to Play Othello with Deep Neural Networks ; Achieving superhuman playing level by AlphaGo corroborated the capabilities of convolutional neural architectures (CNNs) for capturing complex spatial patterns. This result was to a great extent due to several analogies between Go board states and 2D images CNNs have been designed for, in particular translational invariance and a relatively large board. In this paper, we verify whether CNN-based move predictors prove effective for Othello, a game with sig\\n',\n"," ' Deep Learning Can Reverse Photon Migration for Diffuse Optical   Tomography ; Can artificial intelligence (AI) learn complicated non-linear physics? Here we propose a novel deep learning approach that learns non-linear photon scattering physics and obtains accurate 3D distribution of optical anomalies. In contrast to the traditional black-box deep learning approaches to inverse problems, our deep network learns to invert the Lippmann-Schwinger integral equation which describes the essential physics of photo\\n',\n"," ' Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for   Transferable Chemical Property Prediction ; With access to large datasets, deep neural networks (DNN) have achieved human-level accuracy in image and speech recognition tasks. However, in chemistry, data is inherently small and fragmented. In this work, we develop an approach of using rule-based knowledge for training ChemNet, a transferable and generalizable deep neural network for chemical property prediction that learns in a weak-super\\n',\n"," ' Deep Learning in RF Sub-sampled B-mode Ultrasound Imaging ; In portable, three dimensional, and ultra-fast ultrasound (US) imaging systems, there is an increasing need to reconstruct high quality images from a limited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling. However, due to the severe side lobe artifacts from RF sub-sampling, the standard beam-former often produces blurry images with less contrast that are not suitable for diagnostic purpose. To address this problem, some researc\\n',\n"," ' Deep Learning Interior Tomography for Region-of-Interest Reconstruction ; Interior tomography for the region-of-interest (ROI) imaging has advantages of using a small detector and reducing X-ray radiation dose. However, standard analytic reconstruction suffers from severe cupping artifacts due to existence of null space in the truncated Radon transform. Existing penalized reconstruction methods may address this problem but they require extensive computations due to the iterative reconstruction. Inspired by \\n',\n"," ' Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner ; For homeland and transportation security applications, 2D X-ray explosive detection system (EDS) have been widely used, but they have limitations in recognizing 3D shape of the hidden objects. Among various types of 3D computed tomography (CT) systems to address this issue, this paper is interested in a stationary CT using fixed X-ray sources and detectors. However, due to the limited number of projection views, analytic reconstruction\\n',\n"," ' Effective Building Block Design for Deep Convolutional Neural Networks   using Search ; Deep learning has shown promising results on many machine learning tasks but DL models are often complex networks with large number of neurons and layers, and recently, complex layer structures known as building blocks. Finding the best deep model requires a combination of finding both the right architecture and the correct set of parameters appropriate for that architecture. In addition, this complexity (in terms of lay\\n',\n"," ' TVAE: Triplet-Based Variational Autoencoder using Metric Learning ; Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or\\n',\n"," ' Learning to Play with Intrinsically-Motivated Self-Aware Agents ; Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a \"world-m\\n',\n"," ' Emergence of Structured Behaviors from Curiosity-Based Intrinsic   Motivation ; Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent lear\\n',\n"," ' Stochastic Video Generation with a Learned Prior ; Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame\\n',\n"," ' Multi-Evidence Filtering and Fusion for Multi-Label Classification,   Object Detection and Semantic Segmentation Based on Weakly Supervised   Learning ; Supervised object detection and semantic segmentation require object or even pixel level annotations. When there exist image level labels only, it is challenging for weakly supervised algorithms to achieve accurate predictions. The accuracy achieved by top weakly supervised algorithms is still significantly lower than their fully supervised counterparts. In\\n',\n"," ' Neural Networks Should Be Wide Enough to Learn Disconnected Decision   Regions ; In the recent literature the important role of depth in deep learning has been emphasized. In this paper we argue that sufficient width of a feedforward network is equally important by answering the simple question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a pyramidal structure, that is no laye\\n',\n"," \" Visual Explanations From Deep 3D Convolutional Neural Networks for   Alzheimer's Disease Classification ; We develop three efficient approaches for generating visual explanations from 3D convolutional neural networks (3D-CNNs) for Alzheimer's disease classification. One approach conducts sensitivity analysis on hierarchical 3D image segmentation, and the other two visualize network activations on a spatial map. Visual checks and a quantitative localization benchmark indicate that all approaches identify imp\\n\",\n"," ' Averaging Weights Leads to Wider Optima and Better Generalization ; Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, an\\n',\n"," ' SENNS: Sparse Extraction Neural NetworkS for Feature Extraction ; By drawing on ideas from optimisation theory, artificial neural networks (ANN), graph embeddings and sparse representations, I develop a novel technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at addressing the feature extraction problem. The proposed method uses (preferably deep) ANNs for projecting input attribute vectors to an output space wherein pairwise distances are maximized for vectors belonging to different classes,\\n',\n"," ' Generative Models and Model Criticism via Optimized Maximum Mean   Discrepancy ; We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell the\\n',\n"," ' Deep Learning Approximation for Stochastic Control Problems ; Many real world stochastic control problems suffer from the \"curse of dimensionality\". To overcome this difficulty, we develop a deep learning approach that directly solves high-dimensional stochastic control problems based on Monte-Carlo sampling. We approximate the time-dependent controls as feedforward neural networks and stack these networks together through model dynamics. The objective function for the control problem plays the role of the \\n',\n"," ' Generating Focussed Molecule Libraries for Drug Discovery with Recurrent   Neural Networks ; In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the pro\\n',\n"," \" Parameter Space Noise for Exploration ; Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional\\n\",\n"," ' On The Robustness of a Neural Network ; With the development of neural networks based machine learning and their usage in mission critical applications, voices are rising against the \\\\textit{black box} aspect of neural networks as it becomes crucial to understand their limits and capabilities. With the rise of neuromorphic hardware, it is even more critical to understand how a neural network, as a distributed system, tolerates the failures of its computing nodes, neurons, and its communication channels, syn\\n',\n"," ' ZhuSuan: A Library for Bayesian Deep Learning ; In this paper we introduce ZhuSuan, a python probabilistic programming library for Bayesian deep learning, which conjoins the complimentary advantages of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike existing deep learning libraries, which are mainly designed for deterministic neural networks and supervised tasks, ZhuSuan is featured for its deep root into Bayesian inference, thus supporting various kinds of probabilistic models,\\n',\n"," ' Using Parameterized Black-Box Priors to Scale Up Model-Based Policy   Search for Robotics ; The most data-efficient algorithms for reinforcement learning in robotics are model-based policy search algorithms, which alternate between learning a dynamical model of the robot and optimizing a policy to maximize the expected return given the model and its uncertainties. Among the few proposed approaches, the recently introduced Black-DROPS algorithm exploits a black-box optimization algorithm to achieve both high\\n',\n"," ' Bayesian Optimization with Automatic Prior Selection for Data-Efficient   Direct Policy Search ; One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition functi\\n',\n"," ' Bounding and Counting Linear Regions of Deep Neural Networks ; In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly bu\\n',\n"," ' Deep Rewiring: Training very sparse deep networks ; Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automati\\n',\n"," ' Comparing heterogeneous entities using artificial neural networks of   trainable weighted structural components and machine-learned activation   functions ; To compare entities of differing types and structural components, the artificial neural network paradigm was used to cross-compare structural components between heterogeneous documents. Trainable weighted structural components were input into machine-learned activation functions of the neurons. The model was used for matching news articles and videos, w\\n',\n"," ' Active Learning of Inverse Models with Intrinsically Motivated Goal   Exploration in Robots ; We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterize\\n',\n"," \" End-to-End Tracking and Semantic Segmentation Using Recurrent Neural   Networks ; In this work we present a novel end-to-end framework for tracking and classifying a robot's surroundings in complex, dynamic and only partially observable real-world environments. The approach deploys a recurrent neural network to filter an input stream of raw laser measurements in order to directly infer object locations, along with their identity in both visible and occluded areas. To achieve this we first train the network \\n\",\n"," ' Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks ; This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output includin\\n',\n"," ' Deep Predictive Coding Networks for Video Prediction and Unsupervised   Learning ; While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predic\\n',\n"," ' Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient   Convolutional Neural Networks ; This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different\\n',\n"," ' On Convergence and Stability of GANs ; We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibr\\n',\n"," ' Imitation from Observation: Learning to Imitate Behaviors from Raw Video   via Context Translation ; Imitation learning is an effective approach for autonomous systems to acquire control policies when an explicit reward function is unavailable, using supervision provided as demonstrations from an expert, typically a human operator. However, standard imitation learning methods assume that the agent receives examples of observation-action tuples that could be provided, for instance, to a supervised learning a\\n',\n"," ' Convergence rates for pretraining and dropout: Guiding learning   parameters using network structure ; Unsupervised pretraining and dropout have been well studied, especially with respect to regularization and output consistency. However, our understanding about the explicit convergence rates of the parameter estimates, and their dependence on the learning (like denoising and dropout rate) and structural (like depth and layer lengths) aspects of the network is less mature. An interesting question in this co\\n',\n"," ' Learning Discriminative Features via Label Consistent Neural Network ; Deep Convolutional Neural Networks (CNN) enforces supervised information only at the output layer, and hidden layers are trained by back propagating the prediction error from the output layer without explicit supervision. We propose a supervised feature learning approach, Label Consistent Neural Network, which enforces direct supervision in late hidden layers. We associate each neuron in a hidden layer with a particular class label and e\\n',\n"," ' Out-of-Sample Extension for Dimensionality Reduction of Noisy Time   Series ; This paper proposes an out-of-sample extension framework for a global manifold learning algorithm (Isomap) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts. Given a set of noise-free training data and its embedding, the proposed framework extends the embedding for a noisy time series. This is achieved by adding a spatio-temporal compactness term to the optimiz\\n',\n"," ' Adversarial Examples for Semantic Image Segmentation ; Machine learning methods in general and Deep Neural Networks in particular have shown to be vulnerable to adversarial perturbations. So far this phenomenon has mainly been studied in the context of whole-image classification. In this contribution, we analyse how adversarial perturbations can affect the task of semantic segmentation. We show how existing adversarial attackers can be transferred to this task and that it is possible to create imperceptible\\n',\n"," ' Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box   Machine Learning Models ; Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-\\n',\n"," ' Towards Building an Intelligent Anti-Malware System: A Deep Learning   Approach using Support Vector Machine (SVM) for Malware Classification ; Effective and efficient mitigation of malware is a long-time endeavor in the information security community. The development of an anti-malware system that can counteract an unknown malware is a prolific activity that may benefit several sectors. We envision an intelligent anti-malware system that utilizes the power of deep learning (DL) models. Using such models wo\\n',\n"," ' Feature extraction using Latent Dirichlet Allocation and Neural   Networks: A case study on movie synopses ; Feature extraction has gained increasing attention in the field of machine learning, as in order to detect patterns, extract information, or predict future observations from big data, the urge of informative features is crucial. The process of extracting features is highly linked to dimensionality reduction as it implies the transformation of the data from a sparse high-dimensional space, to higher l\\n',\n"," ' A Survey of Available Corpora for Building Data-Driven Dialogue Systems ; During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research\\n',\n"," ' Generative Topic Embedding: a Continuous Representation of Documents   (Extended Version with Proofs) ; Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedd\\n',\n"," ' Fine-Grained Entity Typing with High-Multiplicity Assignments ; As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our m\\n',\n"," ' Towards a Visual Turing Challenge ; As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this go\\n',\n"," ' Interactive Robot Learning of Gestures, Language and Affordances ; A growing field in robotics and Artificial Intelligence (AI) research is human-robot collaboration, whose target is to enable effective teamwork between humans and robots. However, in many situations human teams are still superior to human-robot teams, primarily because human teams can easily agree on a common goal with language, and the individual members observe each other effectively, leveraging their shared motor repertoire and sensorimo\\n',\n"," ' Visual Features for Context-Aware Speech Recognition ; Automatic transcriptions of consumer-generated multi-media content such as \"Youtube\" videos still exhibit high word error rates. Such data typically occupies a very broad domain, has been recorded in challenging conditions, with cheap hardware and a focus on the visual modality, and may have been post-processed or edited. In this paper, we extend our earlier work on adapting the acoustic model of a DNN-based speech recognition system to an RNN language \\n',\n"," ' Examining Cooperation in Visual Dialog Models ; In this work we propose a blackbox intervention method for visual dialog models, with the aim of assessing the contribution of individual linguistic or visual components. Concretely, we conduct structured or randomized interventions that aim to impair an individual component of the model, and observe changes in task performance. We reproduce a state-of-the-art visual dialog model and demonstrate that our methodology yields surprising insights, namely that both\\n',\n"," ' Video Highlight Prediction Using Audience Chat Reactions ; Sports channel video portals offer an exciting domain for research on multimodal, multilingual analysis. We present methods addressing the problem of automatic video highlight prediction based on joint visual features and textual analysis of the real-world audience discourse with complex slang, in both English and traditional Chinese. We present a novel dataset based on League of Legends championships recorded from North American and Taiwanese Twitc\\n',\n"," ' Invariant Representations for Noisy Speech Recognition ; Modern automatic speech recognition (ASR) systems need to be robust under acoustic variability arising from environmental, speaker, channel, and recording conditions. Ensuring such robustness to variability is a challenge in modern day neural network-based ASR systems, especially when all types of variability are not seen during training. We attempt to address this problem by encouraging the neural network acoustic model to learn invariant feature rep\\n',\n"," ' Self-Supervised Vision-Based Detection of the Active Speaker as a   Prerequisite for Socially-Aware Language Acquisition ; This paper presents a self-supervised method for detecting the active speaker in a multi-person spoken interaction scenario. We argue that this capability is a fundamental prerequisite for any artificial cognitive system attempting to acquire language in social settings. Our methods are able to detect an arbitrary number of possibly overlapping active speakers based exclusively on visua\\n',\n"," ' Product Characterisation towards Personalisation: Learning Attributes   from Unstructured Data to Recommend Fashion Products ; In this paper, we describe a solution to tackle a common set of challenges in e-commerce, which arise from the fact that new products are continually being added to the catalogue. The challenges involve properly personalising the customer experience, forecasting demand and planning the product range. We argue that the foundational piece to solve all of these problems is having consi\\n',\n"," ' The Self-Organization of Speech Sounds ; The speech code is a vehicle of language: it defines a set of forms used by a community to carry information. Such a code is necessary to support the linguistic interactions that allow humans to communicate. How then may a speech code be formed prior to the existence of linguistic interactions? Moreover, the human speech code is discrete and compositional, shared by all the individuals of a community but different across communities, and phoneme inventories are chara\\n',\n"," \" What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes ; The F-measure or F-score is one of the most commonly used single number measures in Information Retrieval, Natural Language Processing and Machine Learning, but it is based on a mistake, and the flawed assumptions render it unsuitable for use in most contexts! Fortunately, there are better alternatives.\\n\",\n"," ' A Machine Learning Perspective on Predictive Coding with PAQ ; PAQ8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. This report presents a detailed description of PAQ8 from a statistical machine learning perspective. It shows that it is possible to understand some of the modules of PAQ8 and use this understanding to improve the method. However, intuitive statistical explanations of the behavior of other modules remain elusive. We h\\n',\n"," ' A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale   SVM Training ; Recently, there has been a renewed interest in the machine learning community for variants of a sparse greedy approximation procedure for concave optimization known as {the Frank-Wolfe (FW) method}. In particular, this procedure has been successfully applied to train large-scale instances of non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has allowed to obtain efficient algorithms but also imp\\n',\n"," ' Semi-supervised Vocabulary-informed Learning ; Despite significant progress in object categorization, in recent years, a number of important challenges remain, mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, \\n',\n"," ' Submodular meets Structured: Finding Diverse Subsets in   Exponentially-Large Structured Item Sets ; To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding\\n',\n"," \" ZM-Net: Real-time Zero-shot Image Manipulation Network ; Many problems in image processing and computer vision (e.g. colorization, style transfer) can be posed as 'manipulating' an input image into a corresponding output image given a user-specified guiding signal. A holy-grail solution towards generic image manipulation should be able to efficiently alter an input image with any personalized signals (even signals unseen during training), such as diverse paintings and arbitrary descriptive attributes. Howev\\n\",\n"," \" Multi-Agent Diverse Generative Adversarial Networks ; We propose an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known mode collapse problem. Firstly, we propose a multi-agent GAN architecture incorporating multiple generators and one discriminator. Secondly, to enforce different generators to capture diverse high probability modes, we modify discriminator's objective function where along with finding the real and fake samples, the d\\n\",\n"," ' Geometric GAN ; Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperpla\\n',\n"," ' A Data and Model-Parallel, Distributed and Scalable Framework for   Training of Deep Networks in Apache Spark ; Training deep networks is expensive and time-consuming with the training period increasing with data size and growth in model parameters. In this paper, we provide a framework for distributed training of deep networks over a cluster of CPUs in Apache Spark. The framework implements both Data Parallelism and Model Parallelism making it suitable to use for deep networks which require huge training d\\n',\n"," ' Understanding and Comparing Deep Neural Networks for Age and Gender   Classification ; Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different e\\n',\n"," ' When is a Convolutional Filter Easy To Learn? ; We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional fil\\n',\n"," ' Learning Sparse Visual Representations with Leaky Capped Norm   Regularizers ; Sparsity inducing regularization is an important part for learning over-complete visual representations. Despite the popularity of $\\\\ell_1$ regularization, in this paper, we investigate the usage of non-convex regularizations in this problem. Our contribution consists of three parts. First, we propose the leaky capped norm regularization (LCNR), which allows model weights below a certain threshold to be regularized more strongly \\n',\n"," ' ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection,   Adversarial Examples and Model Criticism ; ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement, the recent studies on the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases (e.g racial biases) questioned the reliability and the sustained development of these methods. This work inv\\n',\n"," \" Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of   Spurious Local Minima ; We consider the problem of learning a one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., $f(\\\\mathbf{Z}; \\\\mathbf{w}, \\\\mathbf{a}) = \\\\sum_j a_j\\\\sigma(\\\\mathbf{w}^\\\\top\\\\mathbf{Z}_j)$, in which both the convolutional weights $\\\\mathbf{w}$ and the output weights $\\\\mathbf{a}$ are parameters to be learned. We prove that with Gaussian input $\\\\mathbf{Z}$, there is a spuriou\\n\",\n"," \" Curiosity-driven Exploration by Self-supervised Prediction ; In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics m\\n\",\n"," ' Houdini: Fooling Deep Structured Prediction Models ; Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-dec\\n',\n"," ' Recent Advances in Zero-shot Recognition ; With the recent renaissance of deep convolution neural networks, encouraging breakthroughs have been achieved on the supervised recognition tasks, where each class has sufficient training data and fully annotated training data. However, to scale the recognition to a large number of classes with few or now training samples for each class remains an unsolved problem. One approach to scaling up the recognition is to develop models capable of recognizing unseen categor\\n',\n"," ' The loss surface and expressivity of deep convolutional neural networks ; We analyze the expressiveness and loss surface of practical deep convolutional neural networks (CNNs) with shared weights and max pooling layers. We show that such CNNs produce linearly independent features at a \"wide\" layer which has more neurons than the number of training samples. This condition holds e.g. for the VGG network. Furthermore, we provide for such wide CNNs necessary and sufficient conditions for global minima with zero\\n',\n"," ' Physics-guided Neural Networks (PGNN): An Application in Lake   Temperature Modeling ; This paper introduces a novel framework for combining scientific knowledge of physics-based models with neural networks to advance scientific discovery. This framework, termed as physics-guided neural network (PGNN), leverages the output of physics-based model simulations along with observational features to generate predictions using a neural network architecture. Further, this paper presents a novel framework for using \\n',\n"," ' Unified Spectral Clustering with Optimal Graph ; Spectral clustering has found extensive use in many areas. Most traditional spectral clustering algorithms work in three separate steps: similarity graph construction; continuous labels learning; discretizing the learned labels by k-means clustering. Such common practice has two potential flaws, which may lead to severe information loss and performance degradation. First, predefined similarity graph might not be optimal for subsequent clustering. It is well-a\\n',\n"," ' On the Inductive Bias of Dropout ; Dropout is a simple but effective technique for learning in neural networks and other settings. A sound theoretical understanding of dropout is needed to determine when dropout should be applied and how to use it most effectively. In this paper we continue the exploration of dropout as a regularizer pioneered by Wager, et.al. We focus on linear classification where a convex proxy to the misclassification loss (i.e. the logistic loss used in logistic regression) is minimize\\n',\n"," ' Surprising properties of dropout in deep networks ; We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that\\n',\n"," ' Training Probabilistic Spiking Neural Networks with First-to-spike   Decoding ; Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at harnessing the energy efficiency of spike-domain processing by building on computing elements that operate on, and exchange, spikes. In this paper, the problem of training a two-layer SNN is studied for the purpose of classification, under a Generalized Linear Model (GLM) probabilistic neural model that was previously considered within the computational \\n',\n"," \" A Novel Clustering Algorithm Based on Quantum Games ; Enormous successes have been made by quantum algorithms during the last decade. In this paper, we combine the quantum game with the problem of data clustering, and then develop a quantum-game-based clustering algorithm, in which data points in a dataset are considered as players who can make decisions and implement quantum strategies in quantum games. After each round of a quantum game, each player's expected payoff is calculated. Later, he uses a link-r\\n\",\n"," ' Exact solutions to the nonlinear dynamics of learning in deep linear   neural networks ; Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonl\\n',\n"," ' Entropy of Overcomplete Kernel Dictionaries ; In signal analysis and synthesis, linear approximation theory considers a linear decomposition of any given signal in a set of atoms, collected into a so-called dictionary. Relevant sparse representations are obtained by relaxing the orthogonality condition of the atoms, yielding overcomplete dictionaries with an extended number of atoms. More generally than the linear decomposition, overcomplete kernel dictionaries provide an elegant nonlinear extension by defi\\n',\n"," ' Rotation-invariant convolutional neural networks for galaxy morphology   prediction ; Measuring the morphological parameters of galaxies is a key requirement for studying their formation and evolution. Surveys such as the Sloan Digital Sky Survey (SDSS) have resulted in the availability of very large collections of images, which have permitted population-wide analyses of galaxy morphology. Morphological analysis has traditionally been carried out mostly via visual inspection by trained experts, which is tim\\n',\n"," ' Kernel Nonnegative Matrix Factorization Without the Curse of the   Pre-image - Application to Unmixing Hyperspectral Images ; The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of\\n',\n"," ' Approximation errors of online sparsification criteria ; Many machine learning frameworks, such as resource-allocating networks, kernel-based methods, Gaussian processes, and radial-basis-function networks, require a sparsification scheme in order to address the online learning paradigm. For this purpose, several online sparsification criteria have been proposed to restrict the model definition on a subset of samples. The most known criterion is the (linear) approximation criterion, which discards any sampl\\n',\n"," ' Discrete Deep Feature Extraction: A Theory and New Architectures ; First steps towards a mathematical theory of deep convolutional neural networks for feature extraction were made---for the continuous-time case---in Mallat, 2012, and Wiatowski and B\\\\\"olcskei, 2015. This paper considers the discrete case, introduces new convolutional neural network architectures, and proposes a mathematical framework for their analysis. Specifically, we establish deformation and translation sensitivity results of local and g\\n',\n"," ' Neural Responding Machine for Short-Text Conversation ; We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a micr\\n',\n"," ' Deep Active Learning for Dialogue Generation ; We propose an online, end-to-end, neural generative conversational model for open-domain dialogue. It is trained using a unique combination of offline two-phase supervised learning and online human-in-the-loop active learning. While most existing research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-cha\\n',\n"," ' Teaching Machines to Read and Comprehend ; Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to de\\n',\n"," ' Syntax-Aware Multi-Sense Word Embeddings for Deep Compositional Models   of Meaning ; Deep compositional models of meaning acting on distributional representations of words in order to produce vectors of larger text constituents are evolving to a popular area of NLP research. We detail a compositional distributional framework based on a rich form of word embeddings that aims at facilitating the interactions between words in the context of a sentence. Embeddings and composition layers are jointly learned aga\\n',\n"," ' A Deep Architecture for Semantic Matching with Multiple Positional   Sentence Representations ; Matching natural language sentences is central for many applications such as information retrieval and question answering. Existing deep models rely on a single sentence representation or multiple granularity representations for matching. However, such methods cannot well capture the contextualized local information in the matching process. To tackle this problem, we present a new deep architecture to match two s\\n',\n"," ' LSTM Neural Reordering Feature for Statistical Machine Translation ; Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM re\\n',\n"," ' Learning Natural Language Inference with LSTM ; Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for natural language inference (NLI). In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently pr\\n',\n"," ' Quantifying the vanishing gradient and long distance dependency problem   in recursive neural networks and recursive LSTMs ; Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper\\n',\n"," ' Implicit Discourse Relation Classification via Multi-Task Neural   Networks ; Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To\\n',\n"," ' Enhancing Sentence Relation Modeling with Auxiliary Character-level   Embedding ; Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary charac\\n',\n"," ' Automatic Open Knowledge Acquisition via Long Short-Term Memory Networks   with Feedback Negative Sampling ; Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns. They manually define patterns or automatically learn them from a large corpus. However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations. In this paper, we address this problem with the following methods. First, we exploit long short-term \\n',\n"," ' Question Answering over Knowledge Base with Neural Attention Combining   Global Knowledge Information ; With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put \\n',\n"," ' Generating Natural Language Inference Chains ; The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task t\\n',\n"," ' MuFuRU: The Multi-Function Recurrent Unit ; Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-of-the-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow fo\\n',\n"," ' LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in   Recurrent Neural Networks ; Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In thi\\n',\n"," ' Compression of Neural Machine Translation Models via Pruning ; Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the effica\\n',\n"," ' Constructing a Natural Language Inference Dataset using Generative   Neural Networks ; Natural Language Inference is an important task for Natural Language Understanding. It is concerned with classifying the logical relation between two sentences. In this paper, we propose several text generative neural networks for generating text hypothesis, which allows construction of new Natural Language Inference datasets. To evaluate the models, we propose a new metric -- the accuracy of the classifier trained on the\\n',\n"," ' Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain   Factoid Question Answering ; While question answering (QA) with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. As existing neural QA methods resolve Q\\n',\n"," ' Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM   Encoder-Decoder ; We present Tweet2Vec, a novel method for generating general-purpose vector representation of tweets. The model learns tweet embeddings using character-level CNN-LSTM encoder-decoder. We trained our model on 3 million, randomly selected English-language tweets. The model was evaluated using two methods: tweet semantic similarity and tweet sentiment categorization, outperforming the previous state-of-the-art in both tasks.\\n',\n"," ' Online Segment to Segment Neural Transduction ; We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottl\\n',\n"," ' Semantic Parsing with Semi-Supervised Sequential Autoencoders ; We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.\\n',\n"," ' Exploiting Sentence and Context Representations in Deep Neural Models   for Spoken Language Understanding ; This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotatio\\n',\n"," ' The Neural Noisy Channel ; We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable\\n',\n"," ' Generative Deep Neural Networks for Dialogue: A Short Review ; Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a mi\\n',\n"," ' Learning Python Code Suggestion with a Sparse Pointer Network ; To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not \\n',\n"," ' OpenNMT: Open-Source Toolkit for Neural Machine Translation ; We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the un\\n',\n"," ' Making Neural QA as Simple as Possible but not Simpler ; Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingred\\n',\n"," ' Survey of the State of the Art in Natural Language Generation: Core   tasks, applications and evaluation ; This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to\\n',\n"," ' A Constrained Sequence-to-Sequence Neural Model for Sentence   Simplification ; Sentence simplification reduces semantic complexity to benefit people with language impairments. Previous simplification studies on the sentence level and word level have achieved promising results but also meet great challenges. For sentence-level studies, sentences after simplification are fluent but sometimes are not really simplified. For word-level studies, words are simplified but also have potential grammar errors due to \\n',\n"," ' Improved Neural Relation Detection for Knowledge Base Question Answering ; Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA sy\\n',\n"," ' ASR error management for improving spoken language understanding ; This paper addresses the problem of automatic speech recognition (ASR) error detection and their use for improving spoken language understanding (SLU) systems. In this study, the SLU task consists in automatically extracting, from ASR transcriptions , semantic concepts and concept/values pairs in a e.g touristic information system. An approach is proposed for enriching the set of semantic labels with error specific labels and by using a rece\\n',\n"," ' Dynamic Integration of Background Knowledge in Neural NLU Systems ; Common-sense or background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, the requisite background knowledge is indirectly acquired from static corpora. We develop a new reading architecture for the dynamic integration of explicit background knowledge in NLU models. A new task-agnostic reading module provides refined word representations to a task-specific NLU architect\\n',\n"," ' Rethinking Skip-thought: A Neighborhood based Approach ; We study the skip-thought model with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qu\\n',\n"," ' Neural Domain Adaptation for Biomedical Question Answering ; Factoid question answering (QA) has recently benefited from the development of deep learning (DL) systems. Neural network models outperform traditional approaches in domains where large datasets exist, such as SQuAD (ca. 100,000 questions) for Wikipedia articles. However, these systems have not yet been applied to QA in more specific domains, such as biomedicine, because datasets are generally too small to train a DL system from scratch. For examp\\n',\n"," ' Neural Models for Key Phrase Detection and Question Generation ; We propose a two-stage neural model to tackle question generation from documents. Our model first estimates the probability that word sequences in a document compose \"interesting\" answers using a neural model trained on a question-answering corpus. We thus take a data-driven approach to interestingness. Predicted key phrases then act as target answers that condition a sequence-to-sequence question generation model with a copy mechanism. Empiri\\n',\n"," ' Neural Question Answering at BioASQ 5B ; This paper describes our submission to the 2017 BioASQ challenge. We participated in Task B, Phase B which is concerned with biomedical question answering (QA). We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output substrings of the provided text snippets. At the core of our system, we use FastQA, a state-of-the-art neural QA system. We extended it with biomedical word embeddings and changed its answer layer to\\n',\n"," ' A Deep Network with Visual Text Composition Behavior ; While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. I\\n',\n"," ' Semi-supervised emotion lexicon expansion with label propagation and   specialized word embeddings ; There exist two main approaches to automatically extract affective orientation: lexicon-based and corpus-based. In this work, we argue that these two methods are compatible and show that combining them can improve the accuracy of emotion classifiers. In particular, we introduce a novel variant of the Label Propagation algorithm that is tailored to distributed word representations, we apply batch gradient des\\n',\n"," ' Modelling Protagonist Goals and Desires in First-Person Narrative ; Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes \\n',\n"," ' Understanding Grounded Language Learning Agents ; Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and even execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome certain well-studied learning challenges that are also fundamental to infants learning their first words. While it is notable that models with no meaningful\\n',\n"," \" Just ASK: Building an Architecture for Extensible Self-Service Spoken   Language Understanding ; This paper presents the design of the machine learning architecture that underlies the Alexa Skills Kit (ASK) a large scale Spoken Language Understanding (SLU) Software Development Kit (SDK) that enables developers to extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the infrastructure powers over 25,000 skills deployed through the ASK, as well as AWS's Amazon Lex SLU Service. The ASK emph\\n\",\n"," ' The NarrativeQA Reading Comprehension Challenge ; Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context \\n',\n"," ' Cognitive Database: A Step towards Endowing Relational Databases with   Artificial Intelligence Capabilities ; We propose Cognitive Databases, an approach for transparently enabling Artificial Intelligence (AI) capabilities in relational databases. A novel aspect of our design is to first view the structured data source as meaningful unstructured text, and then use the text to build an unsupervised neural network model using a Natural Language Processing (NLP) technique called word embedding. This model cap\\n',\n"," ' Feudal Reinforcement Learning for Dialogue Management in Large Domains ; Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation. Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality. We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen\\n',\n"," ' An Analysis of Neural Language Modeling at Multiple Scales ; Many of the leading approaches in language modeling introduce novel, complex and specialized architectures. We take existing state-of-the-art word level language models based on LSTMs and QRNNs and extend them to both larger vocabularies as well as character-level granularity. When properly tuned, LSTMs and QRNNs achieve state-of-the-art results on character-level (Penn Treebank, enwik8) and word-level (WikiText-103) datasets, respectively. Result\\n',\n"," ' Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy   and Reverberant Environments ; We propose a spatial diffuseness feature for deep neural network (DNN)-based automatic speech recognition to improve recognition accuracy in reverberant and noisy environments. The feature is computed in real-time from multiple microphone signals without requiring knowledge or estimation of the direction of arrival, and represents the relative amount of diffuse noise in each time and frequency bin. It is \\n',\n"," ' Character-Aware Neural Language Models ; We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages wit\\n',\n"," ' Neural-based machine translation for medical text domain. Based on   European Medicines Agency leaflet texts ; The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neural machine translation. It aims at building a jointly-tuned single neural network that maximizes tra\\n',\n"," ' Conditional Generation and Snapshot Learning in Neural Dialogue Systems ; Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an end-to-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-\\n',\n"," ' Dialog state tracking, a machine reading approach using Memory Network ; In an end-to-end dialog system, the aim of dialog state tracking is to accurately estimate a compact representation of the current dialog status from a sequence of noisy observations produced by the speech recognition and the natural language understanding modules. This paper introduces a novel method of dialog state tracking based on the general paradigm of machine reading and proposes to solve it using an End-to-End Memory Network, M\\n',\n"," ' A Physical Metaphor to Study Semantic Drift ; In accessibility tests for digital preservation, over time we experience drifts of localized and labelled content in statistical models of evolving semantics represented as a vector field. This articulates the need to detect, measure, interpret and model outcomes of knowledge dynamics. To this end we employ a high-performance machine learning algorithm for the training of extremely large emergent self-organizing maps for exploratory data analysis. The working hy\\n',\n"," \" Optimizing Neural Network Hyperparameters with Gaussian Processes for   Dialog Act Classification ; Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which eit\\n\",\n"," ' A Survey of Voice Translation Methodologies - Acoustic Dialect Decoder ; Speech Translation has always been about giving source text or audio input and waiting for system to give translated output in desired form. In this paper, we present the Acoustic Dialect Decoder (ADD) - a voice to voice ear-piece translation device. We introduce and survey the recent advances made in the field of Speech Engineering, to employ in the ADD, particularly focusing on the three major processing steps of Recognition, Transla\\n',\n"," ' Learning to Reason With Adaptive Computation ; Multi-hop inference is necessary for machine learning systems to successfully solve tasks such as Recognising Textual Entailment and Machine Reading. In this work, we demonstrate the effectiveness of adaptive computation for learning the number of inference steps required for examples of different complexity and that learning the correct number of inference steps is difficult. We introduce the first model involving Adaptive Computation Time which provides a sma\\n',\n"," \" Feature-Augmented Neural Networks for Patient Note De-identification ; Patient notes contain a wealth of information of potentially great interest to medical investigators. However, to protect patients' privacy, Protected Health Information (PHI) must be removed from the patient notes before they can be legally released, a process known as patient note de-identification. The main objective for a de-identification system is to have the highest possible recall. Recently, the first neural-network-based de-iden\\n\",\n"," ' Direct Acoustics-to-Word Models for English Conversational Speech   Recognition ; Recent work on end-to-end automatic speech recognition (ASR) has shown that the connectionist temporal classification (CTC) loss can be used to convert acoustics to phone or character sequences. Such systems are used with a dictionary and separately-trained Language Model (LM) to produce word sequences. However, they are not truly end-to-end in the sense of mapping acoustics directly to words without an intermediate phone repr\\n',\n"," ' Factorization tricks for LSTM networks ; We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is \"matrix factorization by design\" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity whil\\n',\n"," ' NeuroNER: an easy-to-use program for named-entity recognition based on   neural networks ; Named-entity recognition (NER) aims at identifying entities of interest in a text. Artificial neural networks (ANNs) have recently been shown to outperform existing NER systems. However, ANNs remain challenging to use for non-expert users. In this paper, we present NeuroNER, an easy-to-use named-entity recognition tool based on ANNs. Users can annotate entities using a graphical web-based user interface (BRAT): the an\\n',\n"," ' Syllable-aware Neural Language Models: A Failure to Beat Character-aware   Ones ; Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.\\n',\n"," ' A Benchmarking Environment for Reinforcement Learning Based Task   Oriented Dialogue Management ; Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it\\n',\n"," ' Reusing Weights in Subword-aware Neural Language Models ; We propose several ways of reusing subword embeddings and other weights in subword-aware neural language models. The proposed techniques do not benefit a competitive character-aware model, but some of them improve the performance of syllable- and morpheme-aware models while showing significant reductions in model sizes. We discover a simple hands-on principle: in a multi-layer input embedding model, layers should be tied consecutively bottom-up if re\\n',\n"," ' Multi-task Learning of Pairwise Sequence Classification Tasks Over   Disparate Label Spaces ; We combine multi-task learning and semi-supervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets. We evaluate our approach on a variety of sequence classification tasks with disparate label spaces. We outperform strong single and multi-task baselin\\n',\n"," ' Dynamic Memory Networks for Visual and Textual Question Answering ; Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other mod\\n',\n"," ' Picture It In Your Mind: Generating High Level Visual Representations   From Textual Descriptions ; In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not \\n',\n"," \" Where to put the Image in an Image Caption Generator ; When a recurrent neural network language model is used for caption generation, the image information can be fed to the neural network either by directly incorporating it in the RNN -- conditioning the language model by `injecting' image features -- or in a layer following the RNN -- conditioning the language model by `merging' image features. While both options are attested in the literature, there is as yet no systematic comparison between the two. In \\n\",\n"," ' A Focused Dynamic Attention Model for Visual Question Answering ; Visual Question and Answering (VQA) problems are attracting increasing interest from multiple research disciplines. Solving VQA problems requires techniques from both computer vision for understanding the visual contents of a presented image or video, as well as the ones from natural language processing for understanding semantics of the question and generating the answers. Regarding visual content modeling, most of existing VQA methods adopt\\n',\n"," ' Simple Image Description Generator via a Linear Phrase-Based Approach ; Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Co\\n',\n"," ' Multimodal Convolutional Neural Networks for Matching Image and Sentence ; In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content, and one matching CNN learning the joint representation of image and se\\n',\n"," ' Learning to Compose Neural Networks for Question Answering ; We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural model network,\\n',\n"," ' Signer-independent Fingerspelling Recognition with Deep Neural Network   Adaptation ; We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90% accuracies on fingerspelling recognition in a signer-dependent setting. Howe\\n',\n"," ' Full-Network Embedding in a Multimodal Embedding Pipeline ; The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically \\n',\n"," \" What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption   Generator? ; In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component. This view suggests that the image features should be `injected' into the RNN. This is in fact the dominant view in the literature. Alternatively, the RNN can instead be viewed as only encoding the previously generated words. This view suggests that the RNN should only be used to encode linguisti\\n\",\n"," ' A Fixed-Size Encoding Method for Variable-Length Sequences with its   Application to Neural Network Language Models ; In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network languag\\n',\n"," ' Transition-Based Dependency Parsing with Stack Long Short-Term Memory ; We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding\\n',\n"," ' A Semisupervised Approach for Language Identification based on Ladder   Networks ; In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencode\\n',\n"," ' First-Pass Large Vocabulary Continuous Speech Recognition using   Bi-Directional Recurrent DNNs ; We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting tran\\n',\n"," ' Applying deep learning techniques on medical corpora from the World Wide   Web: a prototypical system and evaluation ; BACKGROUND: The amount of biomedical literature is rapidly growing and it is becoming increasingly difficult to keep manually curated knowledge bases and ontologies up-to-date. In this study we applied the word2vec deep learning toolkit to medical corpora to test its potential for identifying relationships from unstructured text. We evaluated the efficiency of word2vec in identifying proper\\n',\n"," ' Syntax-based Deep Matching of Short Texts ; Many tasks in natural language processing, ranging from machine translation to question answering, can be reduced to the problem of matching two sentences or more generally two short texts. We propose a new approach to the problem, called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The approach consists of two components, 1) a mining algorithm to discover patterns for matching two short-texts, defined in the product space of dependency trees, an\\n',\n"," ' Ensemble of Generative and Discriminative Techniques for Sentiment   Analysis of Movie Reviews ; Sentiment analysis is a common task in natural language processing that aims to detect polarity of a text document (typically a consumer review). In the simplest settings, we discriminate only between positive and negative sentiment, turning the task into a standard binary classification problem. We compare several ma- chine learning approaches to this problem, and combine them to achieve the best possible resul\\n',\n"," ' Diverse Embedding Neural Network Language Models ; We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data\\n',\n"," ' Learning linearly separable features for speech recognition using   convolutional neural networks ; Automatic speech recognition systems usually rely on spectral-based features, such as MFCC of PLP. These features are extracted based on prior knowledge such as, speech perception or/and speech production. Recently, convolutional neural networks have been shown to be able to estimate phoneme conditional probabilities in a completely data-driven manner, i.e. using directly temporal raw speech signal as input. \\n',\n"," ' Learning to Transduce with Unbounded Memory ; Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditi\\n',\n"," ' Feedforward Sequential Memory Neural Networks without Recurrent Feedback ; We introduce a new structure for memory neural networks, called feedforward sequential memory networks (FSMN), which can learn long-term dependency without using recurrent feedback. The proposed FSMN is a standard feedforward neural networks equipped with learnable sequential memory blocks in the hidden layers. In this work, we have applied FSMN to several language modeling (LM) tasks. Experimental results have shown that the memory \\n',\n"," ' Towards Structured Deep Neural Network for Automatic Speech Recognition ; In this paper we propose the Structured Deep Neural Network (structured DNN) as a structured and deep learning framework. This approach can learn to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structures rather than item by item.   When automatic speech recognition is viewed as a special case of such a stru\\n',\n"," ' Character-Level Incremental Speech Recognition with Recurrent Neural   Networks ; In real-time speech recognition applications, the latency is an important issue. We have developed a character-level incremental speech recognition (ISR) system that responds quickly even during the speech, where the hypotheses are gradually improved while the speaking proceeds. The algorithm employs a speech-to-character unidirectional recurrent neural network (RNN), which is end-to-end trained with connectionist temporal cla\\n',\n"," ' Globally Normalized Transition-Based Neural Networks ; We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bia\\n',\n"," ' Clinical Information Extraction via Convolutional Neural Network ; We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict\\n',\n"," ' Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations ; We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We per\\n',\n"," ' Stance Detection with Bidirectional Conditional Encoding ; Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experime\\n',\n"," ' SMS Spam Filtering using Probabilistic Topic Modelling and Stacked   Denoising Autoencoder ; In This paper we present a novel approach to spam filtering and demonstrate its applicability with respect to SMS messages. Our approach requires minimum features engineering and a small set of la- belled data samples. Features are extracted using topic modelling based on latent Dirichlet allocation, and then a comprehensive data model is created using a Stacked Denoising Autoencoder (SDA). Topic modelling summarise\\n',\n"," ' Bidirectional Recurrent Neural Networks for Medical Event Detection in   Electronic Health Records ; Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) wi\\n',\n"," ' Sequence Training and Adaptation of Highway Deep Neural Networks ; Highway deep neural network (HDNN) is a type of depth-gated feedforward neural network, which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks (DNNs). Previously, we investigated a structured HDNN architecture for speech recognition, in which the two gate functions were tied across all the hidden layers, and we were able to train a much smaller model withou\\n',\n"," \" Recurrent Highway Networks ; Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this anal\\n\",\n"," ' Towards cross-lingual distributed representations without parallel text   trained with adversarial autoencoders ; Current approaches to learning vector representations of text that are compatible between different languages usually require some amount of parallel text, aligned at word, sentence or at least document level. We hypothesize however, that different natural languages share enough semantic structure that it should be possible, in principle, to learn compatible vector representations just by analyz\\n',\n"," ' Memory Visualization for Gated Recurrent Neural Networks in Speech   Recognition ; Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when per\\n',\n"," ' Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large   Vocabulary Speech Recognition ; We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sp\\n',\n"," ' Unsupervised Pretraining for Sequence to Sequence Learning ; This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subs\\n',\n"," ' Structured Attention Networks ; Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, \\n',\n"," \" End-to-End Multi-View Networks for Text Classification ; We propose a multi-view network for text classification. Our method automatically creates various views of its input text, each taking the form of soft attention weights that distribute the classifier's focus among a set of base features. For a bag-of-words representation, each view focuses on a different subset of the text's words. Aggregating many such views results in a more discriminative and robust representation. Through a novel architecture tha\\n\",\n"," ' Differentiable Scheduled Sampling for Credit Assignment ; We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide info\\n',\n"," ' Phone-aware Neural Language Identification ; Pure acoustic neural models, particularly the LSTM-RNN model, have shown great potential in language identification (LID). However, the phonetic information has been largely overlooked by most of existing neural LID models, although this information has been used in the conventional phonetic LID systems with a great success. We present a phone-aware neural LID architecture, which is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR system. By ut\\n',\n"," ' Detecting Off-topic Responses to Visual Prompts ; Automated methods for essay scoring have made great progress in recent years, achieving accuracies very close to human annotators. However, a known weakness of such automated scorers is not taking into account the semantic relevance of the submitted text. While there is existing work on detecting answer relevance given a textual prompt, very little previous research has been done to incorporate visual writing prompts. We propose a neural architecture and sev\\n',\n"," ' Dual Rectified Linear Units (DReLUs): A Replacement for Tanh Activation   Functions in Quasi-Recurrent Neural Networks ; In this paper, we introduce a novel type of Rectified Linear Unit (ReLU), called a Dual Rectified Linear Unit (DReLU). A DReLU, which comes with an unbounded positive and negative image, can be used as a drop-in replacement for a tanh activation function in the recurrent step of Quasi-Recurrent Neural Networks (QRNNs) (Bradbury et al. (2017)). Similar to ReLUs, DReLUs are less prone to th\\n',\n"," ' Fidelity-Weighted Learning ; Training deep neural networks requires many training samples, but in practice training labels are expensive to obtain and may be of varying quality, as some may be from trusted expert labelers while others might be from heuristics or other sources of weak supervision such as crowd-sourcing. This creates a fundamental quality versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled \\n',\n"," ' Feature Learning in Deep Neural Networks - Studies on Speech Recognition   Tasks ; Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that t\\n',\n"," ' Estimating Phoneme Class Conditional Probabilities from Raw Speech   Signal using Convolutional Neural Networks ; In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic speech recognition (ASR) system, the phoneme class conditional probabilities are estimated by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then modeling the acoustic features with an ANN. Recent advances in machine \\n',\n"," ' Recursive Neural Networks Can Learn Logical Semantics ; Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models---plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)---can correctly learn to identify logical relationships such a\\n',\n"," ' A Re-ranking Model for Dependency Parser with Recursive Convolutional   Neural Network ; In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can\\n',\n"," ' Deep Speaker Vectors for Semi Text-independent Speaker Verification ; Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method.   This paper extends the d-vector approach to semi text-independent speaker verification tas\\n',\n"," ' Advances in Very Deep Convolutional Neural Networks for LVCSR ; Very deep CNNs with small 3x3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper we investigate how to efficiently scale these models to larger datasets. Specifically, we address the design choice of pooling and padding along the time dimension which renders convolutional evaluation of sequences highly inefficient. We propose a new CNN design without \\n',\n"," ' Learning Compact Recurrent Neural Networks ; Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent arch\\n',\n"," ' Dependency Parsing with LSTMs: An Empirical Evaluation ; We propose a transition-based dependency parser using Recurrent Neural Networks with Long Short-Term Memory (LSTM) units. This extends the feedforward neural network parser of Chen and Manning (2014) and enables modelling of entire sequences of shift/reduce transition decisions. On the Google Web Treebank, our LSTM parser is competitive with the best feedforward parser on overall accuracy and notably achieves more than 3% improvement for long-range de\\n',\n"," ' Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis   and Application to Information Retrieval ; This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the \\n',\n"," ' Encoding Source Language with Convolutional Neural Network for Machine   Translation ; The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during \\n',\n"," ' Maximum a Posteriori Adaptation of Network Parameters in Deep Models ; We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-H\\n',\n"," ' Context-Dependent Translation Selection Using Convolutional Neural   Network ; We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture \\n',\n"," ' Convolutional Neural Network Architectures for Matching Natural Language   Sentences ; Semantic matching is of central importance to many natural language tasks \\\\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed\\n',\n"," ' Long Short-Term Memory Over Tree Structures ; The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies,\\n',\n"," ' Improving the Performance of Neural Machine Translation Involving   Morphologically Rich Languages ; The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English - Tamil language pair was \\n',\n"," ' A recurrent neural network without chaos ; We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.\\n',\n"," \" End-to-end Phoneme Sequence Recognition using Convolutional Neural   Networks ; Most phoneme recognition state-of-the-art systems rely on a classical neural network classifiers, fed with highly tuned features, such as MFCC or PLP features. Recent advances in ``deep learning'' approaches questioned such systems, but while some attempts were made with simpler features such as spectrograms, state-of-the-art systems still rely on MFCCs. This might be viewed as a kind of failure from deep learning approaches, wh\\n\",\n"," ' A Deep Learning Approach to Data-driven Parameterizations for   Statistical Parametric Speech Synthesis ; Nearly all Statistical Parametric Speech Synthesizers today use Mel Cepstral coefficients as the vocal tract parameterization of the speech signal. Mel Cepstral coefficients were never intended to work in a parametric speech synthesis framework, but as yet, there has been little success in creating a better parameterization that is more suited to synthesis. In this paper, we use deep learning algorithms\\n',\n"," ' Addressing the Rare Word Problem in Neural Machine Translation ; Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implemen\\n',\n"," ' Investigating the Role of Prior Disambiguation in Deep-learning   Compositional Models of Meaning ; This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.\\n',\n"," ' Deep Speech: Scaling up end-to-end speech recognition ; We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instea\\n',\n"," ' Incremental Adaptation Strategies for Neural Network Language Models ; It is today acknowledged that neural network language models outperform backoff language models in applications like speech recognition or statistical machine translation. However, training these models on large amounts of data can take several days. We present efficient techniques to adapt a neural network language model to new data. Instead of training a completely new model or relying on mixture approaches, we propose two new methods:\\n',\n"," ' Joint RNN-Based Greedy Parsing and Word Composition ; This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which summarizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and \\n',\n"," ' Efficient Exact Gradient Update for training Deep Networks with Very   Large Sparse Targets ; An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden \\n',\n"," \" Discriminative Neural Sentence Modeling by Tree-Based Convolution ; This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences' structural features, and these features are aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, which enables effectiv\\n\",\n"," ' Self-Adaptive Hierarchical Sentence Model ; The ability to accurately model a sentence at varying stages (e.g., word-phrase-sentence) plays a central role in natural language processing. As an effort towards this goal we propose a self-adaptive hierarchical sentence model (AdaSent). AdaSent effectively forms a hierarchy of representations from words to phrases and then to sentences through recursive gated local composition of adjacent segments. We design a competitive mechanism (through gating networks) to \\n',\n"," ' Classifying Relations by Ranking with Convolutional Neural Networks ; Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation classification task using a convolutional neural network that performs classification by ranking (CR-CNN). We propose a new pairwise ranking loss function that makes it easy to reduce the impact of artificial classes. We perform experiments using the the SemEval-2\\n',\n"," ' Lexical Translation Model Using a Deep Neural Network Architecture ; In this paper we combine the advantages of a model using global source sentence contexts, the Discriminative Word Lexicon, and neural networks. By using deep neural networks instead of the linear maximum entropy model in the Discriminative Word Lexicon models, we are able to leverage dependencies between different source words due to the non-linearity. Furthermore, the models for different target words can share parameters and therefore da\\n',\n"," ' Visualizing and Understanding Recurrent Networks ; Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we a\\n',\n"," ' A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for   Unsupervised Discovery of Linguistic Units and Generation of High Quality   Features ; This paper summarizes the work done by the authors for the Zero Resource Speech Challenge organized in the technical program of Interspeech 2015. The goal of the challenge is to discover linguistic units directly from unlabeled speech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work automatically discovers multiple sets of acoust\\n',\n"," ' Author Identification using Multi-headed Recurrent Neural Networks ; Recurrent neural networks (RNNs) are very good at modelling the flow of text, but typically need to be trained on a far larger corpus than is available for the PAN 2015 Author Identification task. This paper describes a novel approach where the output layer of a character-level RNN language model is split into several independent predictive sub-models, each representing an author, while the recurrent layer is shared by all. This allows the\\n',\n"," ' A Deep Memory-based Architecture for Sequence-to-Sequence Learning ; We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence learning, which performs the task through a series of nonlinear transformations from the representation of the input sequence (e.g., a Chinese sentence) to the final output sequence (e.g., translation to English). Inspired by the recently proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate representations in stacked layers of memories, and\\n',\n"," ' Ask Me Anything: Dynamic Memory Networks for Natural Language Processing ; Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous ite\\n',\n"," ' Improved Deep Speaker Feature Learning for Text-Dependent Speaker   Recognition ; A deep learning approach has been proposed recently to derive speaker identifies (d-vector) by a deep neural network (DNN). This approach has been applied to text-dependent speaker recognition tasks and shows reasonable performance gains when combined with the conventional i-vector approach. Although promising, the existing d-vector implementation still can not compete with the i-vector baseline. This paper presents two improv\\n',\n"," ' Grid Long Short-Term Memory ; This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to \\n',\n"," ' A Dependency-Based Neural Network for Relation Classification ; Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic \\n',\n"," ' PTE: Predictive Text Embedding through Large-scale Heterogeneous Text   Networks ; Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding metho\\n',\n"," ' Relation Classification via Recurrent Neural Network ; Deep learning has gained much success in sentence-level relation classification. For example, convolutional neural networks (CNN) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods. Thus a lot of works have been produced based on CNN structures. However, a key issue that has not been well addressed by the CNN-based method is the lack of capability to learn temporal features, especi\\n',\n"," ' Learning from LDA using Deep Neural Networks ; Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model for topic inference. In spite of its great success, inferring the latent topic distribution with LDA is time-consuming. Motivated by the transfer learning approach proposed by~\\\\newcite{hinton2015distilling}, we present a novel method that uses LDA to supervise the training of a deep neural network (DNN), so that the DNN can approximate the costly LDA inference with less computation. \\n',\n"," ' Online Representation Learning in Recurrent Neural Language Models ; We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with \\n',\n"," \" A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional   Neural Networks for Sentence Classification ; Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (kim 2014, kalchbrenner 2014, johnson 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is\\n\",\n"," ' Prediction-Adaptation-Correction Recurrent Neural Networks for   Low-Resource Language Speech Recognition ; In this paper, we investigate the use of prediction-adaptation-correction recurrent neural networks (PAC-RNNs) for low-resource speech recognition. A PAC-RNN is comprised of a pair of neural networks in which a {\\\\it correction} network uses auxiliary information given by a {\\\\it prediction} network to help estimate the state probability. The information from the correction network is also used by the p\\n',\n"," ' Generating Text with Deep Reinforcement Learning ; We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively. The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts. Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal stat\\n',\n"," ' Detecting Interrogative Utterances with Recurrent Neural Networks ; In this paper, we explore different neural network architectures that can predict if a speaker of a given utterance is asking a question or making a statement. We com- pare the outcomes of regularization methods that are popularly used to train deep neural networks and study how different context functions can affect the classification performance. We also compare the efficacy of gated activation functions that are favorably used in recurre\\n',\n"," ' A Neural Transducer ; Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unli\\n',\n"," \" Skip-Thought Memory Networks ; Question Answering (QA) is fundamental to natural language processing in that most nlp problems can be phrased as QA (Kumar et al., 2015). Current weakly supervised memory network models that have been proposed so far struggle at answering questions that involve relations among multiple entities (such as facebook's bAbi qa5-three-arg-relations in (Weston et al., 2015)). To address this problem of learning multi-argument multi-hop semantic relations for the purpose of QA, we pr\\n\",\n"," ' Named Entity Recognition with Bidirectional LSTM-CNNs ; Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding\\n',\n"," ' Generating News Headlines with Recurrent Neural Networks ; We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly,\\n',\n"," ' Words are not Equal: Graded Weighting Model for building Composite   Document Vectors ; Despite the success of distributional semantics, composing phrases from word vectors remains an important challenge. Several methods have been tried for benchmark tasks such as sentiment classification, including word vector averaging, matrix-vector approaches based on parsing, and on-the-fly learning of paragraph vectors. Most models usually omit stop words from the composition. Instead of such an yes-no decision, we co\\n',\n"," ' Small-footprint Deep Neural Networks with Highway Connections for Speech   Recognition ; For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we s\\n',\n"," ' Backward and Forward Language Modeling for Constrained Sentence   Generation ; Recent language models, especially those based on recurrent neural networks (RNNs), make it possible to generate natural language from a learned probability. Language generation has wide applications including machine translation, summarization, question answering, conversation systems, etc. Existing methods typically learn a joint probability of words conditioned on additional information, which is (either statically or dynamica\\n',\n"," ' Online Keyword Spotting with a Character-Level Recurrent Neural Network ; In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-to-end trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, \\n',\n"," ' Domain Specific Author Attribution Based on Feedforward Neural Network   Language Models ; Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventio\\n',\n"," ' Segmental Recurrent Neural Networks for End-to-end Speech Recognition ; We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are\\n',\n"," ' How Transferable are Neural Networks in NLP Applications? ; Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are incon\\n',\n"," ' Recurrent Neural Network Encoder with Attention for Community Question   Answering ; We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity an\\n',\n"," ' Recursive Neural Language Architecture for Tag Prediction ; We consider the problem of learning distributed representations for tags from their associated content for the task of tag recommendation. Considering tagging information is usually very sparse, effective learning from content and tag association is very crucial and challenging task. Recently, various neural representation learning models such as WSABIE and its variants show promising performance, mainly due to compact feature representations learn\\n',\n"," ' On the Compression of Recurrent Neural Networks with an Application to   LVCSR acoustic modeling for Embedded Speech Recognition ; We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses bot\\n',\n"," ' Pointing the Unknown Words ; The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other\\n',\n"," ' Learning Multiscale Features Directly From Waveforms ; Deep learning has dramatically improved the performance of speech recognition systems through learning hierarchies of features optimized for the task at hand. However, true end-to-end learning, where features are learned directly from waveforms, has only recently reached the performance of hand-tailored representations based on the Fourier transform. In this paper, we detail an approach to use convolutional filters to push past the inherent tradeoff of \\n',\n"," ' Joint Learning of Sentence Embeddings for Relevance and Entailment ; We consider the problem of Recognizing Textual Entailment within an Information Retrieval context, where we must simultaneously determine the relevancy as well as degree of entailment for individual pieces of evidence to determine a yes/no answer to a binary natural language question.   We compare several variants of neural networks for sentence embeddings in a setting of decision-making based on evidence of varying relevance. We propose a\\n',\n"," ' Deep API Learning ; Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bag-of-words (i.e., keyword matching or word-to-word alignment) and lack a deep understanding of the semantics of the qu\\n',\n"," ' Does Multimodality Help Human and Machine for Translation and Image   Captioning? ; This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generati\\n',\n"," ' Very Deep Convolutional Networks for Text Classification ; The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are \\n',\n"," ' Improving Recurrent Neural Networks For Sequence Labelling ; In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank \\n',\n"," ' Sentence Similarity Measures for Fine-Grained Estimation of Topical   Relevance in Learner Essays ; We investigate the task of assessing sentence-level prompt relevance in learner essays. Various systems using word overlap, neural embeddings and neural compositional models are evaluated on two datasets of learner writing. We propose a new method for sentence-level similarity calculation, which learns to adjust the weights of pre-trained word embeddings for a specific task, achieving substantially higher acc\\n',\n"," ' Deep CNNs along the Time Axis with Intermap Pooling for Robustness to   Spectral Variations ; Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) laye\\n',\n"," \" Automatic Text Scoring Using Neural Networks ; Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a full\\n\",\n"," ' A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic   Modeling in Speech Recognition ; We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniq\\n',\n"," ' Sequence-Level Knowledge Distillation ; Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standa\\n',\n"," ' Learning Semantically Coherent and Reusable Kernels in Convolution   Neural Nets for Sentence Classification ; The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the\\n',\n"," ' RETURNN: The RWTH Extensible Training framework for Universal Recurrent   Neural Networks ; In this work we release our extensible and easily configurable neural network training software. It provides a rich set of functional layers with a particular focus on efficient training of recurrent neural network topologies on multiple GPUs. The source of the software package is public and freely available for academic research purposes and can be used as a framework or as a standalone tool which supports a flexibl\\n',\n"," ' Character-Level Language Modeling with Hierarchical Recurrent Neural   Networks ; Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling out-of-vocabulary words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules \\n',\n"," ' Multi-task Recurrent Model for True Multilingual Speech Recognition ; Research on multilingual speech recognition remains attractive yet challenging. Recent studies focus on learning shared structures under the multi-task paradigm, in particular a feature sharing structure. This approach has been found effective to improve performance on each individual language. However, this approach is only useful when the deployed system supports just one language. In a true multilingual scenario where multiple language\\n',\n"," ' Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep   Recurrent models ; Sentiment Analysis (SA) is an action research area in the digital age. With rapid and constant growth of online social media sites and services, and the increasing amount of textual data such as - statuses, comments, reviews etc. available in them, application of automatic SA is on the rise. However, most of the research works on SA in natural language processing (NLP) are based on English language. Despite being t\\n',\n"," ' Attending to Characters in Neural Sequence Labeling Models ; Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectu\\n',\n"," ' Visualizing and Understanding Curriculum Learning for Long Short-Term   Memory Networks ; Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term\\n',\n"," ' Dense Prediction on Sequences with Time-Dilated Convolutions for Speech   Recognition ; In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of fr\\n',\n"," ' End-to-End ASR-free Keyword Search from Speech ; End-to-end (E2E) systems have achieved competitive results compared to conventional hybrid hidden Markov model (HMM)-deep neural network based automatic speech recognition (ASR) systems. Such E2E systems are attractive due to the lack of dependence on alignments between input acoustic and output grapheme or HMM state sequence during training. This paper explores the design of an ASR-free end-to-end system for text query-based keyword search (KWS) from speech \\n',\n"," ' Training Language Models Using Target-Propagation ; While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we\\n',\n"," ' Deep Voice: Real-time Neural Text-to-Speech ; We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation mo\\n',\n"," ' Improved Variational Autoencoders for Text Modeling using Dilated   Convolutions ; Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN\\n',\n"," ' Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence   Labelling ; Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this pa-\\n',\n"," ' Ask Me Even More: Dynamic Memory Tensor Networks (Extended Model) ; We examine Memory Networks for the task of question answering (QA), under common real world scenario where training examples are scarce and under weakly supervised scenario, that is only extrinsic labels are available for training. We propose extensions for the Dynamic Memory Network (DMN), specifically within the attention mechanism, we call the resulting Neural Architecture as Dynamic Memory Tensor Network (DMTN). Ultimately, we see that \\n',\n"," ' Simplified End-to-End MMI Training and Voting for ASR ; A simplified speech recognition system that uses the maximum mutual information (MMI) criterion is considered. End-to-end training using gradient descent is suggested, similarly to the training of connectionist temporal classification (CTC). We use an MMI criterion with a simple language model in the training stage, and a standard HMM decoder. Our method compares favorably to CTC in terms of performance, robustness, decoding time, disk footprint and qu\\n',\n"," ' Learning to Generate Reviews and Discovering Sentiment ; We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment\\n',\n"," ' Semi-supervised Multitask Learning for Sequence Labeling ; We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection \\n',\n"," \" Going Wider: Recurrent Neural Network With Parallel Cells ; Recurrent Neural Network (RNN) has been widely applied for sequence modeling. In RNN, the hidden states at current step are full connected to those at previous step, thus the influence from less related features at previous step may potentially decrease model's learning ability. We propose a simple technique called parallel cells (PCs) to enhance the learning ability of Recurrent Neural Network (RNN). In each layer, we run multiple small RNN cells \\n\",\n"," ' Phonetic Temporal Neural Model for Language Identification ; Deep neural models, particularly the LSTM-RNN model, have shown great potential for language identification (LID). However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phon\\n',\n"," ' Relevance-based Word Embedding ; Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently attracted much attention in natural language processing and information retrieval tasks. The embedding vectors are typically learned based on term proximity in a large corpus. This means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is no\\n',\n"," ' Deriving Neural Architectures from Sequence and Graph Kernels ; The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference ob\\n',\n"," ' On Multilingual Training of Neural Dependency Parsers ; We show that a recently proposed neural dependency parser can be improved by joint training on multiple languages from the same family. The parser is implemented as a deep neural network whose only input is orthographic representations of words. In order to successfully parse, the network has to discover how linguistically relevant concepts can be inferred from word spellings. We analyze the representations of characters and words that are learned by t\\n',\n"," ' Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks ; Ladder networks are a notable new concept in the field of semi-supervised learning by showing state-of-the-art results in image recognition tasks while being compatible with many existing neural architectures. We present the recurrent ladder network, a novel modification of the ladder network, for semi-supervised learning of recurrent neural networks which we evaluate with a phoneme recognition task on the TIMIT corpus. Our results show th\\n',\n"," ' Adversarially Regularized Autoencoders ; While autoencoders are a key technique in representation learning for continuous structures, such as images or wave forms, developing general-purpose autoencoders for discrete structures, such as text sequence or discretized images, has proven to be more challenging. In particular, discrete inputs make it more difficult to learn a smooth encoder that preserves the complex local relationships in the input space. In this work, we propose an adversarially regularized au\\n',\n"," ' Auxiliary Objectives for Neural Error Detection Models ; We investigate the utility of different auxiliary objectives and training strategies within a neural sequence labeling approach to error detection in learner writing. Auxiliary costs provide the model with additional linguistic information, allowing it to learn general-purpose compositional features that can then be exploited for other objectives. Our experiments show that a joint learning approach trained with parallel labels on in-domain data improv\\n',\n"," \" An Error-Oriented Approach to Word Embedding Pre-Training ; We propose a novel word embedding pre-training approach that exploits writing errors in learners' scripts. We compare our method to previous models that tune the embeddings based on script scores and the discrimination between correct and corrupt word contexts in addition to the generic commonly-used embeddings pre-trained on large corpora. The comparison is achieved by using the aforementioned models to bootstrap a neural network that learns to pr\\n\",\n"," ' A Continuous Relaxation of Beam Search for End-to-end Training of Neural   Sequence Models ; Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when co\\n',\n"," ' Regularizing and Optimizing LSTM Language Models ; Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hid\\n',\n"," ' Supervised Speech Separation Based on Deep Learning: An Overview ; Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In p\\n',\n"," ' Grasping the Finer Point: A Supervised Similarity Network for Metaphor   Detection ; The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results de\\n',\n"," ' Think Globally, Embed Locally --- Locally Linear Meta-embedding of Words ; Distributed word embeddings have shown superior performances in numerous Natural Language Processing (NLP) tasks. However, their performances vary significantly across different tasks, implying that the word embeddings learnt by those methods capture complementary aspects of lexical semantics. Therefore, we believe that it is important to combine the existing word embeddings to produce more accurate and complete \\\\emph{meta-embeddings\\n',\n"," ' KeyVec: Key-semantics Preserving Document Representations ; Previous studies have demonstrated the empirical success of word embeddings in various applications. In this paper, we investigate the problem of learning distributed representations for text documents which many machine learning algorithms take as input for a number of NLP tasks.   We propose a neural network model, KeyVec, which learns document representations with the goal of preserving key semantics of the input text. It enables the learned low\\n',\n"," ' Exploring Asymmetric Encoder-Decoder Structure for Context-based   Sentence Representation Learning ; Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder. We further combine a suite of\\n',\n"," ' CNN Is All You Need ; The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-to-sequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequen\\n',\n"," ' Combining Representation Learning with Logic for Language Processing ; The current state-of-the-art in many natural language processing and automated knowledge base completion tasks is held by representation learning methods which learn distributed vector representations of symbols via gradient-based optimization. They require little or no hand-crafted features, thus avoiding the need for most preprocessing steps and task-specific assumptions. However, in many cases representation learning requires a large \\n',\n"," ' A Note on Topology Preservation in Classification, and the Construction   of a Universal Neuron Grid ; It will be shown that according to theorems of K. Menger, every neuron grid if identified with a curve is able to preserve the adopted qualitative structure of a data space. Furthermore, if this identification is made, the neuron grid structure can always be mapped to a subset of a universal neuron grid which is constructable in three space dimensions. Conclusions will be drawn for established neuron grid \\n',\n"," ' Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On   Boltzmann Machines ; One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neu\\n',\n"," ' Mapping Temporal Variables into the NeuCube for Improved Pattern   Recognition, Predictive Modelling and Understanding of Stream Data ; This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temp\\n',\n"," ' An Evolutionary Algorithm to Learn SPARQL Queries for   Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia ; Efficient usage of the knowledge provided by the Linked Data community is often hindered by the need for domain experts to formulate the right SPARQL queries to answer questions. For new questions they have to decide which datasets are suitable and in which terminology and modelling style to phrase the SPARQL query.   In this work we present an evolutionary algorithm to help with\\n',\n"," ' A Geometric Framework for Convolutional Neural Networks ; In this paper, a geometric framework for neural networks is proposed. This framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component-based form, but in a coordinate-free manner. Convolutional neural networks are described in this framework in a compact form, with the gradients of standard --- and higher-order --- loss functions calculated for each layer of the network. This approach c\\n',\n"," ' A Novel Representation of Neural Networks ; Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole enti\\n',\n"," ' Converting Cascade-Correlation Neural Nets into Probabilistic Generative   Models ; Humans are not only adept in recognizing what class an input instance belongs to (i.e., classification task), but perhaps more remarkably, they can imagine (i.e., generate) plausible instances of a desired class with ease, when prompted. Inspired by this, we propose a framework which allows transforming Cascade-Correlation Neural Networks (CCNNs) into probabilistic generative models, thereby enabling CCNNs to generate sample\\n',\n"," ' On the Performance of Network Parallel Training in Artificial Neural   Networks ; Artificial Neural Networks (ANNs) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. However, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as a mec\\n',\n"," ' Programmable Agents ; We build deep RL agents that execute declarative programs expressed in formal language. The agents learn to ground the terms in this language in their environment, and can generalize their behavior at test time to execute new programs that refer to objects that were not referenced during training. The agents develop disentangled interpretable representations that allow them to generalize to a wide variety of zero-shot semantic tasks.\\n',\n"," ' Explainable Artificial Intelligence: Understanding, Visualizing and   Interpreting Deep Learning Models ; With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their neste\\n',\n"," ' Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games? ; Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments \\n',\n"," ' Emergence of grid-like representations by training recurrent neural   networks to perform spatial localization ; Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysteriou\\n',\n"," ' Dimensionality Reduction and Reconstruction using Mirroring Neural   Networks and Object Recognition based on Reduced Dimension Characteristic   Vector ; In this paper, we present a Mirroring Neural Network architecture to perform non-linear dimensionality reduction and Object Recognition using a reduced lowdimensional characteristic vector. In addition to dimensionality reduction, the network also reconstructs (mirrors) the original high-dimensional input vector from the reduced low-dimensional data. The M\\n',\n"," ' Parcellation of fMRI Datasets with ICA and PLS-A Data Driven Approach ; Inter-subject parcellation of functional Magnetic Resonance Imaging (fMRI) data based on a standard General Linear Model (GLM)and spectral clustering was recently proposed as a means to alleviate the issues associated with spatial normalization in fMRI. However, for all its appeal, a GLM-based parcellation approach introduces its own biases, in the form of a priori knowledge about the shape of Hemodynamic Response Function (HRF) and tas\\n',\n"," ' Iris Codes Classification Using Discriminant and Witness Directions ; The main topic discussed in this paper is how to use intelligence for biometric decision defuzzification. A neural training model is proposed and tested here as a possible solution for dealing with natural fuzzification that appears between the intra- and inter-class distribution of scores computed during iris recognition tests. It is shown here that the use of proposed neural network support leads to an improvement in the artificial perc\\n',\n"," ' Algorithms for Image Analysis and Combination of Pattern Classifiers   with Application to Medical Diagnosis ; Medical Informatics and the application of modern signal processing in the assistance of the diagnostic process in medical imaging is one of the more recent and active research areas today. This thesis addresses a variety of issues related to the general problem of medical image analysis, specifically in mammography, and presents a series of algorithms and design approaches for all the intermediate\\n',\n"," ' Deep Neural Networks are Easily Fooled: High Confidence Predictions for   Unrecognizable Images ; Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of\\n',\n"," ' Homogeneous Spiking Neuromorphic System for Real-World Pattern   Recognition ; A neuromorphic chip that combines CMOS analog spiking neurons and memristive synapses offers a promising solution to brain-inspired computing, as it can provide massive neural network parallelism and density. Previous hybrid analog CMOS-memristor approaches required extensive CMOS circuitry for training, and thus eliminated most of the density advantages gained by the adoption of memristor synapses. Further, they used different w\\n',\n"," ' Crowd Behavior Analysis: A Review where Physics meets Biology ; Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision stud- ies related to crowd are observed to resonate with the understanding of the emergent behavior in\\n',\n"," ' Can Pretrained Neural Networks Detect Anatomy? ; Convolutional neural networks demonstrated outstanding empirical results in computer vision and speech recognition tasks where labeled training data is abundant. In medical imaging, there is a huge variety of possible imaging modalities and contrasts, where annotated data is usually very scarce. We present two approaches to deal with this challenge. A network pretrained in a different domain with abundant data is used as a feature extractor, while a subsequen\\n',\n"," ' Metaheuristic Algorithms for Convolution Neural Network ; A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where it\\n',\n"," ' Hadamard Product for Low-rank Bilinear Pooling ; Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard produ\\n',\n"," ' Incremental Network Quantization: Towards Lossless CNNs with   Low-Precision Weights ; This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two inno\\n',\n"," ' LesionSeg: Semantic segmentation of skin lesions using Deep   Convolutional Neural Network ; We present a method for skin lesion segmentation for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of vie\\n',\n"," ' Convolutional Spike Timing Dependent Plasticity based Feature Learning   in Spiking Neural Networks ; Brain-inspired learning models attempt to mimic the cortical architecture and computations performed in the neurons and synapses constituting the human brain to achieve its efficiency in cognitive tasks. In this work, we present convolutional spike timing dependent plasticity based feature learning with biologically plausible leaky-integrate-and-fire neurons in Spiking Neural Networks (SNNs). We use shared \\n',\n"," ' Adversarial Transformation Networks: Learning to Generate Adversarial   Examples ; Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We \\n',\n"," ' Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced   Attentive Response Approach for Explaining and Visualizing Deep   Learning-Driven Stock Market Prediction ; Deep learning has been shown to outperform traditional machine learning algorithms across a wide range of problem domains. However, current deep learning algorithms have been criticized as uninterpretable \"black-boxes\" which cannot explain their decision making processes. This is a major shortcoming that prevents the widespread\\n',\n"," ' Fast YOLO: A Fast You Only Look Once System for Real-time Embedded   Object Detection in Video ; Object detection is considered one of the most challenging problems in this field of computer vision, as it involves the combination of object classification and object localization within a scene. Recently, deep neural networks (DNNs) have been demonstrated to achieve superior object detection performance compared to other approaches, with YOLOv2 (an improved You Only Look Once model) being one of the state-of-\\n',\n"," ' NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm ; Neural networks (NNs) have begun to have a pervasive impact on various applications of machine learning. However, the problem of finding an optimal NN architecture for large applications has remained open for several decades. Conventional approaches search for the optimal NN architecture through extensive trial-and-error. Such a procedure is quite inefficient. In addition, the generated NN architectures incur substantial redundancy. \\n',\n"," ' Analysis of supervised and semi-supervised GrowCut applied to   segmentation of masses in mammography images ; Breast cancer is already one of the most common form of cancer worldwide. Mammography image analysis is still the most effective diagnostic method to promote the early detection of breast cancer. Accurately segmenting tumors in digital mammography images is important to improve diagnosis capabilities of health specialists and avoid misdiagnosis. In this work, we evaluate the feasibility of applying\\n',\n"," ' Empirical Explorations in Training Networks with Discrete Activations ; We present extensive experiments training and testing hidden units in deep networks that emit only a predefined, static, number of discretized values. These units provide benefits in real-world deployment in systems in which memory and/or computation may be limited. Additionally, they are particularly well suited for use in large recurrent network models that require the maintenance of large amounts of internal state in memory. Surprisi\\n',\n"," ' Regularized Evolution for Image Classifier Architecture Search ; The effort devoted to hand-crafting image classifiers has motivated the use of architecture search to discover them automatically. Reinforcement learning and evolution have both shown promise for this purpose. This study employs a regularized version of a popular asynchronous evolutionary algorithm. We rigorously compare it to the non-regularized form and to a highly-successful reinforcement learning baseline. Using the same hardware, compute \\n',\n"," ' Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network   for Real-time Embedded Object Detection ; Object detection is a major challenge in computer vision, involving both object classification and object localization within a scene. While deep neural networks have been shown in recent years to yield very powerful techniques for tackling the challenge of object detection, one of the biggest challenges with enabling such object detection networks for widespread deployment on embedded device\\n',\n"," ' Inferencing Based on Unsupervised Learning of Disentangled   Representations ; Combining Generative Adversarial Networks (GANs) with encoders that learn to encode data points has shown promising results in learning data representations in an unsupervised way. We propose a framework that combines an encoder and a generator to learn disentangled representations which encode meaningful information about the data distribution without the need for any labels. While current approaches focus mostly on the generati\\n',\n"," ' The Parameter-Less Self-Organizing Map algorithm ; The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a learning rate and annealing schemes for learning rate and neighbourhood size. We discuss the relative performance of the PLSOM and the SOM and demonstrate some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally we discuss some example applications of the PLSOM and present a proof of orde\\n',\n"," ' Simplified firefly algorithm for 2D image key-points search ; In order to identify an object, human eyes firstly search the field of view for points or areas which have particular properties. These properties are used to recognise an image or an object. Then this process could be taken as a model to develop computer algorithms for images identification. This paper proposes the idea of applying the simplified firefly algorithm to search for key-areas in 2D images. For a set of input test images the proposed \\n',\n"," \" Deep-Plant: Plant Identification with convolutional neural networks ; This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely r\\n\",\n"," ' Adapting Deep Network Features to Capture Psychological Representations ; Deep neural networks have become increasingly successful at solving classic perception problems such as object recognition, semantic segmentation, and scene understanding, often reaching or surpassing human-level accuracy. This success is due in part to the ability of DNNs to learn useful representations of high-dimensional inputs, a problem that humans must also solve. We examine the relationship between the representations learned b\\n',\n"," ' Large-Scale Evolution of Image Classifiers ; Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifica\\n',\n"," \" A Compact DNN: Approaching GoogLeNet-Level Accuracy of Classification   and Domain Adaptation ; Recently, DNN model compression based on network architecture design, e.g., SqueezeNet, attracted a lot attention. No accuracy drop on image classification is observed on these extremely compact networks, compared to well-known models. An emerging question, however, is whether these model compression techniques hurt DNN's learning ability other than classifying images on a single dataset. Our preliminary experime\\n\",\n"," ' Identifying Spatial Relations in Images using Convolutional Neural   Networks ; Traditional approaches to building a large scale knowledge graph have usually relied on extracting information (entities, their properties, and relations between them) from unstructured text (e.g. Dbpedia). Recent advances in Convolutional Neural Networks (CNN) allow us to shift our focus to learning entities and relations from images, as they build robust models that require little or no pre-processing of the images. In this pa\\n',\n"," ' Hierarchical Attentive Recurrent Tracking ; Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate \"where\" and \"what\" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority \\n',\n"," ' PSIque: Next Sequence Prediction of Satellite Images using a   Convolutional Sequence-to-Sequence Network ; Predicting unseen weather phenomena is an important issue for disaster management. In this paper, we suggest a model for a convolutional sequence-to-sequence autoencoder for predicting undiscovered weather situations from previous satellite images. We also propose a symmetric skip connection between encoder and decoder modules to produce more comprehensive image predictions. To examine our model perfo\\n',\n"," \" Evaluation of Alzheimer's Disease by Analysis of MR Images using   Multilayer Perceptrons and Kohonen SOM Classifiers as an Alternative to the   ADC Maps ; Alzheimer's disease is the most common cause of dementia, yet hard to diagnose precisely without invasive techniques, particularly at the onset of the disease. This work approaches image analysis and classification of synthetic multispectral images composed by diffusion-weighted magnetic resonance (MR) cerebral images for the evaluation of cerebrospinal \\n\",\n"," \" Neural tuning size is a key factor underlying holistic face processing ; Faces are a class of visual stimuli with unique significance, for a variety of reasons. They are ubiquitous throughout the course of a person's life, and face recognition is crucial for daily social interaction. Faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics. Furthermore, faces have been empirically found to elicit certain characteristic behavioral phenomena, which are widely held t\\n\",\n"," ' Distribution of the search of evolutionary product unit neural networks   for classification ; This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models w\\n',\n"," ' Correlation Alignment for Unsupervised Domain Adaptation ; In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also \\n',\n"," \" CITlab ARGUS for historical handwritten documents ; We describe CITlab's recognition system for the HTRtS competition attached to the 13. International Conference on Document Analysis and Recognition, ICDAR 2015. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are \\n\",\n"," ' Generalized Haar Filter based Deep Networks for Real-Time Object   Detection in Traffic Scene ; Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter \\n',\n"," \" Autoencoder Regularized Network For Driving Style Representation   Learning ; In this paper, we study learning generalized driving style representations from automobile GPS trip data. We propose a novel Autoencoder Regularized deep neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers' driving styles directly from GPS records, by combining supervised and unsupervised feature learning in a unified architecture. Experiments on a challenging driver number estimation problem and the dri\\n\",\n"," ' Fashioning with Networks: Neural Style Transfer to Design Clothes ; Convolutional Neural Networks have been highly successful in performing a host of computer vision tasks such as object recognition, object detection, image segmentation and texture synthesis. In 2015, Gatys et. al [7] show how the style of a painter can be extracted from an image of the painting and applied to another normal photograph, thus recreating the photo in the style of the painter. The method has been successfully applied to a wide\\n',\n"," ' GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from   Remote Sensing Imagery ; Advances in remote sensing technologies have made it possible to use high-resolution visual data for weather observation and forecasting tasks. We propose the use of multi-layer neural networks for understanding complex atmospheric dynamics based on multichannel satellite images. The capability of our model was evaluated by using a linear regression task for single typhoon coordinates prediction. A specific comb\\n',\n"," ' Improving Efficiency in Convolutional Neural Network with Multilinear   Filters ; The excellent performance of deep neural networks has enabled us to solve several automatization problems, opening an era of autonomous devices. However, current deep net architectures are heavy with millions of parameters and require billions of floating point operations. Several works have been developed to compress a pre-trained deep network to reduce memory footprint and, possibly, computation. Instead of compressing a pre\\n',\n"," ' Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided   Diagnosis of Diabetic Retinopathy ; Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown considerable promise in recent years as a potential tool for improving clinical decision support in medical oncology, particularly those based around the concept of Discovery Radiomics, where radiomic sequencers are discovered through the analysis of medical imaging data. One of the main limitations with current CAD approaches is that it i\\n',\n"," ' HP-GAN: Probabilistic 3D human motion prediction via GAN ; Predicting and understanding human motion dynamics has many applications, such as motion synthesis, augmented reality, security, and autonomous vehicles. Due to the recent success of generative adversarial networks (GAN), there has been much interest in probabilistic estimation and synthetic data generation using deep neural network architectures and learning algorithms.   We propose a novel sequence-to-sequence model for probabilistic human motion \\n',\n"," ' Report: Dynamic Eye Movement Matching and Visualization Tool in Neuro   Gesture ; In the research of the impact of gestures using by a lecturer, one challenging task is to infer the attention of a group of audiences. Two important measurements that can help infer the level of attention are eye movement data and Electroencephalography (EEG) data. Under the fundamental assumption that a group of people would look at the same place if they all pay attention at the same time, we apply a method, \"Time Warp Edit \\n',\n"," ' Nature vs. Nurture: The Role of Environmental Resources in Evolutionary   Deep Intelligence ; Evolutionary deep intelligence synthesizes highly efficient deep neural networks architectures over successive generations. Inspired by the nature versus nurture debate, we propose a study to examine the role of external factors on the network synthesis process by varying the availability of simulated environmental resources. Experimental results were obtained for networks synthesized via asexual evolutionary synth\\n',\n"," ' A stochastic model of human visual attention with a dynamic Bayesian   network ; Recent studies in the field of human vision science suggest that the human responses to the stimuli on a visual display are non-deterministic. People may attend to different locations on the same visual input at the same time. Based on this knowledge, we propose a new stochastic model of visual attention by introducing a dynamic Bayesian network to predict the likelihood of where humans typically focus on a video scene. The pro\\n',\n"," ' Smart Content Recognition from Images Using a Mixture of Convolutional   Neural Networks ; With rapid development of the Internet, web contents become huge. Most of the websites are publicly available, and anyone can access the contents from anywhere such as workplace, home and even schools. Nevertheless, not all the web contents are appropriate for all users, especially children. An example of these contents is pornography images which should be restricted to certain age group. Besides, these images are no\\n',\n"," ' Cortical spatio-temporal dimensionality reduction for visual grouping ; The visual systems of many mammals, including humans, is able to integrate the geometric information of visual stimuli and to perform cognitive tasks already at the first stages of the cortical processing. This is thought to be the result of a combination of mechanisms, which include feature extraction at single cell level and geometric processing by means of cells connectivity. We present a geometric model of such connectivities in the\\n',\n"," ' Visual Sentiment Prediction with Deep Convolutional Neural Networks ; Images have become one of the most popular types of media through which users convey their emotions within online social networks. Although vast amount of research is devoted to sentiment analysis of textual data, there has been very limited work that focuses on analyzing sentiment of image data. In this work, we propose a novel visual sentiment prediction framework that performs image understanding with Deep Convolutional Neural Networks\\n',\n"," ' Correntropy Maximization via ADMM - Application to Robust Hyperspectral   Unmixing ; In hyperspectral images, some spectral bands suffer from low signal-to-noise ratio due to noisy acquisition and atmospheric effects, thus requiring robust techniques for the unmixing problem. This paper presents a robust supervised spectral unmixing approach for hyperspectral images. The robustness is achieved by writing the unmixing problem as the maximization of the correntropy criterion subject to the most commonly used \\n',\n"," ' Identifying individual facial expressions by deconstructing a neural   network ; This paper focuses on the problem of explaining predictions of psychological attributes such as attractiveness, happiness, confidence and intelligence from face photographs using deep neural networks. Since psychological attribute datasets typically suffer from small sample sizes, we apply transfer learning with two base models to avoid overfitting. These models were trained on an age and gender prediction task, respectively. U\\n',\n"," ' Object Boundary Detection and Classification with Image-level Labels ; Semantic boundary and edge detection aims at simultaneously detecting object edge pixels in images and assigning class labels to them. Systematic training of predictors for this task requires the labeling of edges in images which is a particularly tedious task. We propose a novel strategy for solving this task, when pixel-level annotations are not available, performing it in an almost zero-shot manner by relying on conventional whole ima\\n',\n"," ' Evolving Spatially Aggregated Features from Satellite Imagery for   Regional Modeling ; Satellite imagery and remote sensing provide explanatory variables at relatively high resolutions for modeling geospatial phenomena, yet regional summaries are often desirable for analysis and actionable insight. In this paper, we propose a novel method of inducing spatial aggregations as a component of the machine learning process, yielding regional model features whose construction is driven by model prediction perform\\n',\n"," ' Pillar Networks++: Distributed non-parametric deep and wide networks ; In recent work, it was shown that combining multi-kernel based support vector machines (SVMs) can lead to near state-of-the-art performance on an action recognition dataset (HMDB-51 dataset). This was 0.4\\\\% lower than frameworks that used hand-crafted features in addition to the deep convolutional feature extractors. In the present work, we show that combining distributed Gaussian Processes with multi-stream deep convolutional neural net\\n',\n"," ' Market-Based Reinforcement Learning in Partially Observable Worlds ; Unlike traditional reinforcement learning (RL), market-based RL is in principle applicable to worlds described by partially observable Markov Decision Processes (POMDPs), where an agent needs to learn short-term memories of relevant previous events in order to execute optimal actions. Most previous work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here we reimplement a recent approach to market-based RL and for the \\n',\n"," ' Controlled hierarchical filtering: Model of neocortical sensory   processing ; A model of sensory information processing is presented. The model assumes that learning of internal (hidden) generative models, which can predict the future and evaluate the precision of that prediction, is of central importance for information extraction. Furthermore, the model makes a bridge to goal-oriented systems and builds upon the structural similarity between the architecture of a robust controller and that of the hippoca\\n',\n"," ' When Do Differences Matter? On-Line Feature Extraction Through Cognitive   Economy ; For an intelligent agent to be truly autonomous, it must be able to adapt its representation to the requirements of its task as it interacts with the world. Most current approaches to on-line feature extraction are ad hoc; in contrast, this paper presents an algorithm that bases judgments of state compatibility and state-space abstraction on principled criteria derived from the psychological principle of cognitive economy. \\n',\n"," ' Applying Policy Iteration for Training Recurrent Neural Networks ; Recurrent neural networks are often used for learning time-series data. Based on a few assumptions we model this learning task as a minimization problem of a nonlinear least-squares cost function. The special structure of the cost function allows us to build a connection to reinforcement learning. We exploit this connection and derive a convergent, policy iteration-based algorithm. Furthermore, we argue that RNN training can be fit naturally\\n',\n"," ' A Neural-Network Technique to Learn Concepts from Electroencephalograms ; A new technique is presented developed to learn multi-class concepts from clinical electroencephalograms. A desired concept is represented as a neuronal computational model consisting of the input, hidden, and output neurons. In this model the hidden neurons learn independently to classify the electroencephalogram segments presented by spectral and statistical features. This technique has been applied to the electroencephalogram data \\n',\n"," \" Empirical learning aided by weak domain knowledge in the form of feature   importance ; Standard hybrid learners that use domain knowledge require stronger knowledge that is hard and expensive to acquire. However, weaker domain knowledge can benefit from prior knowledge while being cost effective. Weak knowledge in the form of feature relative importance (FRI) is presented and explained. Feature relative importance is a real valued approximation of a feature's importance provided by experts. Advantage of us\\n\",\n"," ' Evolutionary Algorithms for Reinforcement Learning ; There are two distinct approaches to solving reinforcement learning problems, namely, searching in value function space and searching in policy space. Temporal difference methods and evolutionary algorithms are well-known examples of these approaches. Kaelbling, Littman and Moore recently provided an informative survey of temporal difference methods. This article focuses on the application of evolutionary algorithms to the reinforcement learning problem, \\n',\n"," ' On Training Deep Boltzmann Machines ; The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \"deep\" probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic ma\\n',\n"," ' Memristive fuzzy edge detector ; Fuzzy inference systems always suffer from the lack of efficient structures or platforms for their hardware implementation. In this paper, we tried to overcome this problem by proposing new method for the implementation of those fuzzy inference systems which use fuzzy rule base to make inference. To achieve this goal, we have designed a multi-layer neuro-fuzzy computing system based on the memristor crossbar structure by introducing some new concepts like fuzzy minterms. Alt\\n',\n"," ' Echo State Queueing Network: a new reservoir computing learning tool ; In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational app\\n',\n"," ' The Predictron: End-To-End Learning and Planning ; One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple \"imagined\" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trai\\n',\n"," ' Quadratically constrained quadratic programming for classification using   particle swarms and applications ; Particle swarm optimization is used in several combinatorial optimization problems. In this work, particle swarms are used to solve quadratic programming problems with quadratic constraints. The approach of particle swarms is an example for interior point methods in optimization as an iterative technique. This approach is novel and deals with classification problems without the use of a traditional \\n',\n"," ' Learning to Execute ; Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right \\n',\n"," ' Bitwise Neural Networks ; Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained envir\\n',\n"," ' Graying the black box: Understanding DQNs ; In recent years there is a growing interest in using deep representations for reinforcement learning. In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically. The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detec\\n',\n"," ' Evaluation of a Tree-based Pipeline Optimization Tool for Automating   Data Science ; As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectivene\\n',\n"," ' Probabilistic Reasoning via Deep Learning: Neural Association Models ; In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence. We propose to use neural networks to model association between any two events in a domain. Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are to be associated. The actual meaning of the conditional proba\\n',\n"," ' Deep Reinforcement Learning With Macro-Actions ; Deep reinforcement learning has been shown to be a powerful framework for learning policies from complex high-dimensional sensory inputs to actions in complex tasks, such as the Atari domain. In this paper, we explore output representation modeling in the form of temporal abstraction to improve convergence and reliability of deep reinforcement learning approaches. We concentrate on macro-actions, and evaluate these on different Atari 2600 games, where we show\\n',\n"," ' RETAIN: An Interpretable Predictive Model for Healthcare using Reverse   Time Attention Mechanism ; Accuracy and interpretability are two dominant features of successful predictive models. Typically, a choice must be made in favor of complex black box models such as recurrent neural networks (RNN) for accuracy versus less accurate but more interpretable traditional models such as logistic regression. This tradeoff poses challenges in medicine where both accuracy and interpretability are important. We addres\\n',\n"," ' A High Speed Multi-label Classifier based on Extreme Learning Machines ; In this paper a high speed neural network classifier based on extreme learning machines for multi-label classification problem is proposed and dis-cussed. Multi-label classification is a superset of traditional binary and multi-class classification problems. The proposed work extends the extreme learning machine technique to adapt to the multi-label problems. As opposed to the single-label problem, both the number of labels the sample \\n',\n"," ' An Online Universal Classifier for Binary, Multi-class and Multi-label   Classification ; Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are sub-categories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label class\\n',\n"," ' Adaptive Online Sequential ELM for Concept Drift Tackling ; A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier sche\\n',\n"," ' Adaptive Convolutional ELM For Concept Drift Handling in Online Stream   Data ; In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced\\n',\n"," ' Particle Swarm Optimization for Generating Interpretable Fuzzy   Reinforcement Learning Policies ; Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world rein\\n',\n"," ' A Growing Long-term Episodic & Semantic Memory ; The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that \\n',\n"," ' Cognitive Discriminative Mappings for Rapid Learning ; Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognit\\n',\n"," ' Towards a Mathematical Understanding of the Difficulty in Learning with   Feedforward Neural Networks ; Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations\\n',\n"," ' An effective algorithm for hyperparameter optimization of neural   networks ; A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the perf\\n',\n"," ' Evolutionary Training of Sparse Artificial Neural Networks: A Network   Science Perspective ; Through the success of deep learning, Artificial Neural Networks (ANNs) are among the most used artificial intelligence methods nowadays. ANNs have led to major breakthroughs in various domains, such as particle physics, reinforcement learning, speech recognition, computer vision, and so on. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (\\n',\n"," ' Attend and Predict: Understanding Gene Regulation by Selective Attention   on Chromatin ; The past decade has seen a revolution in genomic technologies that enable a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the cor\\n',\n"," ' Parallelizing Linear Recurrent Neural Nets Over Sequence Length ; Recurrent neural networks (RNNs) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. We show the training of RNNs with only linear sequential dependencies can be parallelized over the sequence length using the parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. We develop a parallel linear recu\\n',\n"," ' Feature learning in feature-sample networks using multi-objective   optimization ; Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of the learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those models. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been p\\n',\n"," ' Meta-Learning and Universality: Deep Representations and Gradient   Descent can Approximate any Learning Algorithm ; Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representat\\n',\n"," ' Hindsight policy gradients ; Goal-conditional policies allow reinforcement learning agents to pursue specific goals during different episodes. In addition to their potential to generalize desired behavior to unseen goals, such policies may also help in defining options for arbitrary subgoals, enabling higher-level planning. While trying to achieve a specific goal, an agent may also be able to exploit information about the degree to which it has achieved alternative goals. Reinforcement learning agents have \\n',\n"," ' SquishedNets: Squishing SqueezeNet further for edge device scenarios via   deep evolutionary synthesis ; While deep neural networks have been shown in recent years to outperform other machine learning methods in a wide range of applications, one of the biggest challenges with enabling deep neural networks for widespread deployment on edge devices such as mobile and other consumer devices is high computational and memory requirements. Recently, there has been greater exploration into small deep neural networ\\n',\n"," ' Autonomous development and learning in artificial intelligence and   robotics: Scaling up deep learning to human--like learning ; Autonomous lifelong development and learning is a fundamental capability of humans, differentiating them from current deep learning systems. However, other branches of artificial intelligence have designed crucial ingredients towards autonomous learning: curiosity and intrinsic motivation, social learning and natural interaction with peers, and embodiment. These mechanisms guide \\n',\n"," ' Learning from Scarce Experience ; Searching the space of policies directly for the optimal policy has been one popular method for solving partially observable reinforcement learning problems. Typically, with each change of the target policy, its value is estimated from the results of following that very policy. This requires a large number of interactions with the environment as different polices are considered. We present a family of algorithms based on likelihood ratio estimation that use data gathered wh\\n',\n"," ' Fitness inheritance in the Bayesian optimization algorithm ; This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only\\n',\n"," ' The Combined Technique for Detection of Artifacts in Clinical   Electroencephalograms of Sleeping Newborns ; In this paper we describe a new method combining the polynomial neural network and decision tree techniques in order to derive comprehensible classification rules from clinical electroencephalograms (EEGs) recorded from sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement, muscle and noise artifacts and as a consequence some EEG features are irrelevant to classification proble\\n',\n"," ' Evolving Classifiers: Methods for Incremental Learning ; The ability of a classifier to take on new information and classes by evolving the classifier without it having to be fully retrained is known as incremental learning. Incremental learning has been successfully applied to many classification problems, where the data is changing and is not all available at once. In this paper there is a comparison between Learn++, which is one of the most recent incremental learning algorithms, and the new proposed met\\n',\n"," \" Automatic Pattern Classification by Unsupervised Learning Using   Dimensionality Reduction of Data with Mirroring Neural Networks ; This paper proposes an unsupervised learning technique by using Multi-layer Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer Mirroring Neural Network is a neural network that can be trained with generalized data inputs (different categories of image patterns) to perform non-linear dimensionality reduction and the resultant low-dimensional code is used for \\n\",\n"," ' Improving the Performance of PieceWise Linear Separation Incremental   Algorithms for Practical Hardware Implementations ; In this paper we shall review the common problems associated with Piecewise Linear Separation incremental algorithms. This kind of neural models yield poor performances when dealing with some classification problems, due to the evolving schemes used to construct the resulting networks. So as to avoid this undesirable behavior we shall propose a modification criterion. It is based upon t\\n',\n"," ' A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee   Colony Optimization ; Feature selection refers to the problem of selecting relevant features which produce the most predictive outcome. In particular, feature selection task is involved in datasets containing huge number of features. Rough set theory has been one of the most successful methods used for feature selection. However, this method is still not able to find optimal subsets. This paper proposes a new feature selection method based\\n',\n"," ' Automated Query Learning with Wikipedia and Genetic Programming ; Most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge. Work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries, however, not much research has been done in the direction of incorporating human-and-society level knowledge in the queries. This paper is one of the first attempts where such informati\\n',\n"," \" Scaling Up Estimation of Distribution Algorithms For Continuous   Optimization ; Since Estimation of Distribution Algorithms (EDA) were proposed, many attempts have been made to improve EDAs' performance in the context of global optimization. So far, the studies or applications of multivariate probabilistic model based continuous EDAs are still restricted to rather low dimensional problems (smaller than 100D). Traditional EDAs have difficulties in solving higher dimensional problems because of the curse of \\n\",\n"," ' Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA ; An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test \\n',\n"," ' Discrete Dynamical Genetic Programming in XCS ; A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-ada\\n',\n"," ' Fuzzy Dynamical Genetic Programming in XCSF ; A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to Neural Networks, and more recently Dynamical Genetic Programming (DGP). This paper presents results from an investigation into using a fuzzy DGP representation within the XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks are used to represent the traditional condition-action production system rules. I\\n',\n"," ' Learning-Based Procedural Content Generation ; Procedural content generation (PCG) has recently become one of the hottest topics in computational intelligence and AI game researches. Among a variety of PCG techniques, search-based approaches overwhelmingly dominate PCG development at present. While SBPCG leads to promising results and successful applications, it poses a number of challenges ranging from representation to evaluation of the content being generated. In this paper, we present an alternative yet\\n',\n"," ' Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in   the Othello League ; N-tuple networks have been successfully used as position evaluation functions for board games such as Othello or Connect Four. The effectiveness of such networks depends on their architecture, which is determined by the placement of constituent n-tuples, sequences of board locations, providing input to the network. The most popular method of placing n-tuples consists in randomly generating a small number of long, sn\\n',\n"," ' Towards a Self-Organized Agent-Based Simulation Model for Exploration of   Human Synaptic Connections ; In this paper, the early design of our self-organized agent-based simulation model for exploration of synaptic connections that faithfully generates what is observed in natural situation is given. While we take inspiration from neuroscience, our intent is not to create a veridical model of processes in neurodevelopmental biology, nor to represent a real biological system. Instead, our goal is to design a \\n',\n"," ' Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural   Network ; The paper presents the electronic design and motion planning of a robot based on decision making regarding its straight motion and precise turn using Artificial Neural Network (ANN). The ANN helps in learning of robot so that it performs motion autonomously. The weights calculated are implemented in microcontroller. The performance has been tested to be excellent.\\n',\n"," ' Learning Bayesian Network Equivalence Classes with Ant Colony   Optimization ; Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of\\n',\n"," ' Probabilistic Neural Programs ; We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used t\\n',\n"," ' Cognitive Deep Machine Can Train Itself ; Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning can make mistakes and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and confirmation rules are available in knowledge based systems. We show that in limited c\\n',\n"," ' Summary - TerpreT: A Probabilistic Programming Language for Program   Induction ; We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inf\\n',\n"," ' Learning in the Machine: Random Backpropagation and the Deep Learning   Channel ; Random backpropagation (RBP) is a variant of the backpropagation algorithm for training neural networks, where the transpose of the forward matrices are replaced by fixed random matrices in the calculation of the weight updates. It is remarkable both because of its effectiveness, in spite of using random matrices to communicate error information, and because it completely removes the taxing requirement of maintaining symmetric\\n',\n"," ' Highway and Residual Networks learn Unrolled Iterative Estimation ; The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of i\\n',\n"," ' Deep neural heart rate variability analysis ; Despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular disease, they dominate risk screening and triage. On the other hand, heart rate variability is non-invasive and cheap, but not considered accurate enough for clinical practice. Here, we tackle heart beat interval based classification with deep learning. We introduce an end to end differentiable hybrid architecture, consisting of a layer of biological neuron models of \\n',\n"," ' A neural network approach to ordinal regression ; Ordinal regression is an important type of learning, which has properties of both classification and regression. Here we describe a simple and effective approach to adapt a traditional neural network to learn ordinal categories. Our approach is a generalization of the perceptron method for ordinal regression. On several benchmark datasets, our method (NNRank) outperforms a neural network classification method. Compared with the ordinal regression methods usi\\n',\n"," ' Computational Model of Music Sight Reading: A Reinforcement Learning   Approach ; Although the Music Sight Reading process has been studied from the cognitive psychology view points, but the computational learning methods like the Reinforcement Learning have not yet been used to modeling of such processes. In this paper, with regards to essential properties of our specific problem, we consider the value function concept and will indicate that the optimum policy can be obtained by the method we offer without\\n',\n"," \" Using Artificial Bee Colony Algorithm for MLP Training on Earthquake   Time Series Data Prediction ; Nowadays, computer scientists have shown the interest in the study of social insect's behaviour in neural networks area for solving different combinatorial and statistical problems. Chief among these is the Artificial Bee Colony (ABC) algorithm. This paper investigates the use of ABC algorithm that simulates the intelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron (MLP) trained with the\\n\",\n"," \" Multiple chaotic central pattern generators with learning for legged   locomotion and malfunction compensation ; An originally chaotic system can be controlled into various periodic dynamics. When it is implemented into a legged robot's locomotion control as a central pattern generator (CPG), sophisticated gait patterns arise so that the robot can perform various walking behaviors. However, such a single chaotic CPG controller has difficulties dealing with leg malfunction. Specifically, in the scenarios pre\\n\",\n"," \" Teaching Deep Convolutional Neural Networks to Play Go ; Mastering the game of Go has remained a long standing challenge to the field of AI. Modern computer Go systems rely on processing millions of possible future positions to play well, but intuitively a stronger and more 'humanlike' way to play the game would be to rely on pattern recognition abilities rather then brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the move\\n\",\n"," ' Polyphonic Music Generation by Modeling Temporal Dependencies Using a   RNN-DBN ; In this paper, we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a Deep Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state of the RNN that allows it to provide temporal information and a multi-layer DBN that helps in high level representation of the data. This makes RNN-DBNs ideal for sequence generation. Further, the us\\n',\n"," ' Massively Parallel Methods for Deep Reinforcement Learning ; We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed al\\n',\n"," ' A genetic algorithm for autonomous navigation in partially observable   domain ; The problem of autonomous navigation is one of the basic problems for robotics. Although, in general, it may be challenging when an autonomous vehicle is placed into partially observable domain. In this paper we consider simplistic environment model and introduce a navigation algorithm based on Learning Classifier System.\\n',\n"," ' Distributed Deep Q-Learning ; We propose a distributed deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is based on the deep Q-network, a convolutional neural network trained with a variant of Q-learning. Its input is raw pixels and its output is a value function estimating future rewards from taking an action given a system state. To distribute the deep Q-network training, we adapt the DistBelief software framewo\\n',\n"," ' Lifted Relational Neural Networks ; We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to dif\\n',\n"," \" Giraffe: Using Deep Reinforcement Learning to Play Chess ; This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluati\\n\",\n"," ' Attention with Intention for a Neural Network Conversation Model ; In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a rec\\n',\n"," ' Deep Reinforcement Learning in Parameterized Action Space ; Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces. However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces. To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which feat\\n',\n"," ' MazeBase: A Sandbox for Learning from Games ; This paper introduces MazeBase: an environment for simple 2D games, designed as a sandbox for machine learning approaches to reasoning and planning. Within it, we create 10 simple games embodying a range of algorithmic tasks (e.g. if-then statements or set negation). A variety of neural models (fully connected, convolutional network, memory network) are deployed via reinforcement learning on these games, with and without a procedurally generated curriculum. Desp\\n',\n"," ' On Learning to Think: Algorithmic Information Theory for Novel   Combinations of Reinforcement Learning Controllers and Recurrent Neural World   Models ; This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initia\\n',\n"," ' An Empirical Comparison of Neural Architectures for Reinforcement   Learning in Partially Observable Environments ; This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage v\\n',\n"," ' Predicting Clinical Events by Combining Static and Dynamic Information   Using Recurrent Neural Networks ; In clinical data sets we often find static information (e.g. patient gender, blood type, etc.) combined with sequences of data that are recorded during multiple hospital visits (e.g. medications prescribed, tests performed, etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for modelling sequences of data in many areas of Machine Learning. In this work we present an approach base\\n',\n"," ' Weight Normalization: A Simple Reparameterization to Accelerate Training   of Deep Neural Networks ; We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce an\\n',\n"," ' Bounded Rational Decision-Making in Feedforward Neural Networks ; Bounded rational decision-makers transform sensory input into motor output under limited computational resources. Mathematically, such decision-makers can be modeled as information-theoretic channels with limited transmission rate. Here, we apply this formalism for the first time to multilayer feedforward neural networks. We derive synaptic weight update rules for two scenarios, where either each neuron is considered as a bounded rational dec\\n',\n"," ' Lie Access Neural Turing Machine ; Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space $\\\\mathbb R^n$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a \\n',\n"," ' Towards Machine Intelligence ; There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functiona\\n',\n"," ' Dynamic Frame skip Deep Q Network ; Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and \\n',\n"," ' Programming with a Differentiable Forth Interpreter ; Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth \\n',\n"," ' Generative Choreography using Deep Learning ; Recent advances in deep learning have enabled the extraction of high-level features from raw sensor data which has opened up new possibilities in many different fields, including computer generated choreography. In this paper we present a system chor-rnn for generating novel choreographic material in the nuanced choreographic language and style of an individual choreographer. It also shows promising results in producing a higher level compositional cohesion, rat\\n',\n"," ' Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and   Knowledge ; We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductiv\\n',\n"," ' Identifying and Harnessing the Building Blocks of Machine Learning   Pipelines for Sensible Initialization of a Data Science Automation Tool ; As data science continues to grow in popularity, there will be an increasing need to make data science tools more scalable, flexible, and accessible. In particular, automated machine learning (AutoML) systems seek to automate the process of designing and optimizing machine learning pipelines. In this chapter, we present a genetic programming-based AutoML system calle\\n',\n"," ' Neuroevolution-Based Inverse Reinforcement Learning ; The problem of Learning from Demonstration is targeted at learning to perform tasks based on observed examples. One approach to Learning from Demonstration is Inverse Reinforcement Learning, in which actions are observed to infer rewards. This work combines a feature based state evaluation approach to Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying neural networks based on their performance on a given task. Neural networks ar\\n',\n"," ' TerpreT: A Probabilistic Programming Language for Program Induction ; We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programm\\n',\n"," ' Multi-Label Classification Method Based on Extreme Learning Machines ; In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based mu\\n',\n"," ' A Novel Online Real-time Classifier for Multi-label Data Streams ; In this paper, a novel extreme learning machine based online multi-label classifier for real-time data streams is proposed. Multi-label classification is one of the actively researched machine learning paradigm that has gained much attention in the recent years due to its rapidly increasing real world applications. In contrast to traditional binary and multi-class classification, multi-label classification involves association of each of the\\n',\n"," ' A Novel Progressive Learning Technique for Multi-class Classification ; In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and\\n',\n"," ' A novel online multi-label classifier for high-speed streaming data   applications ; In this paper, a high-speed online neural network classifier based on extreme learning machines for multi-label classification is proposed. In multi-label classification, each of the input data sample belongs to one or more than one of the target labels. The traditional binary and multi-class classification where each sample belongs to only one target class forms the subset of multi-label classification. Multi-label classif\\n',\n"," ' Ternary Neural Networks for Resource-Efficient AI Applications ; The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limits their deployability on ubiquitous computing devices such as smart phones, wearables and autonomous drones. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach based on a novel, layer-wise greedy methodology. Thanks to our tw\\n',\n"," ' Fitted Learning: Models with Awareness of their Limits ; Though deep learning has pushed the boundaries of classification forward, in recent years hints of the limits of standard classification have begun to emerge. Problems such as fooling, adding new classes over time, and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime, where a comprehensive a priori knowledge of the possible classes or concepts i\\n',\n"," ' Learning to learn with backpropagation of Hebbian plasticity ; Hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience. By contrast, artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete. While recent methods can endow neural networks with long-term memories, Hebbian plasticity is currently not amenable to gradient descent. Here we derive analytical expressions for \\n',\n"," ' Learning by Stimulation Avoidance: A Principle to Control Spiking Neural   Networks Dynamics ; Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical sta\\n',\n"," \" Surprisal-Driven Zoneout ; We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.31 on the Hutter Prize Wikipedia \\n\",\n"," ' Neural Architecture Search with Reinforcement Learning ; Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIF\\n',\n"," \" Emergence of foveal image sampling from learning to attend in visual   scenes ; We describe a neural attention model with a learnable retinal sampling lattice. The model is trained on a visual search task requiring the classification of an object embedded in a visual scene amidst background distractors using the smallest number of fixations. We explore the tiling properties that emerge in the model's retinal sampling lattice after training. Specifically, we show that this lattice resembles the eccentricity \\n\",\n"," \" Long Timescale Credit Assignment in NeuralNetworks with External Memory ; Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. This creates many problems, such as vanishing gradients, that have been well studied. In contrast, a NNEM's architecture recurrent activity doesn't involve a long chain of activity (\\n\",\n"," ' Energy Saving Additive Neural Network ; In recent years, machine learning techniques based on neural networks for mobile computing become increasingly popular. Classical multi-layer neural networks require matrix multiplications at each stage. Multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable funct\\n',\n"," ' Learning to Repeat: Fine Grained Action Repetition for Deep   Reinforcement Learning ; Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Acti\\n',\n"," \" Survey of reasoning using Neural networks ; Reason and inference require process as well as memory skills by humans. Neural networks are able to process tasks like image recognition (better than humans) but in memory aspects are still limited (by attention mechanism, size). Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve small memory contexts, but as context becomes larger than a threshold, it is difficult to use them. The Solution is to use large external memory. Still, it p\\n\",\n"," ' One-Shot Imitation Learning ; Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capabi\\n',\n"," ' Deep Learning for Explicitly Modeling Optimization Landscapes ; In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimiza\\n',\n"," ' Stochastic Neural Networks for Hierarchical Reinforcement Learning ; Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intr\\n',\n"," ' Batch Reinforcement Learning on the Industrial Benchmark: First   Experiences ; The Particle Swarm Optimization Policy (PSO-P) has been recently introduced and proven to produce remarkable results on interacting with academic reinforcement learning benchmarks in an off-policy, batch-based setting. To further investigate the properties and feasibility on real-world applications, this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a novel reinforcement learning (RL) benchmark that aims a\\n',\n"," ' End-to-End Differentiable Proving ; We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning wi\\n',\n"," ' Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments ; We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is a\\n',\n"," ' Getting deep recommenders fit: Bloom embeddings for sparse binary   input/output networks ; Recommendation algorithms that incorporate techniques from deep learning are becoming increasingly popular. Due to the structure of the data coming from recommendation domains (i.e., one-hot-encoded vectors of item preferences), these algorithms tend to have large input and output dimensionalities that dominate their overall size. This makes them difficult to train, due to the limited memory of graphical processing u\\n',\n"," ' Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural   Network and Long-Term Evaluation ; Monte Carlo tree search (MCTS) is extremely popular in computer Go which determines each action by enormous simulations in a broad and deep search tree. However, human experts select most actions by pattern analysis and careful evaluation rather than brute search of millions of future nteractions. In this paper, we propose a computer Go system that follows experts way of thinking and playing. Our sys\\n',\n"," ' Hindsight Experience Replay ; Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.   We demonstrate our approach on the task of manipulating objects wit\\n',\n"," ' Trial without Error: Towards Safe Reinforcement Learning via Human   Intervention ; AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven\\'t yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human \"in the loop\" and ready t\\n',\n"," ' Reverse Curriculum Generation for Reinforcement Learning ; Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some le\\n',\n"," ' Ideological Sublations: Resolution of Dialectic in Population-based   Optimization ; A population-based optimization algorithm was designed, inspired by two main thinking modes in philosophy, both based on dialectic concept and thesis-antithesis paradigm. They impose two different kinds of dialectics. Idealistic and materialistic antitheses are formulated as optimization models. Based on the models, the population is coordinated for dialectical interactions. At the population-based context, the formulated o\\n',\n"," ' ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural   Projections ; Deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. However, it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. While these devices could make use of machine learning models running on high-performance data\\n',\n"," ' A Flow Model of Neural Networks ; Based on a natural connection between ResNet and transport equation or its characteristic equation, we propose a continuous flow model for both ResNet and plain net. Through this continuous model, a ResNet can be explicitly constructed as a refinement of a plain net. The flow model provides an alternative perspective to understand phenomena in deep neural networks, such as why it is necessary and sufficient to use 2-layer blocks in ResNets, why deeper is better, and why Res\\n',\n"," ' Multimodal Content Analysis for Effective Advertisements on YouTube ; The rapid advances in e-commerce and Web 2.0 technologies have greatly increased the impact of commercial advertisements on the general public. As a key enabling technology, a multitude of recommender systems exists which analyzes user features and browsing patterns to recommend appealing advertisements to users. In this work, we seek to study the characteristics or attributes that characterize an effective advertisement and recommend a u\\n',\n"," ' Overcoming Exploration in Reinforcement Learning with Demonstrations ; Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In thi\\n',\n"," ' Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency   for Sequence Modeling ; Recurrent neural networks have shown remarkable success in modeling sequences. However low resource situations still adversely affect the generalizability of these models. We introduce a new family of models, called Lattice Recurrent Units (LRU), to address the challenge of learning deep multi-layer recurrent models with limited resources. LRU models achieve this goal by creating distinct (but coupled) flow of\\n',\n"," ' Scalable Recollections for Continual Lifelong Learning ; Given the recent success of Deep Learning applied to a variety of single tasks, it is natural to consider more human-realistic settings. Perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online over a continuous stream of non-stationary data. A continual lifelong learning system must have three primary capabilities to succeed: it must learn and adapt over time, it must not forget what it ha\\n',\n"," ' Hidden Tree Markov Networks: Deep and Wide Learning for Structured Data ; The paper introduces the Hidden Tree Markov Network (HTN), a neuro-probabilistic hybrid fusing the representation power of generative models for trees with the incremental and discriminative learning capabilities of neural networks. We put forward a modular architecture in which multiple generative models of limited complexity are trained to learn structural feature detectors whose outputs are then combined and integrated by neural la\\n',\n"," ' Hierarchical Actor-Critic ; The ability to learn at different resolutions in time may help overcome one of the main challenges in deep reinforcement learning -- sample efficiency. Hierarchical agents that operate at different levels of temporal abstraction can learn tasks more quickly because they can divide the work of learning behaviors among multiple policies and can also explore the environment at a higher level. In this paper, we present a novel approach to hierarchical reinforcement learning called Hi\\n',\n"," ' Proximodistal Exploration in Motor Learning as an Emergent Property of   Optimization ; To harness the complexity of their high-dimensional bodies during sensorimotor development, infants are guided by patterns of freezing and freeing of degrees of freedom. For instance, when learning to reach, infants free the degrees of freedom in their arm proximodistally, i.e. from joints that are closer to the body to those that are more distant. Here, we formulate and study computationally the hypothesis that such pat\\n',\n"," ' Null Dynamical State Models of Human Cognitive Dysfunction ; The hard problem in artificial intelligence asks how the shuffling of syntactical symbols in a program can lead to systems which experience semantics and qualia. We address this question in three stages. First, we introduce a new class of human semantic symbols which appears when unexpected and drastic environmental change causes humans to become surprised, confused, uncertain, and in extreme cases, unresponsive, passive and dysfunctional. For thi\\n',\n"," \" Accelerating Deep Learning with Memcomputing ; Restricted Boltzmann machines (RBMs) and their extensions, called 'deep-belief networks', are powerful neural networks that have found applications in the fields of machine learning and big data. The standard way to training these models resorts to an iterative unsupervised procedure based on Gibbs sampling, called 'contrastive divergence' (CD), and additional supervised tuning via back-propagation. However, this procedure has been shown not to follow any gradi\\n\",\n"," ' mvn2vec: Preservation and Collaboration in Multi-View Network Embedding ; Multi-view networks are ubiquitous in real-world applications. In order to extract knowledge or business value, it is of interest to transform such networks into representations that are easily machine-actionable. Meanwhile, network embedding has emerged as an effective approach to generate distributed network representations. Therefore, we are motivated to study the problem of multi-view network embedding, with a focus on the charact\\n',\n"," ' Granger-causal Attentive Mixtures of Experts ; Several methods have recently been proposed to detect salient input features for outputs of neural networks. Those methods offer a qualitative glimpse at feature importance, but they fall short of providing quantifiable attributions that can be compared across decisions and measures of the expected quality of their explanations. To address these shortcomings, we present an attentive mixture of experts (AME) that couples attentive gating with a Granger-causal ob\\n',\n"," ' Memorize or generalize? Searching for a compositional RNN in a haystack ; Neural networks are very powerful learning systems, but they do not readily generalize from one task to the other. This is partly due to the fact that they do not learn in a compositional way, that is, by discovering skills that are shared by different tasks, and recombining them to solve new problems. In this paper, we explore the compositional generalization capabilities of recurrent neural networks (RNNs). We first propose the look\\n',\n"," ' Continual Reinforcement Learning with Complex Synapses ; Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components tha\\n',\n"," ' Meta-Reinforcement Learning of Structured Exploration Strategies ; Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an a\\n',\n"," ' Approximation Algorithms for Cascading Prediction Models ; We present an approximation algorithm that takes a pool of pre-trained models as input and produces from it a cascaded model with similar accuracy but lower average-case cost. Applied to state-of-the-art ImageNet classification models, this yields up to a 2x reduction in floating point multiplications, and up to a 6x reduction in average-case memory I/O. The auto-generated cascades exhibit intuitive properties, such as using lower-resolution input f\\n',\n"," ' Coloring black boxes: visualization of neural network decisions ; Neural networks are commonly regarded as black boxes performing incomprehensible functions. For classification problems networks provide maps from high dimensional feature space to K-dimensional image space. Images of training vector are projected on polygon vertices, providing visualization of network function. Such visualization may show the dynamics of learning, allow for comparison of different networks, display training vectors around wh\\n',\n"," ' Relational Neural Expectation Maximization: Unsupervised Discovery of   Objects and their Interactions ; Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel metho\\n',\n"," ' A Bayesian Model for Activities Recommendation and Event Structure   Optimization Using Visitors Tracking ; In events that are composed by many activities, there is a problem that involves retrieve and management the information of visitors that are visiting the activities. This management is crucial to find some activities that are drawing attention of visitors; identify an ideal positioning for activities; which path is more frequented by visitors. In this work, these features are studied using Complex Ne\\n',\n"," ' The Lottery Ticket Hypothesis: Training Pruned Neural Networks ; Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. L\\n',\n"," ' Learning recurrent dynamics in spiking networks ; Spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. While the output of recurrent network models can learn to perform various tasks, the possible range of recurrent dynamics that emerge after learning remains unknown. Here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks t\\n',\n"," \" Principal Graphs and Manifolds ; In many physical, statistical, biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and/or complexity. For this purpose, Karl Pearson invented principal component analysis in 1901 and found 'lines and planes of closest fit to system of points'. The famous k-means algorithm solves the approximation problem too, but by finite sets instead of lines and planes. This chapter gives a brief practical introduction into t\\n\",\n"," ' Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint ; Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoret\\n',\n"," ' Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary   Independent Stochastic Neurons ; In this paper, a simple, general method of adding auxiliary stochastic neurons to a multi-layer perceptron is proposed. It is shown that the proposed method is a generalization of recently successful methods of dropout (Hinton et al., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and semantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework, an extension of dro\\n',\n"," ' Locally Imposing Function for Generalized Constraint Neural Networks - A   Study on Equality Constraints ; This work is a further study on the Generalized Constraint Neural Network (GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to embed any type of prior information and to select its imposing schemes. The work focuses on the second challenge and studies a new constraint imposing scheme for equality constraints. A new method called locally imposing function (LIF) is proposed to \\n',\n"," ' Evolution of Covariance Functions for Gaussian Process Regression using   Genetic Programming ; In this contribution we describe an approach to evolve composite covariance functions for Gaussian processes using genetic programming. A critical aspect of Gaussian processes and similar kernel-based models such as SVM is, that the covariance function should be adapted to the modeled data. Frequently, the squared exponential covariance function is used as a default. However, this can lead to a misspecified model\\n',\n"," \" Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image   Statistics ; We present a theoretical analysis of Gaussian-binary restricted Boltzmann machines (GRBMs) from the perspective of density models. The key aspect of this analysis is to show that GRBMs can be formulated as a constrained mixture of Gaussians, which gives a much better insight into the model's capabilities and limitations. We show that GRBMs are capable of learning meaningful features both in a two-dimensional blind source \\n\",\n"," ' Training Restricted Boltzmann Machine by Perturbation ; A new approach to maximum likelihood learning of discrete graphical models and RBM in particular is introduced. Our method, Perturb and Descend (PD) is inspired by two ideas (I) perturb and MAP method for sampling (II) learning by Contrastive Divergence minimization. In contrast to perturb and MAP, PD leverages training data to learn the models that do not allow efficient MAP estimation. During the learning, to produce a sample from the current model, \\n',\n"," ' Multilayer bootstrap networks ; Multilayer bootstrap network builds a gradually narrowed multilayer nonlinear network from bottom up for unsupervised nonlinear dimensionality reduction. Each layer of the network is a nonparametric density estimator. It consists of a group of k-centroids clusterings. Each clustering randomly selects data points with randomly selected features as its centroids, and learns a one-hot encoder by one-nearest-neighbor optimization. Geometrically, the nonparametric density estimato\\n',\n"," ' Invariant backpropagation: how to train a transformation-invariant   neural network ; In many classification problems a classifier should be robust to small variations in the input vector. This is a desired property not only for particular transformations, such as translation and rotation in image classification problems, but also for all others for which the change is small enough to retain the object perceptually indistinguishable. We propose two extensions of the backpropagation algorithm that train a ne\\n',\n"," ' Shared latent subspace modelling within Gaussian-Binary Restricted   Boltzmann Machines for NIST i-Vector Challenge 2014 ; This paper presents a novel approach to speaker subspace modelling based on Gaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is based on the idea of shared factors as in the Probabilistic Linear Discriminant Analysis (PLDA). GRBM hidden layer is divided into speaker and channel factors, herein the speaker factor is shared over all vectors of the speaker. Then Max\\n',\n"," ' A Neural Transfer Function for a Smooth and Differentiable Transition   Between Additive and Multiplicative Interactions ; Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. This leads either to an inefficient distribution of computational resources or an extensive increase in the computational complexity of the training procedure.   We present a novel, p\\n',\n"," ' A Probabilistic Framework for Deep Learning ; We develop a probabilistic framework for deep learning based on the Deep Rendering Mixture Model (DRMM), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables. We demonstrate that max-sum inference in the DRMM yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (DCNs), providing a first principles derivation. Our framework provides new insights into the s\\n',\n"," ' Neurogenesis Deep Learning ; Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited abilit\\n',\n"," \" Deep learning for neuroimaging: a validation study ; Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization prob\\n\",\n"," ' Improving Deep Neural Networks with Probabilistic Maxout Units ; We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation\\n',\n"," ' How Many Dissimilarity/Kernel Self Organizing Map Variants Do We Need? ; In numerous applicative contexts, data are too rich and too complex to be represented by numerical vectors. A general approach to extend machine learning and data mining techniques to such data is to really on a dissimilarity or on a kernel that measures how different or similar two objects are. This approach has been used to define several variants of the Self Organizing Map (SOM). This paper reviews those variants in using a common s\\n',\n"," ' Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures ; Model-based methods and deep neural networks have both been tremendously successful paradigms in machine learning. In model-based methods, problem domain knowledge can be built into the constraints of the model, typically at the expense of difficulties during inference. In contrast, deterministic deep neural networks are constructed in such a way that inference is straightforward, but their architectures are generic and it is unclear how \\n',\n"," ' Learning deep dynamical models from image pixels ; Modeling dynamical systems is important in many disciplines, e.g., control, robotics, or neurotechnology. Commonly the state of these systems is not directly observed, but only available through noisy and potentially high-dimensional observations. In these cases, system identification, i.e., finding the measurement mapping and the transition mapping (system dynamics) in latent space can be challenging. For linear system dynamics and measurement mappings eff\\n',\n"," ' From neural PCA to deep unsupervised learning ; A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical \\n',\n"," ' Qualitatively characterizing neural network optimization problems ; Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We\\n',\n"," ' Why does Deep Learning work? - A perspective from Group Theory ; Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning.   One factor behind the recent resurgence of the subject is a key algorithmic step called pre-training: first search for a good generative model for the input samples, and repeat the process one layer at a t\\n',\n"," ' ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient ; Stochastic gradient algorithms have been the main focus of large-scale learning problems and they led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The informa\\n',\n"," ' A Unified Perspective on Multi-Domain and Multi-Task Learning ; In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), wher\\n',\n"," ' A Neural Network Anomaly Detector Using the Random Cluster Model ; The random cluster model is used to define an upper bound on a distance measure as a function of the number of data points to be classified and the expected value of the number of classes to form in a hybrid K-means and regression classification methodology, with the intent of detecting anomalies. Conditions are given for the identification of classes which contain anomalies and individual anomalies within identified classes. A neural networ\\n',\n"," ' A Group Theoretic Perspective on Unsupervised Deep Learning ; Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning.   One factor behind the recent resurgence of the subject is a key algorithmic step called {\\\\em pretraining}: first search for a good generative model for the input samples, and repeat the process one layer at a\\n',\n"," ' A Generative Model for Deep Convolutional Learning ; A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets.\\n',\n"," ' Knowledge Transfer Pre-training ; Pre-training is crucial for learning deep neural networks. Most of existing pre-training methods train simple models (e.g., restricted Boltzmann machines) and then stack them layer by layer to form the deep structure. This layer-wise pre-training has found strong theoretical foundation and broad empirical support. However, it is not easy to employ such method to pre-train models without a clear multi-layer structure,e.g., recurrent neural networks (RNNs). This paper present\\n',\n"," ' Stacked What-Where Auto-encoders ; We present a novel architecture, the \"stacked what-where auto-encoders\" (SWWAE), which integrates discriminative and generative pathways and provides a unified approach to supervised, semi-supervised and unsupervised learning without relying on sampling during training. An instantiation of SWWAE uses a convolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and employs a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the reconstruction. Th\\n',\n"," ' Training recurrent networks online without backtracking ; We introduce the \"NoBackTrack\" algorithm to train the parameters of dynamical systems such as recurrent neural networks. This algorithm works in an online, memoryless setting, thus requiring no backpropagation through time, and is scalable, avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters.   The algorithm essentially maintains, at each time, a single search direction\\n',\n"," ' Deep clustering: Discriminative embeddings for segmentation and   separation ; We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how \\n',\n"," ' Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural   Networks ; Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a me\\n',\n"," ' Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A   Systematic Study ; This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks. Using variants of the asynchronous stochastic gradient descent algorithm we study the impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtime performance and model accuracy. We introduce a new learning rate modulation\\n',\n"," ' Convolutional Networks on Graphs for Learning Molecular Fingerprints ; We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.\\n',\n"," ' Population-Contrastive-Divergence: Does Consistency help with RBM   training? ; Estimating the log-likelihood gradient with respect to the parameters of a Restricted Boltzmann Machine (RBM) typically requires sampling using Markov Chain Monte Carlo (MCMC) techniques. To save computation time, the Markov chains are only run for a small number of steps, which leads to a biased estimate. This bias can cause RBM training algorithms such as Contrastive Divergence (CD) learning to deteriorate. We adopt the idea b\\n',\n"," ' AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction   in Structure-based Drug Discovery ; Deep convolutional neural networks comprise a subclass of deep neural networks (DNN) with a constrained architecture that leverages the spatial and temporal structure of the domain they model. Convolutional networks achieve the best predictive performance in areas such as speech and image recognition by hierarchically composing simple local features into complex models. Although DNNs have been used \\n',\n"," ' Distillation as a Defense to Adversarial Perturbations against Deep   Neural Networks ; Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, so\\n',\n"," ' The Variational Gaussian Process ; Variational inference is a powerful tool for approximate inference, and it has been recently applied for representation learning with deep generative models. We develop the variational Gaussian process (VGP), a Bayesian nonparametric variational family, which adapts its shape to match complex posterior distributions. The VGP generates approximate posterior samples by generating latent inputs and warping them through random non-linear mappings; the distribution over random \\n',\n"," ' Partial Reinitialisation for Optimisers ; Heuristic optimisers which search for an optimal configuration of variables relative to an objective function often get stuck in local optima where the algorithm is unable to find further improvement. The standard approach to circumvent this problem involves periodically restarting the algorithm from random initial configurations when no further improvement can be found. We propose a method of partial reinitialization, whereby, in an attempt to find a better solutio\\n',\n"," ' Efficient Representation of Low-Dimensional Manifolds using Deep   Networks ; We consider the ability of deep neural networks to represent data that lies near a low-dimensional manifold in a high-dimensional space. We show that deep networks can efficiently extract the intrinsic, low-dimensional coordinates of such data. We first show that the first two layers of a deep network can exactly embed points lying on a monotonic chain, a special type of piecewise linear manifold, mapping them to a low-dimensional\\n',\n"," ' Enhanced perceptrons using contrastive biclusters ; Perceptrons are neuronal devices capable of fully discriminating linearly separable classes. Although straightforward to implement and train, their applicability is usually hindered by non-trivial requirements imposed by real-world classification problems. Therefore, several approaches, such as kernel perceptrons, have been conceived to counteract such difficulties. In this paper, we investigate an enhanced perceptron model based on the notion of contrasti\\n',\n"," ' Alternating optimization method based on nonnegative matrix   factorizations for deep neural networks ; The backpropagation algorithm for calculating gradients has been widely used in computation of weights for deep neural networks (DNNs). This method requires derivatives of objective functions and has some difficulties finding appropriate parameters such as learning rate. In this paper, we propose a novel approach for computing weight matrices of fully-connected DNNs by using two types of semi-nonnegative \\n',\n"," \" Robust Large Margin Deep Neural Networks ; The generalization error of deep neural networks via their classification margin is studied in this work. Our approach is based on the Jacobian matrix of a deep neural network and can be applied to networks with arbitrary non-linearities and pooling layers, and to networks with different architectures such as feed forward networks and residual networks. Our analysis leads to the conclusion that a bounded spectral norm of the network's Jacobian matrix in the neighbo\\n\",\n"," ' No bad local minima: Data independent training error guarantees for   multilayer neural networks ; We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almos\\n',\n"," ' Learning Structured Sparsity in Deep Neural Networks ; High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to effic\\n',\n"," ' Depth-Width Tradeoffs in Approximating Natural Functions with Neural   Networks ; We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the $L_1$ norm; and smooth non-linear functions. We also show t\\n',\n"," ' Tensor Switching Networks ; We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence\\n',\n"," ' Survey of Expressivity in Deep Neural Networks ; We survey results on neural network expressivity described in \"On the Expressive Power of Deep Neural Networks\". The paper motivates and develops three natural measures of expressiveness, which all display an exponential dependence on the depth of the network. In fact, all of these measures are related to a fourth quantity, trajectory length. This quantity grows exponentially in the depth of the network, and is responsible for the depth sensitivity observed. \\n',\n"," ' Precise Recovery of Latent Vectors from Generative Adversarial Networks ; Generative adversarial networks (GANs) transform latent vectors into visually plausible images. It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of th\\n',\n"," ' Predicting Surgery Duration with Neural Heteroscedastic Regression ; Scheduling surgeries is a challenging task due to the fundamental uncertainty of the clinical environment, as well as the risks and costs associated with under- and over-booking. We investigate neural regression algorithms to estimate the parameters of surgery case durations, focusing on the issue of heteroscedasticity. We seek to simultaneously estimate the duration of each surgery, as well as a surgery-specific notion of our uncertainty \\n',\n"," ' Depth Creates No Bad Local Minima ; In deep learning, \\\\textit{depth}, as well as \\\\textit{nonlinearity}, create non-convex loss surfaces. Then, does depth alone create bad local minima? In this paper, we prove that without nonlinearity, depth alone does not create bad local minima, although it induces non-convex loss surface. Using this insight, we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. Our theoretical resu\\n',\n"," ' Deep Semi-Random Features for Nonlinear Function Approximation ; We propose semi-random features for nonlinear function approximation. The flexibility of semi-random feature lies between the fully adjustable units in deep learning and the random features used in kernel methods. For one hidden layer models with semi-random features, we prove with no unrealistic assumptions that the model classes contain an arbitrarily good function as the width increases (universality), and despite non-convexity, we can find\\n',\n"," ' Curriculum Dropout ; Dropout is a very effective way of regularizing neural networks. Stochastically \"dropping out\" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout pro\\n',\n"," ' The power of deeper networks for expressing natural functions ; It is well-known that neural networks are universal approximators, but that deeper networks tend to be much more efficient than shallow ones. We shed light on this by proving that the total number of neurons $m$ required to approximate natural classes of multivariate polynomials of $n$ variables grows only linearly with $n$ for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence t\\n',\n"," ' Gradient Descent for Spiking Neural Networks ; Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network\\n',\n"," ' Unsure When to Stop? Ask Your Semantic Neighbors ; In iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data. If the search is continued beyond this point, the risk of overfitting increases significantly. Following the recent developments in inductive semantic stochastic methods, this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search\\n',\n"," ' Anomaly Detection on Graph Time Series ; In this paper, we use variational recurrent neural network to investigate the anomaly detection problem on graph time series. The temporal correlation is modeled by the combination of recurrent neural network (RNN) and variational inference (VI), while the spatial information is captured by the graph convolutional network. In order to incorporate external factors, we use feature extractor to augment the transition of latent variables, which can learn the influence of\\n',\n"," ' A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and   Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data ; Gated Recurrent Unit (GRU) is a recently-developed variation of the long short-term memory (LSTM) unit, both of which are types of recurrent neural network (RNN). Through empirical evidence, both models have been proven to be effective in a wide variety of machine learning tasks such as natural language processing (Wen et al., 2015), speech recognition (Chor\\n',\n"," ' DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in   Neural Networks ; Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classif\\n',\n"," ' A Method of Generating Random Weights and Biases in Feedforward Neural   Networks with Random Hidden Nodes ; Neural networks with random hidden nodes have gained increasing interest from researchers and practical applications. This is due to their unique features such as very fast training and universal approximation property. In these networks the weights and biases of hidden nodes determining the nonlinear feature mapping are set randomly and are not learned. Appropriate selection of the intervals from wh\\n',\n"," ' Rotational Unit of Memory ; The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks. However, RNN still have a limited capacity to manipulate long-term memory. To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational U\\n',\n"," ' Progressive Growing of GANs for Improved Quality, Stability, and   Variation ; We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simpl\\n',\n"," ' Generative Adversarial Source Separation ; Generative source separation methods such as non-negative matrix factorization (NMF) or auto-encoders, rely on the assumption of an output probability density. Generative Adversarial Networks (GANs) can learn data distributions without needing a parametric assumption on the output density. We show on a speech source separation experiment that, a multi-layer perceptron trained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders trained with maximum lik\\n',\n"," ' A Supervised STDP-based Training Algorithm for Living Neural Networks ; Neural networks have shown great potential in many applications like speech recognition, drug discovery, image classification, and object detection. Neural network models are inspired by biological neural networks, but they are optimized to perform machine learning tasks on digital computers. The proposed work explores the possibilities of using living neural networks in vitro as basic computational elements for machine learning applica\\n',\n"," ' Improving Factor-Based Quantitative Investing by Forecasting Company   Fundamentals ; On a periodic basis, publicly traded companies are required to report fundamentals: financial data such as revenue, operating income, debt, among others. These data points provide some insight into the financial health of a company. Academic research has identified some factors, i.e. computed features of the reported data, that are known through retrospective analysis to outperform the market average. Two popular factors a\\n',\n"," \" Genetic Algorithms for Mentor-Assisted Evaluation Function Optimization ; In this paper we demonstrate how genetic algorithms can be used to reverse engineer an evaluation function's parameters for computer chess. Our results show that using an appropriate mentor, we can evolve a program that is on par with top tournament-playing chess programs, outperforming a two-time World Computer Chess Champion. This performance gain is achieved by evolving a program with a smaller number of parameters in its evaluatio\\n\",\n"," ' Block Neural Network Avoids Catastrophic Forgetting When Learning   Multiple Task ; In the present work we propose a Deep Feed Forward network architecture which can be trained according to a sequential learning paradigm, where tasks of increasing difficulty are learned sequentially, yet avoiding catastrophic forgetting. The proposed architecture can re-use the features learned on previous tasks in a new task when the old tasks and the new one are related. The architecture needs fewer computational resource\\n',\n"," ' A Scalable Deep Neural Network Architecture for Multi-Building and   Multi-Floor Indoor Localization Based on Wi-Fi Fingerprinting ; One of the key technologies for future large-scale location-aware services covering a complex of multi-story buildings --- e.g., a big shopping mall and a university campus --- is a scalable indoor localization technique. In this paper, we report the current status of our investigation on the use of deep neural networks (DNNs) for scalable building/floor classification and flo\\n',\n"," ' Dynamic Boltzmann Machines for Second Order Moments and Generalized   Gaussian Distributions ; Dynamic Boltzmann Machine (DyBM) has been shown highly efficient to predict time-series data. Gaussian DyBM is a DyBM that assumes the predicted data is generated by a Gaussian distribution whose first-order moment (mean) dynamically changes over time but its second-order moment (variance) is fixed. However, in many financial applications, the assumption is quite limiting in two aspects. First, even when the data \\n',\n"," ' Multi-timescale memory dynamics in a reinforcement learning network with   attention-gated memory ; Learning and memory are intertwined in our brain and their relationship is at the core of several recent neural network models. In particular, the Attention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning network with an emphasis on biological plausibility of memory dynamics and learning. We find that the AuGMEnT network does not solve some hierarchical tasks, where higher-level stimuli have \\n',\n"," ' Weighted Contrastive Divergence ; Learning algorithms for energy based Boltzmann architectures that rely on gradient descent are in general computationally prohibitive, typically due to the exponential number of terms involved in computing the partition function. In this way one has to resort to approximation schemes for the evaluation of the gradient. This is the case of Restricted Boltzmann Machines (RBM) and its learning algorithm Contrastive Divergence (CD). It is well-known that CD has a number of shor\\n',\n"," ' Dynamic Optimization of Neural Network Structures Using Probabilistic   Modeling ; Deep neural networks (DNNs) are powerful machine learning models and have succeeded in various artificial intelligence tasks. Although various architectures and modules for the DNNs have been proposed, selecting and designing the appropriate network structure for a target problem is a challenging task. In this paper, we propose a method to simultaneously optimize the network structure and weight parameters during neural netwo\\n',\n"," ' Pruning Techniques for Mixed Ensembles of Genetic Programming Models ; The objective of this paper is to define an effective strategy for building an ensemble of Genetic Programming (GP) models. Ensemble methods are widely used in machine learning due to their features: they average out biases, they reduce the variance and they usually generalize better than single models. Despite these advantages, building ensemble of GP models is not a well-developed topic in the evolutionary computation community. To fil\\n',\n"," ' Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines ; This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the m\\n',\n"," ' Stochastic Pooling for Regularization of Deep Convolutional Neural   Networks ; We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches,\\n',\n"," \" Training Neural Networks with Stochastic Hessian-Free Optimization ; Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens' HF for \\n\",\n"," ' Reversible Jump MCMC Simulated Annealing for Neural Networks ; We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated annealing algorithm to optimize radial basis function (RBF) networks. This algorithm enables us to maximize the joint posterior distribution of the network parameters and the number of basis functions. It performs a global search in the joint space of the parameters and number of parameters, thereby surmounting the problem of local minima. We also show that by calibrati\\n',\n"," ' Predicting Parameters in Deep Learning ; We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able t\\n',\n"," ' Disentangling Factors of Variation via Generative Entangling ; Here we propose a novel model family with the objective of learning to disentangle the factors of variation in data. Our approach is based on the spike-and-slab restricted Boltzmann machine which we generalize to include higher-order interactions among multiple latent variables. Seen from a generative perspective, the multiplicative interactions emulates the entangling of factors of variation. Inference in the model can be seen as disentangling \\n',\n"," ' Neural Networks for Complex Data ; Artificial neural networks are simple and efficient machine learning tools. Defined originally in the traditional setting of simple vector data, neural network models have evolved to address more and more difficulties of complex real world problems, ranging from time evolving data to sophisticated data structures such as graphs and functions. This paper summarizes advances on those themes from the last decade, with a focus on results obtained by members of the SAMM team of\\n',\n"," \" Multi-task Neural Networks for QSAR Predictions ; Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural \\n\",\n"," ' A Hybrid Latent Variable Neural Network Model for Item Recommendation ; Collaborative filtering is used to recommend items to a user without requiring a knowledge of the item itself and tends to outperform other techniques. However, collaborative filtering suffers from the cold-start problem, which occurs when an item has not yet been rated or a user has not rated any items. Incorporating additional information, such as item or user descriptions, into collaborative filtering can address the cold-start probl\\n',\n"," ' Techniques for Learning Binary Stochastic Feedforward Neural Networks ; Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance pot\\n',\n"," ' Learning ELM network weights using linear discriminant analysis ; We present an alternative to the pseudo-inverse method for determining the hidden to output weight values for Extreme Learning Machines performing classification tasks. The method is based on linear discriminant analysis and provides Bayes optimal single point estimates for the weight values.\\n',\n"," ' Exponentially Increasing the Capacity-to-Computation Ratio for   Conditional Computation in Deep Learning ; Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to\\n',\n"," ' Soft-Deep Boltzmann Machines ; We present a layered Boltzmann machine (BM) that can better exploit the advantages of a distributed representation. It is widely believed that deep BMs (DBMs) have far greater representational power than its shallow counterpart, restricted Boltzmann machines (RBMs). However, this expectation on the supremacy of DBMs over RBMs has not ever been validated in a theoretical fashion. In this paper, we provide both theoretical and empirical evidences that the representational power \\n',\n"," ' Domain-Adversarial Training of Neural Networks ; We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the con\\n',\n"," ' Deep Online Convex Optimization with Gated Games ; Methods from convex optimization are widely used as building blocks for deep learning algorithms. However, the reasons for their empirical success are unclear, since modern convolutional networks (convnets), incorporating rectifier units and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. This paper provides the first convergence rates for gradient descent on rectifier convnets. The proof utilizes the particular struc\\n',\n"," ' Churn analysis using deep convolutional neural networks and autoencoders ; Customer temporal behavioral data was represented as images in order to perform churn prediction by leveraging deep learning architectures prominent in image classification. Supervised learning was performed on labeled data of over 6 million customers using deep convolutional neural networks, which achieved an AUC of 0.743 on the test dataset using no more than 12 temporal features for each customer. Unsupervised learning was conduct\\n',\n"," ' Developing an ICU scoring system with interaction terms using a genetic   algorithm ; ICU mortality scoring systems attempt to predict patient mortality using predictive models with various clinical predictors. Examples of such systems are APACHE, SAPS and MPM. However, most such scoring systems do not actively look for and include interaction terms, despite physicians intuitively taking such interactions into account when making a diagnosis. One barrier to including such terms in predictive models is the d\\n',\n"," ' Scale Normalization ; One of the difficulties of training deep neural networks is caused by improper scaling between layers. Scaling issues introduce exploding / gradient problems, and have typically been addressed by careful scale-preserving initialization. We investigate the value of preserving scale, or isometry, beyond the initial weights. We propose two methods of maintaing isometry, one exact and one stochastic. Preliminary experiments show that for both determinant and scale-normalization effectively\\n',\n"," ' Layer-wise learning of deep generative models ; When using deep, multi-layered architectures to build generative models of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as generative models, by showing that they train a lower bound of this criterion. We test the new lear\\n',\n"," ' Distributed optimization of deeply nested systems ; In science and engineering, intelligent processing of complex signals such as images, sound or language is often performed by a parameterized hierarchy of nonlinear processing layers, sometimes biologically inspired. Hierarchical systems (or, more generally, nested systems) offer a way to generate complex mappings using simple stages. Each layer performs a different operation and achieves an ever more sophisticated representation of the input, as, for exam\\n',\n"," ' Understanding Boltzmann Machine and Deep Learning via A Confident   Information First Principle ; Typical dimensionality reduction methods focus on directly reducing the number of random variables while retaining maximal variations in the data. In this paper, we consider the dimensionality reduction in parameter spaces of binary multivariate distributions. We propose a general Confident-Information-First (CIF) principle to maximally preserve parameters with confident estimates and rule out unreliable or noi\\n',\n"," ' Canonical dual solutions to nonconvex radial basis neural network   optimization problem ; Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in regression problems. One of their principal drawbacks is that the formulation corresponding to the training with the supervision of both the centers and the weights is a highly non-convex optimization problem, which leads to some fundamentally difficulties for traditional optimization theory and methods.   This paper presents a generalized canoni\\n',\n"," \" On the Number of Linear Regions of Deep Neural Networks ; We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enable\\n\",\n"," ' Geometry and Expressive Power of Conditional Restricted Boltzmann   Machines ; Conditional restricted Boltzmann machines are undirected stochastic neural networks with a layer of input and output units connected bipartitely to a layer of hidden units. These networks define models of conditional probability distributions on the states of the output units given the states of the input units, parametrized by interaction weights and biases. We address the representational power of these models, proving results \\n',\n"," ' Is Joint Training Better for Deep Auto-Encoders? ; Traditionally, when generative models of data are developed via deep architectures, greedy layer-wise pre-training is employed. In a well-trained model, the lower layer of the architecture models the data distribution conditional upon the hidden variables, while the higher layers model the hidden distribution prior. But due to the greedy scheme of the layerwise training technique, the parameters of lower layers are fixed when training higher layers. This ma\\n',\n"," ' Massively Multitask Networks for Drug Discovery ; Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some i\\n',\n"," ' Gated Feedback Recurrent Neural Networks ; In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evalua\\n',\n"," \" Deep Learning with Limited Numerical Precision ; Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed\\n\",\n"," \" MADE: Masked Autoencoder for Distribution Estimation ; There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted\\n\",\n"," ' Simple, Efficient, and Neural Algorithms for Sparse Coding ; Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Re- cent work has resulted in several algorithms for sparse coding with provable guarantee\\n',\n"," ' Toxicity Prediction using Deep Learning ; Everyday we are exposed to various chemicals via food additives, cleaning and cosmetic products and medicines -- and some of them might be toxic. However testing the toxicity of all existing compounds by biological experiments is neither financially nor logistically feasible. Therefore the government agencies NIH, EPA and FDA launched the Tox21 Data Challenge within the \"Toxicology in the 21st Century\" (Tox21) initiative. The goal of this challenge was to assess the\\n',\n"," ' To Drop or Not to Drop: Robustness, Consistency and Differential Privacy   Properties of Dropout ; Training deep belief networks (DBNs) requires optimizing a non-convex function with an extremely large number of parameters. Naturally, existing gradient descent (GD) based methods are prone to arbitrarily poor local minima. In this paper, we rigorously show that such local minima can be avoided (upto an approximation error) by using the dropout technique, a widely used heuristic in this domain. In particular,\\n',\n"," ' Distilling the Knowledge in a Neural Network ; A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible\\n',\n"," ' A mathematical motivation for complex-valued convolutional networks ; A complex-valued convolutional network (convnet) implements the repeated application of the following composition of three operations, recursively applying the composition to an input vector of nonnegative real numbers: (1) convolution with complex-valued vectors followed by (2) taking the absolute value of every entry of the resulting vectors followed by (3) local averaging. For processing real-valued random vectors, complex-valued convn\\n',\n"," \" Optimizing Neural Networks with Kronecker-factored Approximate Curvature ; We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire\\n\",\n"," ' Unsupervised model compression for multilayer bootstrap networks ; Recently, multilayer bootstrap network (MBN) has demonstrated promising performance in unsupervised dimensionality reduction. It can learn compact representations in standard data sets, i.e. MNIST and RCV1. However, as a bootstrap method, the prediction complexity of MBN is high. In this paper, we propose an unsupervised model compression framework for this general problem of unsupervised bootstrap methods. The framework compresses a large u\\n',\n"," ' Positive blood culture detection in time series data using a BiLSTM   network ; The presence of bacteria or fungi in the bloodstream of patients is abnormal and can lead to life-threatening conditions. A computational model based on a bidirectional long short-term memory artificial neural network, is explored to assist doctors in the intensive care unit to predict whether examination of blood cultures of patients will return positive. As input it uses nine monitored clinical parameters, presented as time se\\n',\n"," ' Known Unknowns: Uncertainty Quality in Bayesian Neural Networks ; We evaluate the uncertainty quality in neural networks using anomaly detection. We extract uncertainty measures (e.g. entropy) from the predictions of candidate models, use those measures as features for an anomaly detector, and gauge how well the detector differentiates known from unknown classes. We assign higher uncertainty quality to candidate models that lead to better detectors. We also propose a novel method for sampling a variational \\n',\n"," ' Semi-Supervised Learning with the Deep Rendering Mixture Model ; Semi-supervised learning algorithms reduce the high cost of acquiring labeled training data by using both labeled and unlabeled data during learning. Deep Convolutional Networks (DCNs) have achieved great success in supervised tasks and as such have been widely employed in the semi-supervised learning. In this paper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a probabilistic generative model that models latent nuisa\\n',\n"," ' Self-calibrating Neural Networks for Dimensionality Reduction ; Recently, a novel family of biologically plausible online algorithms for reducing the dimensionality of streaming data has been derived from the similarity matching principle. In these algorithms, the number of output dimensions can be determined adaptively by thresholding the singular values of the input data matrix. However, setting such threshold requires knowing the magnitude of the desired singular values in advance. Here we propose online\\n',\n"," ' Tunable Efficient Unitary Neural Networks (EUNN) and their application   to RNNs ; Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantage\\n',\n"," ' Sequence Transduction with Recurrent Neural Networks ; Many machine learning tasks can be expressed as the transformation---or \\\\emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and tran\\n',\n"," ' On Fast Dropout and its Applicability to Recurrent Networks ; Recurrent Neural Networks (RNNs) are rich models for the processing of sequential data. Recent work on advancing the state of the art has been focused on the optimization or modelling of RNNs, mostly motivated by adressing the problems of the vanishing and exploding gradients. The control of overfitting has seen considerably less attention. This paper contributes to that by analyzing fast dropout, a recent regularization method for generalized li\\n',\n"," ' Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks ; In this paper we propose and investigate a novel nonlinear unit, called $L_p$ unit, for deep neural networks. The proposed $L_p$ unit receives signals from several projections of a subset of units in the layer below and computes a normalized $L_p$ norm. We notice two interesting interpretations of the $L_p$ unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as averag\\n',\n"," ' Missing Value Imputation With Unsupervised Backpropagation ; Many data mining and data analysis techniques operate on dense matrices or complete tables of data. Real-world data sets, however, often contain unknown values. Even many classification algorithms that are designed to operate with missing values still exhibit deteriorated accuracy. One approach to handling missing values is to fill in (impute) the missing values. In this paper, we present a technique for unsupervised learning called Unsupervised B\\n',\n"," ' Stochastic Gradient Estimate Variance in Contrastive Divergence and   Persistent Contrastive Divergence ; Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are popular methods for training the weights of Restricted Boltzmann Machines. However, both methods use an approximate method for sampling from the model distribution. As a side effect, these approximations yield significantly different biases and variances for stochastic gradient estimates of individual data points. It is well kno\\n',\n"," ' How to Construct Deep Recurrent Neural Networks ; In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \\\\textit{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on \\n',\n"," ' Neuronal Synchrony in Complex-Valued Deep Networks ; Deep learning has recently led to great successes in tasks such as image recognition (e.g Krizhevsky et al., 2012). However, deep networks are still outmatched by the power and versatility of the brain, perhaps in part due to the richer neuronal computations available to cortical circuits. The challenge is to identify which neuronal mechanisms are relevant, and to find suitable abstractions to model them. Here, we show how aspects of spike timing, long hy\\n',\n"," ' An empirical analysis of dropout in piecewise linear networks ; The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the\\n',\n"," ' An Empirical Investigation of Catastrophic Forgetting in Gradient-Based   Neural Networks ; Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both establi\\n',\n"," ' Unsupervised Domain Adaptation by Backpropagation ; Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amou\\n',\n"," \" Deep Directed Generative Autoencoders ; For discrete data, the likelihood $P(x)$ can be rewritten exactly and parametrized into $P(X = x) = P(X = x | H = f(x)) P(H = f(x))$ if $P(X | H)$ has enough capacity to put no probability mass on any $x'$ for which $f(x')\\\\neq f(x)$, where $f(\\\\cdot)$ is a deterministic discrete function. The log of the first factor gives rise to the log-likelihood reconstruction error of an autoencoder with $f(\\\\cdot)$ as the encoder and $P(X|H)$ as the (probabilistic) decoder. The log\\n\",\n"," ' An exact mapping between the Variational Renormalization Group and Deep   Learning ; Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood \\n',\n"," ' Non-parametric Bayesian Learning with Deep Learning Structure and Its   Applications in Wireless Networks ; In this paper, we present an infinite hierarchical non-parametric Bayesian model to extract the hidden factors over observed data, where the number of hidden factors for each layer is unknown and can be potentially infinite. Moreover, the number of layers can also be infinite. We construct the model structure that allows continuous values for the hidden factors and weights, which makes the model suita\\n',\n"," ' Parallel training of DNNs with Natural Gradient and Parameter Averaging ; We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically e\\n',\n"," ' End-to-end Continuous Speech Recognition using Attention-based Recurrent   NN: First Results ; We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset \\n',\n"," ' Provable Methods for Training Neural Networks with Sparse Connectivity ; We provide novel guaranteed approaches for training feedforward neural networks with sparse connectivity. We leverage on the techniques developed previously for learning linear networks and show that they can also be effectively adopted to learn non-linear networks. We operate on the moments involving label and the score function of the input, and show that their factorization provably yields the weight matrix of the first layer of a d\\n',\n"," ' Domain-Adversarial Neural Networks ; We introduce a new representation learning algorithm suited to the context of domain adaptation, in which data at training and test time come from similar but different distributions. Our algorithm is directly inspired by theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on a data representation that cannot discriminate between the training (source) and test (target) domains. We propose a training ob\\n',\n"," ' Learning with Pseudo-Ensembles ; We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In \\n',\n"," ' Random Walk Initialization for Training Very Deep Feedforward Networks ; Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not as difficult as previously thought. Unlike when back-propagation is applied to a recurrent network, application to an FFN amounts to multiplying the error gradient by a differ\\n',\n"," ' Variational Recurrent Auto-Encoders ; In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order t\\n',\n"," ' Neural Network Regularization via Robust Weight Factorization ; Regularization is essential when training large neural networks. As deep neural networks can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new regularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the Univ\\n',\n"," ' A Bayesian encourages dropout ; Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.\\n',\n"," ' Deep Fried Convnets ; The fully connected layers of a deep convolutional neural network typically contain over 90% of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices.   In this paper we show how kernel methods, in particular a single\\n',\n"," ' Lateral Connections in Denoising Autoencoders Support Supervised   Learning ; We show how a deep denoising autoencoder with lateral connections can be used as an auxiliary unsupervised learning task to support supervised learning. The proposed model is trained to minimize simultaneously the sum of supervised and unsupervised cost functions by back-propagation, avoiding the need for layer-wise pretraining. It improves the state of the art significantly in the permutation-invariant MNIST classification task.\\n',\n"," ' Deep Neural Networks with Random Gaussian Weights: A Universal   Classification Strategy? ; Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these netw\\n',\n"," ' Imaging Time-Series to Improve Classification and Imputation ; Inspired by recent successes of deep learning in computer vision, we propose a novel framework for encoding time series as different types of images, namely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov Transition Fields (MTF). This enables the use of techniques from computer vision for time series classification and imputation. We used Tiled Convolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn high-l\\n',\n"," \" Blocks and Fuel: Frameworks for deep learning ; We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visua\\n\",\n"," ' Adaptive Normalized Risk-Averting Training For Deep Neural Networks ; This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard $L_p$-norm error. By analyzing the gradient on the convexity index $\\\\lambda$, we explain the reason why to learn $\\\\lambda$\\n',\n"," ' Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer   Free Energy ; Restricted Boltzmann machines are undirected neural networks which have been shown to be effective in many applications, including serving as initializations for training deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence, for unsupervised training. We propose an alternative deterministic iterative p\\n',\n"," ' Pointer Networks ; We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various c\\n',\n"," ...]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["d = \"sadsadsadsfffffffddd dddsfd dsD\"\n","print(list(set(list(d))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hspbf3NY8yfS","executionInfo":{"status":"ok","timestamp":1710779913514,"user_tz":-180,"elapsed":118,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"8ced24f4-9ef4-46ad-979d-1c7bf284055b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['d', 's', 'D', ' ', 'a', 'f']\n"]}]},{"cell_type":"markdown","metadata":{"id":"fG0k_DCg3lnO"},"source":["Our next step is __building char-level vocabulary__. Put simply, you need to assemble a list of all unique tokens in the dataset."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6T_bCYAX3lnO","executionInfo":{"status":"ok","timestamp":1710780148542,"user_tz":-180,"elapsed":235031,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"1d614978-fc2c-465b-9c14-9458827882b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["n_tokens =  136\n"]}],"source":["# get all unique characters from lines (including capital letters and symbols)\n","tokens = list()\n","for line in lines:\n","  tokens.append(list(set(list(line))))\n","tokens = list(set(sum(tokens, [])))\n","\n","tokens = sorted(tokens)\n","n_tokens = len(tokens)\n","print ('n_tokens = ',n_tokens)\n","assert 100 < n_tokens < 150\n","assert BOS in tokens, EOS in tokens"]},{"cell_type":"markdown","metadata":{"id":"lFvmUKJI3lnP"},"source":["We can now assign each character with its index in tokens list. This way we can encode a string into a torch-friendly integer vector."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pTgCZgoq3lnP","executionInfo":{"status":"ok","timestamp":1710780148542,"user_tz":-180,"elapsed":6,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["# dictionary of character -> its identifier (index in tokens list)\n","id_to_token = dict(enumerate(tokens))\n","token_to_id = {item: i for i, item in id_to_token.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eur3sZCP3lnQ","executionInfo":{"status":"ok","timestamp":1710780148542,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"c20f00cd-7325-4844-b978-fff8826f770c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Seems alright!\n"]}],"source":["assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n","for i in range(n_tokens):\n","    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n","\n","print(\"Seems alright!\")"]},{"cell_type":"markdown","metadata":{"id":"ZzjSy0gp3lnQ"},"source":["Our final step is to assemble several strings in a integer matrix with shape `[batch_size, text_length]`.\n","\n","The only problem is that each sequence has a different length. We can work around that by padding short sequences with extra `\"EOS\"` tokens or cropping long sequences. Here's how it works:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"omF6-Yrz3lnQ","executionInfo":{"status":"ok","timestamp":1710780148542,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["def to_matrix(lines, max_len=None, pad=token_to_id[EOS], dtype=np.int64):\n","    \"\"\"Casts a list of lines into torch-digestable matrix\"\"\"\n","    max_len = max_len or max(map(len, lines))\n","    lines_ix = np.full([len(lines), max_len], pad, dtype=dtype)\n","    for i in range(len(lines)):\n","        line_ix = list(map(token_to_id.get, lines[i][:max_len]))\n","        lines_ix[i, :len(line_ix)] = line_ix\n","    return lines_ix"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSUj1GeT3lnR","executionInfo":{"status":"ok","timestamp":1710780148542,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"b89ff68a-95a4-4219-8feb-2f3a62b3f3ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n"," [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n"]}],"source":["#Example: cast 4 random names to a single matrix, pad with zeros where needed.\n","dummy_lines = [\n","    ' abc\\n',\n","    ' abacaba\\n',\n","    ' abc1234567890\\n',\n","]\n","print(to_matrix(dummy_lines))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_c5X1pIv3lnR"},"source":["### Neural Language Model (2 points including training)\n","\n","Just like for N-gram LMs, we want to estimate probability of text as a joint probability of tokens (symbols this time).\n","\n","$$P(X) = \\prod_t P(x_t \\mid x_0, \\dots, x_{t-1}).$$\n","\n","Instead of counting all possible statistics, we want to train a neural network with parameters $\\theta$ that estimates the conditional probabilities:\n","\n","$$ P(x_t \\mid x_0, \\dots, x_{t-1}) \\approx p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) $$\n","\n","\n","But before we optimize, we need to define our neural network. Let's start with a fixed-window (aka convolutional) architecture:\n","\n","<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/fixed_window_lm.jpg' width=400px>\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"2O_Mw4ht3lnR","executionInfo":{"status":"ok","timestamp":1710780153837,"user_tz":-180,"elapsed":5298,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["emb = nn.Embedding(6000, 16)\n","pad = nn.ZeroPad2d((32, 0, 0, 0))\n","conv = nn.Conv1d(16, 64, kernel_size=5, padding=0)\n","conv1 = nn.Conv1d(16, 64, kernel_size=5, padding=0, dilation=8)\n","conv2 = nn.Conv1d(64, 64, kernel_size=5, padding=0, dilation=8)\n","dense = nn.Linear(64, 6000)\n","input = torch.ones(11, 35000).long()"],"metadata":{"id":"cM7f_10303nI","executionInfo":{"status":"ok","timestamp":1710780153838,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["h = emb(input)\n","h = h.permute(0, 2, 1)\n","print(f'before pads and convs shape is: {h.shape}')\n","h = pad(h)\n","h = conv1(h)\n","print(f'after first pad and conv shape is: {h.shape}')\n","h = pad(h)\n","h = conv2(h)\n","print(f'after second pad and conv shape is: {h.shape}')\n","h = pad(h)\n","h = conv2(h)\n","print(f'after third pad and conv shape is: {h.shape}')\n","h = h.permute(0, 2, 1)\n","h = dense(h)\n","print(f'finally shape is: {h.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxYVOh7tnJnI","executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":22494,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"fe5c7b8d-98fc-4ace-b11a-557ac5590fc1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["before pads and convs shape is: torch.Size([11, 16, 35000])\n","after first pad and conv shape is: torch.Size([11, 64, 35000])\n","after second pad and conv shape is: torch.Size([11, 64, 35000])\n","after third pad and conv shape is: torch.Size([11, 64, 35000])\n","finally shape is: torch.Size([11, 35000, 6000])\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"IIyzGhcG3lnS","executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":11,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["class FixedWindowLanguageModel(nn.Module):\n","    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=64):\n","        \"\"\"\n","        A fixed window model that looks on at least 5 previous symbols.\n","\n","        Note: fixed window LM is effectively performing a convolution over a sequence of words.\n","        This convolution only looks on current and previous words.\n","        Such convolution can be represented as a sequence of 2 operations:\n","        - pad input vectors by {strides * (filter_size - 1)} zero vectors on the \"left\", do not pad right\n","        - perform regular convolution with {filter_size} and {strides}\n","\n","        - If you're absolutely lost, here's a hint: use nn.ZeroPad2d((NUM_LEADING_ZEROS, 0, 0, 0))\n","          followed by a nn.Conv1d(..., padding=0). And yes, its okay that padding is technically \"2d\".\n","        \"\"\"\n","        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n","\n","        # YOUR CODE - create layers/variables and any metadata you want, e.g. self.emb = L.Embedding(...)\n","        strides = 1\n","        filter_size = 33\n","        kernel_size = 5\n","\n","\n","        self.emb = nn.Embedding(n_tokens, emb_size)\n","        self.pad = nn.ZeroPad2d((strides*(filter_size-1), 0, 0, 0))\n","        self.conv1 = nn.Conv1d(emb_size, hid_size, kernel_size=kernel_size, padding=0, dilation=8)\n","        self.conv2 = nn.Conv1d(hid_size, hid_size, kernel_size=kernel_size, padding=0, dilation=8)\n","        self.dense = nn.Linear(hid_size, n_tokens)\n","        #END OF YOUR CODE\n","\n","    def __call__(self, input_ix):\n","        \"\"\"\n","        compute language model logits given input tokens\n","        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n","        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n","            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n","\n","        :note: that convolutions operate with tensors of shape [batch, channels, length], while linear layers\n","         and *embeddings* use [batch, length, channels] tensors. Use tensor.permute(...) to adjust shapes.\n","\n","        \"\"\"\n","\n","        sequence_length = len(input_ix[0])\n","        # YOUR CODE - apply layers, see docstring above\n","        h = self.emb(input_ix)\n","        h = h.permute(0, 2, 1)\n","        h = self.pad(h)\n","        h = self.conv1(h)\n","        h = self.pad(h)\n","        h = self.conv2(h)\n","        h = self.pad(h)\n","        h = self.conv2(h)\n","        h = h.permute(0, 2, 1)\n","        return self.dense(h) # output tensor should be of shape [batch_size, sequence_length, n_tokens]\n","\n","    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n","        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n","        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64)\n","        with torch.no_grad():\n","            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n","        return dict(zip(tokens, probs))\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"7-cKvAeT3lnS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":10,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"5668478e-c742-4ee7-d074-80edadb4a6d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weights: ('emb.weight', 'conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'dense.weight', 'dense.bias')\n"]}],"source":["dummy_model = FixedWindowLanguageModel()\n","\n","dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n","dummy_logits = dummy_model(dummy_input_ix)\n","\n","print('Weights:', tuple(name for name, w in dummy_model.named_parameters()))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"-Ane7BFi3lnS","executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["assert isinstance(dummy_logits, torch.Tensor)\n","assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n","assert np.all(np.isfinite(dummy_logits.data.cpu().numpy())), \"inf/nan encountered\"\n","assert not np.allclose(dummy_logits.data.cpu().numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\""]},{"cell_type":"code","execution_count":16,"metadata":{"id":"QMmyuh5a3lnS","executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["# test for lookahead\n","dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n","dummy_logits_2 = dummy_model(dummy_input_ix_2)\n","\n","assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n","    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n","    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""]},{"cell_type":"markdown","metadata":{"id":"1aP5uCFb3lnT"},"source":["We can now tune our network's parameters to minimize categorical crossentropy over training dataset $D$:\n","\n","$$ L = {\\frac1{|D|}} \\sum_{X \\in D} \\sum_{x_i \\in X} - \\log p(x_t \\mid x_1, \\dots, x_{t-1}, \\theta) $$\n","\n","As usual with with neural nets, this optimization is performed via stochastic gradient descent with backprop.  One can also note that minimizing crossentropy is equivalent to minimizing model __perplexity__, KL-divergence or maximizng log-likelihood."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"0fs3zB583lnT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":9,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"1a0ad5a9-0566-492b-b8f1-250c06953a99"},"outputs":[{"output_type":"stream","name":"stdout","text":["matrix:\n"," [[ 1 66 67 68  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 1 66 67 66 68 66 67 66  0  0  0  0  0  0  0]\n"," [ 1 66 67 68 18 19 20 21 22 23 24 25 26 17  0]]\n","mask: [[1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n"," [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n"," [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n","lengths: [ 5  9 15]\n"]}],"source":["def compute_mask(input_ix, eos_ix=token_to_id[EOS]):\n","    \"\"\" compute a boolean mask that equals \"1\" until first EOS (including that EOS) \"\"\"\n","    return F.pad(torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1, pad=(1, 0, 0, 0), value=True)\n","\n","print('matrix:\\n', dummy_input_ix.numpy())\n","print('mask:', compute_mask(dummy_input_ix).to(torch.int32).cpu().numpy())\n","print('lengths:', compute_mask(dummy_input_ix).sum(-1).cpu().numpy())"]},{"cell_type":"code","source":["def softmax_cross_entropy_with_logits(labels, logits):\n","    return (-labels * F.log_softmax(logits, dim=-1).permute(2, 0, 1)).sum(dim=-1)"],"metadata":{"id":"hPnZ1uwnS1Si","executionInfo":{"status":"ok","timestamp":1710780176329,"user_tz":-180,"elapsed":7,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"id":"pk-FYG2B3lnT","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["def compute_loss(model, input_ix):\n","    \"\"\"\n","    :param model: language model that can compute next token logits given token indices\n","    :param input ix: int32 matrix of tokens, shape: [batch_size, length]; padded with eos_ix\n","    :returns: scalar loss function, mean crossentropy over non-eos tokens\n","    \"\"\"\n","    input_ix = torch.as_tensor(input_ix, dtype=torch.int64)\n","\n","\n","    logits = model(input_ix[:, :-1])\n","    logits = logits.permute(0, 2, 1)\n","    reference_answers = input_ix[:, 1:]\n","\n","    # Your task: implement loss function as per formula above\n","    # your loss should only be computed on actual tokens, excluding padding\n","    # predicting actual tokens and first EOS do count. Subsequent EOS-es don't\n","    # you may or may not want to use the compute_mask function from above.\n","\n","    mask = compute_mask(input_ix[:, :-1]).to(torch.int32)\n","    lengths = mask.sum(-1)\n","    loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits))\n","    #loss = F.nll_loss(F.log_softmax(torch.tensor(logits), dim=-1), torch.tensor(reference_answers))\n","    #loss = F.cross_entropy(torch.tensor(logits), torch.tensor(reference_answers))\n","    masked_loss = loss @ mask\n","    return masked_loss.sum(-1) / sum(lengths) # scalar loss\n","\n"]},{"cell_type":"code","source":["input_ix = torch.as_tensor(to_matrix(dummy_lines, max_len=15), dtype=torch.int64)\n","logits = dummy_model(input_ix[:, :-1])#.permute(0, 2, 1)\n","reference_answers = input_ix[:, 1:]"],"metadata":{"id":"WpiGK6Yuhbqe","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["mask = compute_mask(input_ix[:, :-1]).to(torch.float32)"],"metadata":{"id":"-imBSHRXmWy8","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":8,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["mask.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9Ho4KTUmYja","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":7,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"25014023-b9ba-4720-f4a0-361a7d8383cd"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 14])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["mask.sum(-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMBqUBae39MO","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":6,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"bf4fe11b-5630-42d6-9ccb-c0c65486ddfb"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 5.,  9., 14.])"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stU7Yo0Qk3vN","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":5,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"a8953732-5de1-4a09-985a-8e967e00f64d"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 14, 136])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["reference_answers.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PoU4c-_k6Re","executionInfo":{"status":"ok","timestamp":1710780176330,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"dc8e89fc-b9e1-4904-d831-a50b6846611c"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 14])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits))\n","print(loss.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vL_ZGNRcmeXY","executionInfo":{"status":"ok","timestamp":1710780176695,"user_tz":-180,"elapsed":368,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"7ee3d568-db7f-4319-edc3-46c9931e27cd"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([136, 3])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-26-1c515543dcaf>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits))\n"]}]},{"cell_type":"code","source":["#loss = loss @ mask\n","ans = loss.sum(-1) / sum(mask.sum(-1))\n","print(ans.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1CU9uG033VjI","executionInfo":{"status":"ok","timestamp":1710780176695,"user_tz":-180,"elapsed":7,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"1f8bb85a-ac7f-46e0-fb3b-dd292d3a0d6d"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([136])\n"]}]},{"cell_type":"code","source":["reference_answers * F.log_softmax(logits, dim=-1).permute(2, 0, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzn0yMMBoSSL","executionInfo":{"status":"ok","timestamp":1710780176695,"user_tz":-180,"elapsed":4,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"7a7f0c14-96cf-4858-88c7-839a3a001ca0"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[-318.3719, -321.3402, -327.7745,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-318.3719, -321.3402, -318.1340,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-318.3719, -321.3402, -327.7745,  ..., -124.8267,  -81.6358,\n","            -0.0000]],\n","\n","        [[-331.8119, -335.8052, -340.7312,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-331.8119, -335.8052, -330.7097,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-331.8119, -335.8052, -340.7312,  ..., -129.8148,  -85.8113,\n","            -0.0000]],\n","\n","        [[-331.1422, -335.4001, -339.8178,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-331.1422, -335.4001, -329.8231,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-331.1422, -335.4001, -339.8178,  ..., -129.4892,  -85.1712,\n","            -0.0000]],\n","\n","        ...,\n","\n","        [[-317.3971, -322.6024, -327.1830,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-317.3971, -322.6024, -317.5599,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-317.3971, -322.6024, -327.1830,  ..., -124.7782,  -81.6807,\n","            -0.0000]],\n","\n","        [[-333.0435, -337.6808, -343.4045,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-333.0435, -337.6808, -333.3044,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-333.0435, -337.6808, -343.4045,  ..., -131.7779,  -85.8950,\n","            -0.0000]],\n","\n","        [[-334.8712, -341.5909, -345.8014,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-334.8712, -341.5909, -335.6307,  ...,   -0.0000,   -0.0000,\n","            -0.0000],\n","         [-334.8712, -341.5909, -345.8014,  ..., -132.4620,  -86.3502,\n","            -0.0000]]], grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":32,"metadata":{"id":"6vMZWmmS3lnT","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"error","timestamp":1710692390567,"user_tz":-180,"elapsed":257,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"1c8bf8ab-0eee-4ae9-f501-a1dd7185b10c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-31-d00a36d6de39>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits))\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (14) must match the size of tensor b (136) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-4cd2d7ac3ffa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#loss_2 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=16))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#assert torch.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#'Hint: use compute_mask. Beware +/-1 errors. And be careful when averaging!'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-d00a36d6de39>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, input_ix)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#loss = F.nll_loss(F.log_softmax(torch.tensor(logits), dim=-1), torch.tensor(reference_answers))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#loss = F.cross_entropy(torch.tensor(logits), torch.tensor(reference_answers))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-a07b12730eb6>\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(labels, logits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (14) must match the size of tensor b (136) at non-singleton dimension 2"]}],"source":["loss_1 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=15))\n","#loss_2 = compute_loss(dummy_model, to_matrix(dummy_lines, max_len=16))\n","#assert (np.ndim(loss_1) == 0) and (0 < loss_1 < 100), \"loss must be a positive scalar\"\n","#assert torch.allclose(loss_1, loss_2), 'do not include  AFTER first EOS into loss. '\\\n","    #'Hint: use compute_mask. Beware +/-1 errors. And be careful when averaging!'"]},{"cell_type":"code","source":[],"metadata":{"id":"BYn6bA6y1w1F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0Olm34-3lnT"},"source":["### Evaluation\n","\n","You will need two functions: one to compute test loss and another to generate samples. For your convenience, we implemented them both in your stead."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Hll2NJFL3lnT","executionInfo":{"status":"ok","timestamp":1710782847508,"user_tz":-180,"elapsed":258,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["def score_lines(model, dev_lines, batch_size):\n","    \"\"\" computes average loss over the entire dataset \"\"\"\n","    dev_loss_num, dev_loss_len = 0., 0.\n","    with torch.no_grad():\n","        for i in range(0, len(dev_lines), batch_size):\n","            batch_ix = to_matrix(dev_lines[i: i + batch_size])\n","            dev_loss_num += compute_loss(model, batch_ix).item() * len(batch_ix)\n","            dev_loss_len += len(batch_ix)\n","    return dev_loss_num / dev_loss_len\n","\n","def generate(model, prefix=BOS, temperature=1.0, max_len=100):\n","    \"\"\"\n","    Samples output sequence from probability distribution obtained by model\n","    :param temperature: samples proportionally to model probabilities ^ temperature\n","        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n","    \"\"\"\n","    with torch.no_grad():\n","        while True:\n","            token_probs = model.get_possible_next_tokens(prefix)\n","            tokens, probs = zip(*token_probs.items())\n","            if temperature == 0:\n","                next_token = tokens[np.argmax(probs)]\n","            else:\n","                probs = np.array([p ** (1. / temperature) for p in probs])\n","                probs /= sum(probs)\n","                next_token = np.random.choice(tokens, p=probs)\n","\n","            prefix += next_token\n","            if next_token == EOS or len(prefix) > max_len: break\n","    return prefix"]},{"cell_type":"markdown","metadata":{"id":"7U3Wm4co3lnU"},"source":["### Training loop\n","\n","Finally, let's train our model on minibatches of data"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"sbHAUF8w3lnU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710782935362,"user_tz":-180,"elapsed":1700,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"d322c6f0-c3be-479e-e438-2fc5b993c900"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample before training: BridgingïeΩPôg4sτ*c71\"õ$C69É`{Oó#p_(α?p#N;.mEoCK[Éβ!sCüäωNjöχσHh2öá/ï:1`67)cv+ÉDf%e\n","\n"]}],"source":["from sklearn.model_selection import train_test_split\n","train_lines, dev_lines = train_test_split(lines, test_size=0.25, random_state=42)\n","\n","batch_size = 256\n","score_dev_every = 250\n","train_history, dev_history = [], []\n","model = FixedWindowLanguageModel()\n","opt = torch.optim.Adam(model.parameters())\n","\n","# hint: if you ever wanted to switch to cuda, do it now.\n","\n","# score untrained model\n","#dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n","print(\"Sample before training:\", generate(model, 'Bridging'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awkF7hhU3lnU","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"error","timestamp":1708364703962,"user_tz":-180,"elapsed":2796,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"f3ef570a-8965-4491-d3b1-573e6b1fed2b"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5000 [00:00<?, ?it/s]<ipython-input-131-4ca7450d9053>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits.view(-1, 1)))\n","  0%|          | 0/5000 [00:02<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (256) must match the size of tensor b (17860608) at non-singleton dimension 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-135-5eedf822fe43>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-131-4ca7450d9053>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, input_ix)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scalar loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-130-0463af23517e>\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(labels, logits, dim)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (17860608) at non-singleton dimension 0"]}],"source":["from IPython.display import clear_output\n","from random import sample\n","from tqdm import trange\n","\n","for i in trange(len(train_history), 5000):\n","    batch = to_matrix(sample(train_lines, batch_size))\n","\n","\n","    loss_i = compute_loss(model, batch)\n","\n","    opt.zero_grad()\n","    loss_i.backward()\n","    opt.step()\n","\n","    train_history.append((i, loss_i.item()))\n","\n","    if (i + 1) % 50 == 0:\n","        clear_output(True)\n","        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n","        if len(dev_history):\n","            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n","        plt.legend(); plt.grid(); plt.show()\n","        print(\"Generated examples (tau=0.5):\")\n","        for _ in range(3):\n","            print(generate(model, temperature=0.5))\n","\n","    if (i + 1) % score_dev_every == 0:\n","        print(\"Scoring dev...\")\n","        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n","        print('#%i Dev loss: %.3f' % dev_history[-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRjQFkUt3lnU"},"outputs":[],"source":["assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n","print(\"Final dev loss:\", dev_history[-1][-1])\n","\n","for i in range(10):\n","    print(generate(model, temperature=0.5))"]},{"cell_type":"markdown","metadata":{"id":"k6I1stZr3lnU"},"source":["### RNN Language Models (3 points including training)\n","\n","Fixed-size architectures are reasonably good when capturing short-term dependencies, but their design prevents them from capturing any signal outside their window. We can mitigate this problem by using a __recurrent neural network__:\n","\n","$$ h_0 = \\vec 0 ; \\quad h_{t+1} = RNN(x_t, h_t) $$\n","\n","$$ p(x_t \\mid x_0, \\dots, x_{t-1}, \\theta) = dense_{softmax}(h_{t-1}) $$\n","\n","Such model processes one token at a time, left to right, and maintains a hidden state vector between them. Theoretically, it can learn arbitrarily long temporal dependencies given large enough hidden size.\n","\n","<img src='https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/rnn_lm.jpg' width=480px>"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"O6roAhGK3lnV","executionInfo":{"status":"ok","timestamp":1710782336391,"user_tz":-180,"elapsed":6,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["class RNNLanguageModel(nn.Module):\n","    def __init__(self, n_tokens=n_tokens, emb_size=16, hid_size=256):\n","        \"\"\"\n","        Build a recurrent language model.\n","        You are free to choose anything you want, but the recommended architecture is\n","        - token embeddings\n","        - one or more LSTM/GRU layers with hid size\n","        - linear layer to predict logits\n","\n","        :note: if you use nn.RNN/GRU/LSTM, make sure you specify batch_first=True\n","         With batch_first, your model operates with tensors of shape [batch_size, sequence_length, num_units]\n","         Also, please read the docs carefully: they don't just return what you want them to return :)\n","        \"\"\"\n","        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n","\n","        # YOUR CODE - create layers/variables/etc\n","        self.emb = nn.Embedding(n_tokens, emb_size)\n","        self.lstm = nn.LSTM(emb_size, hid_size, batch_first=True)\n","        self.dense = nn.Linear(hid_size, n_tokens)\n","\n","        #END OF YOUR CODE\n","\n","    def __call__(self, input_ix):\n","        \"\"\"\n","        compute language model logits given input tokens\n","        :param input_ix: batch of sequences with token indices, tensor: int32[batch_size, sequence_length]\n","        :returns: pre-softmax linear outputs of language model [batch_size, sequence_length, n_tokens]\n","            these outputs will be used as logits to compute P(x_t | x_0, ..., x_{t - 1})\n","        \"\"\"\n","        # YOUR CODE - apply layers, see docstring above\n","        x = self.emb(input_ix)\n","        output, (h_n, c_n) = self.lstm(x)\n","        return self.dense(output) # output tensor should be of shape [batch_size, sequence_length, n_tokens]\n","\n","    def get_possible_next_tokens(self, prefix=BOS, temperature=1.0, max_len=100):\n","        \"\"\" :returns: probabilities of next token, dict {token : prob} for all tokens \"\"\"\n","        prefix_ix = torch.as_tensor(to_matrix([prefix]), dtype=torch.int64)\n","        with torch.no_grad():\n","            probs = torch.softmax(self(prefix_ix)[0, -1], dim=-1).cpu().numpy()  # shape: [n_tokens]\n","        return dict(zip(tokens, probs))\n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"2hssRVAd3lnV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710782339256,"user_tz":-180,"elapsed":441,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"1af9f581-fcb1-4919-b9d7-9652cb6c3d66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weights: ('emb.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'dense.weight', 'dense.bias')\n"]}],"source":["model = RNNLanguageModel()\n","\n","dummy_input_ix = torch.as_tensor(to_matrix(dummy_lines))\n","dummy_logits = model(dummy_input_ix)\n","\n","assert isinstance(dummy_logits, torch.Tensor)\n","assert dummy_logits.shape == (len(dummy_lines), max(map(len, dummy_lines)), n_tokens), \"please check output shape\"\n","assert not np.allclose(dummy_logits.cpu().data.numpy().sum(-1), 1), \"please predict linear outputs, don't use softmax (maybe you've just got unlucky)\"\n","print('Weights:', tuple(name for name, w in model.named_parameters()))"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"AllYOsgx3lnV","executionInfo":{"status":"ok","timestamp":1710782579685,"user_tz":-180,"elapsed":304,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}}},"outputs":[],"source":["# test for lookahead\n","dummy_input_ix_2 = torch.as_tensor(to_matrix([line[:3] + 'e' * (len(line) - 3) for line in dummy_lines]))\n","dummy_logits_2 = model(dummy_input_ix_2)\n","\n","assert torch.allclose(dummy_logits[:, :3], dummy_logits_2[:, :3]), \"your model's predictions depend on FUTURE tokens. \" \\\n","    \" Make sure you don't allow any layers to look ahead of current token.\" \\\n","    \" You can also get this error if your model is not deterministic (e.g. dropout). Disable it for this test.\""]},{"cell_type":"markdown","metadata":{"id":"TASq_7_H3lnV"},"source":["### RNN training\n","\n","Our RNN language model should optimize the same loss function as fixed-window model. But there's a catch. Since RNN recurrently multiplies gradients through many time-steps, gradient values may explode, [ruining](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/nan.jpg) your model.\n","The common solution to that problem is to clip gradients either [individually](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_value) or [globally](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/clip_by_global_norm).\n","\n","Your task here is to implement the training code that minimizes the loss function. If you encounter large loss fluctuations during training, please add [gradient clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) using urls above. But its **not necessary** to use gradient clipping if you don't need it.\n","\n","_Note: gradient clipping is not exclusive to RNNs. Convolutional networks with enough depth often suffer from the same issue._"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"zVydwiti3lnV","colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"status":"error","timestamp":1710782941976,"user_tz":-180,"elapsed":1574,"user":{"displayName":"Anna Chernova","userId":"05759544138678713523"}},"outputId":"604bd458-22d5-4ca8-ad86-752320e4e1ef"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-19-d00a36d6de39>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  loss = softmax_cross_entropy_with_logits(torch.tensor(reference_answers), torch.tensor(logits))\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (513) must match the size of tensor b (136) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-62f23278f09c>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# score untrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdev_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample before training:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bridging'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-caede4fba0fe>\u001b[0m in \u001b[0;36mscore_lines\u001b[0;34m(model, dev_lines, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mbatch_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_lines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mdev_loss_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mdev_loss_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdev_loss_num\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdev_loss_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-d00a36d6de39>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(model, input_ix)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m#loss = F.nll_loss(F.log_softmax(torch.tensor(logits), dim=-1), torch.tensor(reference_answers))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#loss = F.cross_entropy(torch.tensor(logits), torch.tensor(reference_answers))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-a07b12730eb6>\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(labels, logits)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (513) must match the size of tensor b (136) at non-singleton dimension 2"]}],"source":["batch_size = 64         # <-- please tune batch size to fit your CPU/GPU configuration\n","score_dev_every = 250\n","train_history, dev_history = [], []\n","\n","model = RNNLanguageModel()\n","opt = torch.optim.Adam(model.parameters())\n","\n","# score untrained model\n","dev_history.append((0, score_lines(model, dev_lines, batch_size)))\n","print(\"Sample before training:\", generate(model, 'Bridging'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhMqVoAN3lnW"},"outputs":[],"source":["from IPython.display import clear_output\n","from random import sample\n","from tqdm import trange\n","\n","for i in trange(len(train_history), 5000):\n","    batch = to_matrix(sample(train_lines, batch_size))\n","\n","    <YOUR CODE - one step of the training loop for your RNN model>\n","\n","    loss_i = <...>\n","\n","    train_history.append((i, float(loss_i)))\n","\n","    if (i + 1) % 50 == 0:\n","        clear_output(True)\n","        plt.scatter(*zip(*train_history), alpha=0.1, label='train_loss')\n","        if len(dev_history):\n","            plt.plot(*zip(*dev_history), color='red', label='dev_loss')\n","        plt.legend(); plt.grid(); plt.show()\n","        print(\"Generated examples (tau=0.5):\")\n","        for _ in range(3):\n","            print(generate(model, temperature=0.5))\n","\n","    if (i + 1) % score_dev_every == 0:\n","        print(\"Scoring dev...\")\n","        dev_history.append((i, score_lines(model, dev_lines, batch_size)))\n","        print('#%i Dev loss: %.3f' % dev_history[-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMpOqomE3lnW"},"outputs":[],"source":["assert np.mean(train_history[:10], axis=0)[1] > np.mean(train_history[-10:], axis=0)[1], \"The model didn't converge.\"\n","print(\"Final dev loss:\", dev_history[-1][-1])\n","for i in range(10):\n","    print(generate(model, temperature=0.5))"]},{"cell_type":"markdown","metadata":{"id":"JaS6T-ma3lnW"},"source":["### Alternative sampling strategies (1 point)\n","\n","So far we've sampled tokens from the model in proportion with their probability.\n","However, this approach can sometimes generate nonsense words due to the fact that softmax probabilities of these words are never exactly zero. This issue can be somewhat mitigated with sampling temperature, but low temperature harms sampling diversity. Can we remove the nonsense words without sacrificing diversity? __Yes, we can!__ But it takes a different sampling strategy.\n","\n","__Top-k sampling:__ on each step, sample the next token from __k most likely__ candidates from the language model.\n","\n","Suppose $k=3$ and the token probabilities are $p=[0.1, 0.35, 0.05, 0.2, 0.3]$. You first need to select $k$ most likely words and set the probability of the rest to zero: $\\hat p=[0.0, 0.35, 0.0, 0.2, 0.3]$ and re-normalize:\n","$p^*\\approx[0.0, 0.412, 0.0, 0.235, 0.353]$.\n","\n","__Nucleus sampling:__ similar to top-k sampling, but this time we select $k$ dynamically. In nucleus sampling, we sample from top-__N%__ fraction of the probability mass.\n","\n","Using the same  $p=[0.1, 0.35, 0.05, 0.2, 0.3]$ and nucleus N=0.9, the nucleus words consist of:\n","1. most likely token $w_2$, because $p(w_2) < N$\n","2. second most likely token $w_5$, $p(w_2) + p(w_5) = 0.65 < N$\n","3. third most likely token $w_4$ because $p(w_2) + p(w_5) + p(w_4) = 0.85 < N$\n","\n","And thats it, because the next most likely word would overflow: $p(w_2) + p(w_5) + p(w_4) + p(w_1) = 0.95 > N$.\n","\n","After you've selected the nucleus words, you need to re-normalize them as in top-k sampling and generate the next token.\n","\n","__Your task__ is to implement nucleus sampling variant and see if it is any good."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1k6Yg9U3lnW"},"outputs":[],"source":["def generate_nucleus(model, prefix=BOS, nucleus=0.9, max_len=100):\n","    \"\"\"\n","    Generate a sequence with nucleus sampling\n","    :param prefix: a string containing space-separated previous tokens\n","    :param nucleus: N from the formulae above, N \\in [0, 1]\n","    :param max_len: generate sequences with at most this many tokens, including prefix\n","\n","    :note: make sure that nucleus always contains at least one word, even if p(w*) > nucleus\n","\n","    \"\"\"\n","    while True:\n","        token_probs = model.get_possible_next_tokens(prefix)\n","        tokens, probs = zip(*token_probs.items())\n","\n","        <YOUR CODE HERE>\n","\n","        prefix += <YOUR CODE>\n","        if next_token == EOS or len(prefix) > max_len: break\n","    return prefix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfYFHnrR3lnW"},"outputs":[],"source":["for i in range(10):\n","    print(generate_nucleus(model, nucleus_size=PLAY_WITH_ME_SENPAI))"]},{"cell_type":"markdown","metadata":{"id":"CfRlOGaL3lnb"},"source":["### Bonus quest I: Beam Search (2 pts incl. samples)\n","\n","At times, you don't really want the model to generate diverse outputs as much as you want a __single most likely hypothesis.__ A single best translation, most likely continuation of the search query given prefix, etc. Except, you can't get it.\n","\n","In order to find the exact most likely sequence containing 10 tokens, you would need to enumerate all $|V|^{10}$ possible hypotheses. In practice, 9 times out of 10 you will instead find an approximate most likely output using __beam search__.\n","\n","Here's how it works:\n","0. Initial `beam` = [prefix], max beam_size = k\n","1. for T steps:\n","2. ` ... ` generate all possible next tokens for all hypotheses in beam, formulate `len(beam) * len(vocab)` candidates\n","3. ` ... ` select beam_size best for all candidates as new `beam`\n","4. Select best hypothesis (-es?) from beam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSuQAx3P3lnb"},"outputs":[],"source":["from IPython.display import HTML\n","# Here's what it looks like:\n","!wget -q https://raw.githubusercontent.com/yandexdataschool/nlp_course/2020/resources/beam_search.html\n","HTML(\"beam_search.html\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OS8xLAIU3lnb"},"outputs":[],"source":["def generate_beamsearch(model, prefix=BOS, beam_size=4, length=5):\n","    \"\"\"\n","    Generate a sequence with nucleus sampling\n","    :param prefix: a string containing space-separated previous tokens\n","    :param nucleus: N from the formulae above, N \\in [0, 1]\n","    :param length: generate sequences with at most this many tokens, NOT INCLUDING PREFIX\n","    :returns: beam_size most likely candidates\n","    :note: make sure that nucleus always contains at least one word, even if p(w*) > nucleus\n","    \"\"\"\n","\n","    <YOUR CODE HERE>\n","\n","    return <most likely sequence>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2B5qFYT3lnb"},"outputs":[],"source":["generate_beamsearch(model, prefix=' deep ', beam_size=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"z0X9CevO3lnb"},"outputs":[],"source":["# check it out: which beam size works best?\n","# find at least 5 prefixes where beam_size=1 and 8 generates different sequences"]},{"cell_type":"markdown","metadata":{"id":"j39dTXsT3lnc"},"source":["### Bonus quest II: Ultimate Language Model (2+ pts)\n","\n","So you've learned the building blocks of neural language models, you can now build the ultimate monster:  \n","* Make it char-level, word level or maybe use sub-word units like [bpe](https://github.com/rsennrich/subword-nmt);\n","* Combine convolutions, recurrent cells, pre-trained embeddings and all the black magic deep learning has to offer;\n","  * Use strides to get larger window size quickly. Here's a [scheme](https://storage.googleapis.com/deepmind-live-cms/documents/BlogPost-Fig2-Anim-160908-r01.gif) from google wavenet.\n","* Train on large data. Like... really large. Try [1 Billion Words](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz) benchmark;\n","* Use training schedules to speed up training. Start with small length and increase over time; Take a look at [one cycle](https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6) for learning rate;\n","\n","_You are NOT required to submit this assignment. Please make sure you don't miss your deadline because of it :)_"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}